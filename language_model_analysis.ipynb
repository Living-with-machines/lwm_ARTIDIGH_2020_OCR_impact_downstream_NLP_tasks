{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "from time import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 17:15:39,234 : INFO : Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "2019-11-11 17:15:39,240 : INFO : Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 17:15:47,990 : INFO : loading Word2Vec object from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\n",
      "2019-11-11 17:15:49,144 : INFO : loading wv recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.wv.* with mmap=None\n",
      "2019-11-11 17:15:49,145 : INFO : loading vectors from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.wv.vectors.npy with mmap=None\n",
      "2019-11-11 17:15:49,515 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-11 17:15:49,517 : INFO : loading vocabulary recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.vocabulary.* with mmap=None\n",
      "2019-11-11 17:15:49,518 : INFO : loading trainables recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.trainables.* with mmap=None\n",
      "2019-11-11 17:15:49,519 : INFO : loading syn1neg from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-11 17:15:49,871 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-11 17:15:49,872 : INFO : loaded /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=434049, size=300, alpha=0.03)\n"
     ]
    }
   ],
   "source": [
    "# model types\n",
    "# w2v: word2vec\n",
    "# ft: fasttext\n",
    "model_type = \"w2v\"   \n",
    "model_path = \"/Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\"\n",
    "\n",
    "if model_type.lower() in [\"w2v\", \"word2vec\"]:\n",
    "    # Word2Vec\n",
    "    embedding_model = Word2Vec.load(model_path)\n",
    "elif model_type.lower() in [\"ft\", \"fasttext\"]:\n",
    "    # FastText\n",
    "    embedding_model = FastText.load(model_path)\n",
    "print(embedding_model)\n",
    "embedding_model_orig = copy.deepcopy(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>articleId</th>\n",
       "      <th>articleType</th>\n",
       "      <th>year</th>\n",
       "      <th>ocrText</th>\n",
       "      <th>humanText</th>\n",
       "      <th>corrected</th>\n",
       "      <th>human_ocr_char_diff</th>\n",
       "      <th>str_similarity</th>\n",
       "      <th>str_length_humanText</th>\n",
       "      <th>str_length_ocrText</th>\n",
       "      <th>quality_band</th>\n",
       "      <th>use_human</th>\n",
       "      <th>corrected_sentencizer</th>\n",
       "      <th>corrected_dict_lookup</th>\n",
       "      <th>ocr_sentencizer</th>\n",
       "      <th>ocr_dict_lookup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18378453</td>\n",
       "      <td>Article ILLUSTRATED</td>\n",
       "      <td>1953</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIÄÜ I ^PI A^H\"...</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...</td>\n",
       "      <td>74</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>746</td>\n",
       "      <td>820</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[FROM, RIVER, CROSSING, TO, END, OF, TRIAL, S...</td>\n",
       "      <td>[[], [], [], [], [], [], [], []]</td>\n",
       "      <td>[[FROM, RIVER, CROSSING, TO, END, OF, TRIÄÜ, I...</td>\n",
       "      <td>[[], [], [], [], [], [], [], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18363627</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.964119</td>\n",
       "      <td>641</td>\n",
       "      <td>630</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Natural, Childbirth, Sir,-We, nurses, have, ...</td>\n",
       "      <td>[[], [], [], [], [], [], []]</td>\n",
       "      <td>[[Natural, Childbirth, Sir,-We, nurses, have, ...</td>\n",
       "      <td>[[], [], [], [], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18366055</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>FIRST CHURCH I SERVICE 1 Presbyterian I ' Anni...</td>\n",
       "      <td>FIRST CHURCH SERVICE Presbyterian Anniversary ...</td>\n",
       "      <td>FIRST CHURCH SERVICE Presbyterian Anniversary ...</td>\n",
       "      <td>114</td>\n",
       "      <td>0.738901</td>\n",
       "      <td>946</td>\n",
       "      <td>832</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[[FIRST, CHURCH, SERVICE, Presbyterian, Annive...</td>\n",
       "      <td>[[], [], [], [], [], [], [], [], [], [], [], []]</td>\n",
       "      <td>[[FIRST, CHURCH, I, SERVICE, 1, Presbyterian, ...</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18386137</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium District...</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium  Arthur ...</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium  Arthur ...</td>\n",
       "      <td>210</td>\n",
       "      <td>0.493898</td>\n",
       "      <td>2950</td>\n",
       "      <td>2740</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[[\", Bob, \", Lulham, 's, Fight, Against, Thall...</td>\n",
       "      <td>[[], [], [], [], [], [], [], [], [], [], [], [...</td>\n",
       "      <td>[[\", Bob, \", Lulham, 's, Fight, Against, Thall...</td>\n",
       "      <td>[[], [], [], [], [], [], [], [], [], [], [], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18368961</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr Justic...</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr. Justi...</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr. Justi...</td>\n",
       "      <td>98</td>\n",
       "      <td>0.894176</td>\n",
       "      <td>1219</td>\n",
       "      <td>1121</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[[DIVORCE, Before, The, Judge, In, Divorce, ,,...</td>\n",
       "      <td>[[], [], [], [], [], [], [], []]</td>\n",
       "      <td>[[DIVORCE, Before, The, Judge, In, Divorce, ,,...</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filePath articleId  \\\n",
       "1  ./trove_overproof/datasets/dataset1/rawTextAnd...  18378453   \n",
       "2  ./trove_overproof/datasets/dataset1/rawTextAnd...  18363627   \n",
       "3  ./trove_overproof/datasets/dataset1/rawTextAnd...  18366055   \n",
       "4  ./trove_overproof/datasets/dataset1/rawTextAnd...  18386137   \n",
       "5  ./trove_overproof/datasets/dataset1/rawTextAnd...  18368961   \n",
       "\n",
       "            articleType  year  \\\n",
       "1  Article ILLUSTRATED   1953   \n",
       "2               Article  1953   \n",
       "3               Article  1953   \n",
       "4               Article  1953   \n",
       "5               Article  1953   \n",
       "\n",
       "                                             ocrText  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIÄÜ I ^PI A^H\"...   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...   \n",
       "3  FIRST CHURCH I SERVICE 1 Presbyterian I ' Anni...   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium District...   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr Justic...   \n",
       "\n",
       "                                           humanText  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...   \n",
       "3  FIRST CHURCH SERVICE Presbyterian Anniversary ...   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium  Arthur ...   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr. Justi...   \n",
       "\n",
       "                                           corrected human_ocr_char_diff  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...                  74   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...                  11   \n",
       "3  FIRST CHURCH SERVICE Presbyterian Anniversary ...                 114   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium  Arthur ...                 210   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr. Justi...                  98   \n",
       "\n",
       "   str_similarity  str_length_humanText  str_length_ocrText  quality_band  \\\n",
       "1        0.847561                   746                 820             2   \n",
       "2        0.964119                   641                 630             1   \n",
       "3        0.738901                   946                 832             3   \n",
       "4        0.493898                  2950                2740             4   \n",
       "5        0.894176                  1219                1121             2   \n",
       "\n",
       "   use_human                              corrected_sentencizer  \\\n",
       "1          1  [[FROM, RIVER, CROSSING, TO, END, OF, TRIAL, S...   \n",
       "2          1  [[Natural, Childbirth, Sir,-We, nurses, have, ...   \n",
       "3          1  [[FIRST, CHURCH, SERVICE, Presbyterian, Annive...   \n",
       "4          1  [[\", Bob, \", Lulham, 's, Fight, Against, Thall...   \n",
       "5          1  [[DIVORCE, Before, The, Judge, In, Divorce, ,,...   \n",
       "\n",
       "                               corrected_dict_lookup  \\\n",
       "1                   [[], [], [], [], [], [], [], []]   \n",
       "2                       [[], [], [], [], [], [], []]   \n",
       "3   [[], [], [], [], [], [], [], [], [], [], [], []]   \n",
       "4  [[], [], [], [], [], [], [], [], [], [], [], [...   \n",
       "5                   [[], [], [], [], [], [], [], []]   \n",
       "\n",
       "                                     ocr_sentencizer  \\\n",
       "1  [[FROM, RIVER, CROSSING, TO, END, OF, TRIÄÜ, I...   \n",
       "2  [[Natural, Childbirth, Sir,-We, nurses, have, ...   \n",
       "3  [[FIRST, CHURCH, I, SERVICE, 1, Presbyterian, ...   \n",
       "4  [[\", Bob, \", Lulham, 's, Fight, Against, Thall...   \n",
       "5  [[DIVORCE, Before, The, Judge, In, Divorce, ,,...   \n",
       "\n",
       "                                     ocr_dict_lookup  \n",
       "1           [[], [], [], [], [], [], [], [], [], []]  \n",
       "2                       [[], [], [], [], [], [], []]  \n",
       "3                                           [[], []]  \n",
       "4  [[], [], [], [], [], [], [], [], [], [], [], [...  \n",
       "5                                           [[], []]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentence = pd.read_pickle(\"./db_trove_sentencizer.pkl\")\n",
    "db_sentence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef cleanup(myrow, colname=\"corrected\"):\\n    # remove all # and @§\\n    \\n    corpus = [re.sub(r\\'#\\', \\'\\', element, flags=re.IGNORECASE) for element in corpus]\\n    corpus = [re.sub(r\\'@\\', \\'\\', element, flags=re.IGNORECASE) for element in corpus]\\n    \\n    # --- remove 2 or more .\\n    corpus = [re.sub(\\'[.]{2,}\\', \\'.\\', element) for element in corpus]\\n    # --- add a space before and after a list of punctuations\\n    corpus = [re.sub(r\"([.,!?:;\"\\'])\", r\" \\x01 \", element) for element in corpus]\\n    # --- remove everything except:\\n    #corpus = [re.sub(r\"([^a-zA-Z\\\\-.:;,!?\\\\d+]+)\", r\" \", element) for element in corpus]\\n    corpus = [re.sub(r\"([^a-zA-Z\\\\d+]+)\", r\" \", element) for element in corpus]\\n    # --- replace numbers with <NUM>\\n    corpus = [re.sub(r\\'\\x08\\\\d+\\x08\\', \\'<NUM>\\', element) for element in corpus]\\n    corpus = [re.sub(\\'--\\', \\'\\', element) for element in corpus]\\n    # --- normalize white spaces\\n    corpus = [re.sub(\\'\\\\s+\\', \\' \\', element) for element in corpus]\\n    \\n    # remove multiple spaces\\n    corpus = [re.sub(r\\'\\\\s+\\', \\' \\', element, flags=re.IGNORECASE) for element in corpus]\\n    corpus = [element.strip() for element in corpus]\\n    #corpus = [element.lower() for element in corpus]\\n    return corpus\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def cleanup(myrow, colname=\"corrected\"):\n",
    "    # remove all # and @§\n",
    "    \n",
    "    corpus = [re.sub(r'#', '', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [re.sub(r'@', '', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    \n",
    "    # --- remove 2 or more .\n",
    "    corpus = [re.sub('[.]{2,}', '.', element) for element in corpus]\n",
    "    # --- add a space before and after a list of punctuations\n",
    "    corpus = [re.sub(r\"([.,!?:;\\\"\\'])\", r\" \\1 \", element) for element in corpus]\n",
    "    # --- remove everything except:\n",
    "    #corpus = [re.sub(r\"([^a-zA-Z\\-.:;,!?\\d+]+)\", r\" \", element) for element in corpus]\n",
    "    corpus = [re.sub(r\"([^a-zA-Z\\d+]+)\", r\" \", element) for element in corpus]\n",
    "    # --- replace numbers with <NUM>\n",
    "    corpus = [re.sub(r'\\b\\d+\\b', '<NUM>', element) for element in corpus]\n",
    "    corpus = [re.sub('--', '', element) for element in corpus]\n",
    "    # --- normalize white spaces\n",
    "    corpus = [re.sub('\\s+', ' ', element) for element in corpus]\n",
    "    \n",
    "    # remove multiple spaces\n",
    "    corpus = [re.sub(r'\\s+', ' ', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [element.strip() for element in corpus]\n",
    "    #corpus = [element.lower() for element in corpus]\n",
    "    return corpus\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(myrow, col_name):\n",
    "    all_clean_rows = []\n",
    "    for sent in myrow[col_name]:\n",
    "        one_clean_row = []\n",
    "        for token in sent:\n",
    "            one_clean_row.append(token.lower())\n",
    "        all_clean_rows.append(one_clean_row)\n",
    "    return all_clean_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_sentence[\"ocr_sentencizer_cleaned\"] = db_sentence.apply(cleanup, args=[\"ocr_sentencizer\"], axis=1)\n",
    "db_sentence[\"corrected_sentencizer_cleaned\"] = db_sentence.apply(cleanup, args=[\"corrected_sentencizer\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update a pre-trained LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args for Word2Vec\n",
    "w2v_args = Namespace(\n",
    "    epochs=1, \n",
    "    # only for Word2Vec\n",
    "    compute_loss=True,                               # If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "\n",
    "#     size=100,                                        # Dimensionality of the word vectors.\n",
    "#     alpha=0.03,                                      # The initial learning rate.\n",
    "#     min_alpha=0.0007,                                # Learning rate will linearly drop to min_alpha as training progresses.\n",
    "#     sg=1,                                            # Training algorithm: skip-gram if sg=1, otherwise CBOW.\n",
    "#     hs=0,                                            # If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
    "#     negative=20,                                     # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used. \n",
    "#     min_count=5,                                    # The model ignores all words with total frequency lower than this.\n",
    "#     window=5,                                        # The maximum distance between the current and predicted word within a sentence.\n",
    "#     sample=1e-3,                                     # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "#     workers=8, \n",
    "#     cbow_mean=1,                                     # If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "#     null_word=0,                                     # \n",
    "#     trim_rule=None,                                  # \n",
    "#     sorted_vocab=1,                                  # If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
    "#     batch_words=10000,                               # Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
    "    \n",
    "#     seed=1364,                                       # Seed for the random number generator.\n",
    "#     # only for FastText (compare to word2vec)\n",
    "#     #word_ngrams=1,                                   # If 1, uses enriches word vectors with subword(n-grams) information. If 0, this is equivalent to Word2Vec. \n",
    "#     #min_n=2,                                         # Minimum length of char n-grams to be used for training word representations.\n",
    "#     #max_n=15,                                        # Max length of char ngrams to be used for training word representations. Set max_n to be lesser than min_n to avoid char ngrams being used.\n",
    "#     #bucket=2000000                                  # Character ngrams are hashed into a fixed number of buckets, in order to limit the memory usage of the model. This option specifies the number of buckets used by the model.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw2v_model = Word2Vec(\\n                 size=w2v_args.size, \\n                 alpha=w2v_args.alpha,\\n                 min_alpha=w2v_args.min_alpha, \\n                 sg=w2v_args.sg, \\n                 hs=w2v_args.hs, \\n                 negative=w2v_args.negative, \\n                 iter=w2v_args.epochs, \\n                 min_count=w2v_args.min_count, \\n                 window=w2v_args.window, \\n                 sample=w2v_args.sample, \\n                 workers=w2v_args.workers, \\n                 cbow_mean=w2v_args.cbow_mean, \\n                 null_word=w2v_args.null_word, \\n                 trim_rule=w2v_args.trim_rule, \\n                 sorted_vocab=w2v_args.sorted_vocab, \\n                 batch_words=w2v_args.batch_words, \\n                 seed=w2v_args.seed, \\n                 compute_loss=w2v_args.compute_loss)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only if a new LM needs to be trained (from scratch)\n",
    "\"\"\"\n",
    "w2v_model = Word2Vec(\n",
    "                 size=w2v_args.size, \n",
    "                 alpha=w2v_args.alpha,\n",
    "                 min_alpha=w2v_args.min_alpha, \n",
    "                 sg=w2v_args.sg, \n",
    "                 hs=w2v_args.hs, \n",
    "                 negative=w2v_args.negative, \n",
    "                 iter=w2v_args.epochs, \n",
    "                 min_count=w2v_args.min_count, \n",
    "                 window=w2v_args.window, \n",
    "                 sample=w2v_args.sample, \n",
    "                 workers=w2v_args.workers, \n",
    "                 cbow_mean=w2v_args.cbow_mean, \n",
    "                 null_word=w2v_args.null_word, \n",
    "                 trim_rule=w2v_args.trim_rule, \n",
    "                 sorted_vocab=w2v_args.sorted_vocab, \n",
    "                 batch_words=w2v_args.batch_words, \n",
    "                 seed=w2v_args.seed, \n",
    "                 compute_loss=w2v_args.compute_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before creating/updating LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def preprocess4LM(myrow, col_name=\"ocrText_cleaned_tokenize\"):\n",
    "    txt = [token.lemma_ for token in nlp(myrow[col_name].lower())]\n",
    "    return txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "db_sentence[\"ocrText_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"ocrText_cleaned\"], axis=1)\n",
    "db_sentence[\"corrected_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"corrected_cleaned\"], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences: 30509\n"
     ]
    }
   ],
   "source": [
    "list_sentences = db_sentence[\"ocr_sentencizer_cleaned\"].to_list()\n",
    "print('#sentences: {}'.format(len(list_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_sentences = [val for sublist in list_sentences for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_ocr = copy.deepcopy(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_ocr.workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 17:26:03,481 : INFO : collecting all words and their counts\n",
      "2019-11-11 17:26:03,482 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-11 17:26:03,611 : INFO : PROGRESS: at sentence #10000, processed 525154 words, keeping 85319 word types\n",
      "2019-11-11 17:26:03,683 : INFO : PROGRESS: at sentence #20000, processed 856174 words, keeping 117320 word types\n",
      "2019-11-11 17:26:03,785 : INFO : PROGRESS: at sentence #30000, processed 1262312 words, keeping 157071 word types\n",
      "2019-11-11 17:26:03,880 : INFO : PROGRESS: at sentence #40000, processed 1640602 words, keeping 191468 word types\n",
      "2019-11-11 17:26:03,963 : INFO : PROGRESS: at sentence #50000, processed 1983065 words, keeping 219352 word types\n",
      "2019-11-11 17:26:04,058 : INFO : PROGRESS: at sentence #60000, processed 2332908 words, keeping 244407 word types\n",
      "2019-11-11 17:26:04,146 : INFO : PROGRESS: at sentence #70000, processed 2683098 words, keeping 272037 word types\n",
      "2019-11-11 17:26:04,240 : INFO : PROGRESS: at sentence #80000, processed 3061759 words, keeping 300138 word types\n",
      "2019-11-11 17:26:04,325 : INFO : PROGRESS: at sentence #90000, processed 3400806 words, keeping 325517 word types\n",
      "2019-11-11 17:26:04,457 : INFO : PROGRESS: at sentence #100000, processed 3888563 words, keeping 363271 word types\n",
      "2019-11-11 17:26:04,561 : INFO : PROGRESS: at sentence #110000, processed 4305098 words, keeping 391802 word types\n",
      "2019-11-11 17:26:04,647 : INFO : PROGRESS: at sentence #120000, processed 4663026 words, keeping 415432 word types\n",
      "2019-11-11 17:26:04,785 : INFO : PROGRESS: at sentence #130000, processed 5214928 words, keeping 457311 word types\n",
      "2019-11-11 17:26:04,880 : INFO : PROGRESS: at sentence #140000, processed 5590287 words, keeping 482760 word types\n",
      "2019-11-11 17:26:05,019 : INFO : PROGRESS: at sentence #150000, processed 6128230 words, keeping 522484 word types\n",
      "2019-11-11 17:26:05,133 : INFO : PROGRESS: at sentence #160000, processed 6545889 words, keeping 548178 word types\n",
      "2019-11-11 17:26:05,231 : INFO : PROGRESS: at sentence #170000, processed 6934913 words, keeping 571470 word types\n",
      "2019-11-11 17:26:05,351 : INFO : PROGRESS: at sentence #180000, processed 7400878 words, keeping 603179 word types\n",
      "2019-11-11 17:26:05,468 : INFO : PROGRESS: at sentence #190000, processed 7822628 words, keeping 627282 word types\n",
      "2019-11-11 17:26:05,616 : INFO : PROGRESS: at sentence #200000, processed 8351540 words, keeping 660582 word types\n",
      "2019-11-11 17:26:05,715 : INFO : PROGRESS: at sentence #210000, processed 8741601 words, keeping 682611 word types\n",
      "2019-11-11 17:26:05,860 : INFO : PROGRESS: at sentence #220000, processed 9130858 words, keeping 706628 word types\n",
      "2019-11-11 17:26:05,946 : INFO : PROGRESS: at sentence #230000, processed 9470776 words, keeping 725143 word types\n",
      "2019-11-11 17:26:06,050 : INFO : PROGRESS: at sentence #240000, processed 9871436 words, keeping 749265 word types\n",
      "2019-11-11 17:26:06,179 : INFO : PROGRESS: at sentence #250000, processed 10329460 words, keeping 777098 word types\n",
      "2019-11-11 17:26:06,288 : INFO : PROGRESS: at sentence #260000, processed 10738641 words, keeping 801955 word types\n",
      "2019-11-11 17:26:06,368 : INFO : PROGRESS: at sentence #270000, processed 11043909 words, keeping 820773 word types\n",
      "2019-11-11 17:26:06,479 : INFO : PROGRESS: at sentence #280000, processed 11476758 words, keeping 848391 word types\n",
      "2019-11-11 17:26:06,627 : INFO : PROGRESS: at sentence #290000, processed 12093328 words, keeping 889865 word types\n",
      "2019-11-11 17:26:06,687 : INFO : collected 902618 word types from a corpus of 12356836 raw words and 297582 sentences\n",
      "2019-11-11 17:26:06,688 : INFO : Updating model with new vocabulary\n",
      "2019-11-11 17:26:07,028 : INFO : New added 13224 unique words (1% of original 915842) and increased the count of 13224 pre-existing words (1% of original 915842)\n",
      "2019-11-11 17:26:07,112 : INFO : deleting the raw counts dictionary of 902618 items\n",
      "2019-11-11 17:26:07,133 : INFO : sample=0.001 downsamples 84 most-common words\n",
      "2019-11-11 17:26:07,134 : INFO : downsampling leaves estimated 14909125 word corpus (141.4% of prior 10542323)\n",
      "2019-11-11 17:26:08,274 : INFO : estimated required memory for 26448 words and 300 dimensions: 76699200 bytes\n",
      "2019-11-11 17:26:08,274 : INFO : updating layer weights\n",
      "2019-11-11 17:26:09,140 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-11 17:26:09,141 : INFO : training model with 8 workers on 435091 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=20 window=5\n",
      "2019-11-11 17:26:10,354 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 94837 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:11,507 : INFO : EPOCH 1 - PROGRESS: at 0.00% examples, 116676 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:12,693 : INFO : EPOCH 1 - PROGRESS: at 0.01% examples, 122743 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:13,889 : INFO : EPOCH 1 - PROGRESS: at 0.01% examples, 123995 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-11 17:26:14,907 : INFO : EPOCH 1 - PROGRESS: at 0.01% examples, 124315 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:15,909 : INFO : EPOCH 1 - PROGRESS: at 0.02% examples, 125780 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:16,924 : INFO : EPOCH 1 - PROGRESS: at 0.02% examples, 128160 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:17,944 : INFO : EPOCH 1 - PROGRESS: at 0.02% examples, 130017 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:18,961 : INFO : EPOCH 1 - PROGRESS: at 0.03% examples, 130877 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:20,055 : INFO : EPOCH 1 - PROGRESS: at 0.03% examples, 131634 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-11 17:26:21,071 : INFO : EPOCH 1 - PROGRESS: at 0.03% examples, 132687 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-11 17:26:22,142 : INFO : EPOCH 1 - PROGRESS: at 0.04% examples, 133953 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-11 17:26:23,210 : INFO : EPOCH 1 - PROGRESS: at 0.04% examples, 134743 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:24,217 : INFO : EPOCH 1 - PROGRESS: at 0.05% examples, 135738 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-11 17:26:25,285 : INFO : EPOCH 1 - PROGRESS: at 0.05% examples, 135999 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:26,289 : INFO : EPOCH 1 - PROGRESS: at 0.05% examples, 134905 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:27,383 : INFO : EPOCH 1 - PROGRESS: at 0.05% examples, 134948 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:28,413 : INFO : EPOCH 1 - PROGRESS: at 0.06% examples, 135520 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:29,414 : INFO : EPOCH 1 - PROGRESS: at 0.06% examples, 134982 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:30,435 : INFO : EPOCH 1 - PROGRESS: at 0.06% examples, 135418 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:31,502 : INFO : EPOCH 1 - PROGRESS: at 0.07% examples, 135679 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-11 17:26:32,565 : INFO : EPOCH 1 - PROGRESS: at 0.07% examples, 135784 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-11 17:26:33,607 : INFO : EPOCH 1 - PROGRESS: at 0.07% examples, 135285 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-11 17:26:34,614 : INFO : EPOCH 1 - PROGRESS: at 0.07% examples, 135821 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:35,619 : INFO : EPOCH 1 - PROGRESS: at 0.08% examples, 135464 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-11 17:26:36,622 : INFO : EPOCH 1 - PROGRESS: at 0.08% examples, 135565 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:37,659 : INFO : EPOCH 1 - PROGRESS: at 0.08% examples, 135664 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-11 17:26:38,700 : INFO : EPOCH 1 - PROGRESS: at 0.08% examples, 135303 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:39,784 : INFO : EPOCH 1 - PROGRESS: at 0.09% examples, 135241 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-11 17:26:40,986 : INFO : EPOCH 1 - PROGRESS: at 0.09% examples, 135137 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:42,101 : INFO : EPOCH 1 - PROGRESS: at 0.09% examples, 135266 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:43,162 : INFO : EPOCH 1 - PROGRESS: at 0.10% examples, 135073 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-11 17:26:44,176 : INFO : EPOCH 1 - PROGRESS: at 0.10% examples, 134580 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-11 17:26:45,216 : INFO : EPOCH 1 - PROGRESS: at 0.10% examples, 133840 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8000ba3a34ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw2v_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                           compute_loss=w2v_args.compute_loss)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37torch/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_model_ocr.build_vocab(flattened_list_sentences, update=True)\n",
    "embedding_model_ocr.train(flattened_list_sentences, \n",
    "                          total_examples=embedding_model.corpus_count,\n",
    "                          epochs=w2v_args.epochs,  \n",
    "                          compute_loss=w2v_args.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences = db_sentence[\"corrected_sentencizer_cleaned\"].to_list()\n",
    "print('#sentences: {}'.format(len(list_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_sentences = [val for sublist in list_sentences for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_corrected = copy.deepcopy(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_ocr.workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_corrected.build_vocab(flattened_list_sentences, update=True)\n",
    "embedding_model_corrected.train(flattened_list_sentences, \n",
    "                                total_examples=embedding_model.corpus_count,\n",
    "                                epochs=w2v_args.epochs,  \n",
    "                                compute_loss=w2v_args.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_orig.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.wv.most_similar(\"oldham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word error rates and dictionary lookup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a spacy model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "spacy_dict = list(nlp.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_lookup(corpus_series, nlp_tool=\"spacy\", use_lemma=False):    \n",
    "    fulltxt_sent_list = []\n",
    "    fulltxt_found_dict = []\n",
    "    counter = 0\n",
    "    for corpus in corpus_series:\n",
    "        sent_list_tmp = []\n",
    "        found_dict_tmp = []\n",
    "        \n",
    "        print(counter)\n",
    "        corpus_spacy = nlp(corpus)\n",
    "        if nlp_tool == \"spacy\":\n",
    "            for sentence in list(corpus_spacy.sents):\n",
    "                if use_lemma:\n",
    "                    # --- lemmatization WITH punctuation and stop wors\n",
    "                    txt = [token.lemma_ for token in sentence]\n",
    "                    txt_dict = [str(len(token.text)) if str(token.lemma_) in spacy_dict else str(-len(token.text)) for token in sentence]\n",
    "                    # --- lemmatization, remove punctuation and stop wors\n",
    "                    #txt = [token.lemma_ for token in sentence if not token.is_punct | token.is_stop]\n",
    "                    #txt_pos = [token.pos_ for token in sentence if not token.is_punct | token.is_stop]\n",
    "                else:\n",
    "                    txt = [token.text for token in sentence]\n",
    "                    txt_dict = [str(len(token.text)) if str(token.text) in spacy_dict else str(-len(token.text)) for token in sentence]\n",
    "\n",
    "                sent_list_tmp.append(txt)\n",
    "                found_dict_tmp.append(txt_dict)\n",
    "        fulltxt_sent_list.append(sent_list_tmp)\n",
    "        fulltxt_found_dict.append(found_dict_tmp)\n",
    "        counter += 1\n",
    "    return fulltxt_sent_list, fulltxt_found_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltxt_sent_list, fulltxt_found_dict = dictionary_lookup(db[\"GS_cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"sent_GS_cleaned\"] = fulltxt_sent_list\n",
    "db[\"dict_GS_cleaned\"] = fulltxt_found_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"sent_GS_cleaned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"dict_GS_cleaned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
