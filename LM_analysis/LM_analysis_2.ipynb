{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 10:28:33,829 : INFO : loading Word2Vec object from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\n",
      "2019-11-20 10:28:35,867 : INFO : loading wv recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.wv.* with mmap=None\n",
      "2019-11-20 10:28:35,869 : INFO : loading vectors from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.wv.vectors.npy with mmap=None\n",
      "2019-11-20 10:28:36,419 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-20 10:28:36,421 : INFO : loading vocabulary recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.vocabulary.* with mmap=None\n",
      "2019-11-20 10:28:36,422 : INFO : loading trainables recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.trainables.* with mmap=None\n",
      "2019-11-20 10:28:36,423 : INFO : loading syn1neg from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-20 10:28:36,911 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-20 10:28:36,913 : INFO : loaded /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\n",
      "2019-11-20 10:28:38,571 : INFO : loading Word2Vec object from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_1_2.model\n",
      "2019-11-20 10:28:40,596 : INFO : loading wv recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_1_2.model.wv.* with mmap=None\n",
      "2019-11-20 10:28:40,597 : INFO : loading vectors from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_1_2.model.wv.vectors.npy with mmap=None\n",
      "2019-11-20 10:28:41,088 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-20 10:28:41,090 : INFO : loading vocabulary recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_1_2.model.vocabulary.* with mmap=None\n",
      "2019-11-20 10:28:41,091 : INFO : loading trainables recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_1_2.model.trainables.* with mmap=None\n",
      "2019-11-20 10:28:41,093 : INFO : loading syn1neg from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_1_2.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-20 10:28:41,630 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-20 10:28:41,632 : INFO : loaded /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_1_2.model\n",
      "2019-11-20 10:28:43,255 : INFO : loading Word2Vec object from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_1_2.model\n",
      "2019-11-20 10:28:44,936 : INFO : loading wv recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_1_2.model.wv.* with mmap=None\n",
      "2019-11-20 10:28:44,938 : INFO : loading vectors from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_1_2.model.wv.vectors.npy with mmap=None\n",
      "2019-11-20 10:28:45,495 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-20 10:28:45,497 : INFO : loading vocabulary recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_1_2.model.vocabulary.* with mmap=None\n",
      "2019-11-20 10:28:45,499 : INFO : loading trainables recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_1_2.model.trainables.* with mmap=None\n",
      "2019-11-20 10:28:45,502 : INFO : loading syn1neg from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_1_2.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-20 10:28:46,098 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-20 10:28:46,099 : INFO : loaded /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_1_2.model\n",
      "2019-11-20 10:28:48,108 : INFO : loading Word2Vec object from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_3_4.model\n",
      "2019-11-20 10:28:50,990 : INFO : loading wv recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_3_4.model.wv.* with mmap=None\n",
      "2019-11-20 10:28:50,992 : INFO : loading vectors from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_3_4.model.wv.vectors.npy with mmap=None\n",
      "2019-11-20 10:28:51,590 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-20 10:28:51,592 : INFO : loading vocabulary recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_3_4.model.vocabulary.* with mmap=None\n",
      "2019-11-20 10:28:51,594 : INFO : loading trainables recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_3_4.model.trainables.* with mmap=None\n",
      "2019-11-20 10:28:51,595 : INFO : loading syn1neg from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_3_4.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-20 10:28:52,093 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-20 10:28:52,094 : INFO : loaded /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_3_4.model\n",
      "2019-11-20 10:28:53,818 : INFO : loading Word2Vec object from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_3_4.model\n",
      "2019-11-20 10:28:55,931 : INFO : loading wv recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_3_4.model.wv.* with mmap=None\n",
      "2019-11-20 10:28:55,933 : INFO : loading vectors from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_3_4.model.wv.vectors.npy with mmap=None\n",
      "2019-11-20 10:28:56,418 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-20 10:28:56,420 : INFO : loading vocabulary recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_3_4.model.vocabulary.* with mmap=None\n",
      "2019-11-20 10:28:56,421 : INFO : loading trainables recursively from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_3_4.model.trainables.* with mmap=None\n",
      "2019-11-20 10:28:56,422 : INFO : loading syn1neg from /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_3_4.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-20 10:28:56,911 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-20 10:28:56,913 : INFO : loaded /Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_3_4.model\n"
     ]
    }
   ],
   "source": [
    "# embedding models, base model\n",
    "model_path = \"/Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\"\n",
    "w2v = Word2Vec.load(model_path)\n",
    "\n",
    "# OCR model, quality 1, 2\n",
    "model_path = \"/Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_1_2.model\"\n",
    "w2v_em_ocr_qual_1_2 = Word2Vec.load(model_path)\n",
    "\n",
    "# corrected model, quality 1, 2\n",
    "model_path = \"/Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_1_2.model\"\n",
    "w2v_em_corr_qual_1_2 = Word2Vec.load(model_path)\n",
    "\n",
    "# OCR model, quality 3, 4\n",
    "model_path = \"/Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_ocr_qual_3_4.model\"\n",
    "w2v_em_ocr_qual_3_4 = Word2Vec.load(model_path)\n",
    "\n",
    "# corrected model, quality 3, 4\n",
    "model_path = \"/Users/khosseini/myJobs/ATI/Projects/2019/lwm_ocr_assessment/LMs/w2v_005_EM_corr_qual_3_4.model\"\n",
    "w2v_em_corr_qual_3_4 = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def found_neighbors(myrow, embedding, colname='vocab', topn=1):\n",
    "    try:\n",
    "        vocab_neigh = embedding.wv.most_similar([myrow['vocab']], topn=topn)\n",
    "        return list(np.array(vocab_neigh)[:, 0])\n",
    "    except KeyError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_df(myrow, colname_1, colname_2, make_lowercase=True):\n",
    "    \"\"\"\n",
    "    Jaccard similarity between two documents (e.g., OCR and Human) on flattened list of words\n",
    "    \"\"\"\n",
    "    list1 = myrow[colname_1]\n",
    "    list2 = myrow[colname_2]\n",
    "    if make_lowercase:\n",
    "        list1 = [x.lower() for x in list1]\n",
    "        list2 = [x.lower() for x in list2]\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality bands 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list of words and their frequencies in the corrected set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_corrected = []\n",
    "for item in w2v_em_corr_qual_3_4.wv.vocab:\n",
    "    words_corrected.append([item, int(w2v_em_corr_qual_3_4.wv.vocab[item].count)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_words = pd.DataFrame(words_corrected, columns=['vocab', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_words = pd_words.sort_values(by=['count'], ascending=False)\n",
    "print(\"size: {}\".format(len(pd_words)))\n",
    "pd_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd2search = pd_words[0:5000]\n",
    "pd2search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_jaccard_bands_3_4 = []\n",
    "\n",
    "for topn in [1, 2, 5, 10, 50, 100, 500, 1000, 5000, 10000, 50000]:\n",
    "    print(\"topn: {}\".format(topn))\n",
    "    t1 = time.time()\n",
    "    \n",
    "    pd2search = pd_words[0:1000]\n",
    "    pd2search['w2v_em_corr_qual_3_4'] = pd2search.apply(found_neighbors, args=[w2v_em_corr_qual_3_4, \n",
    "                                                                               'vocab', \n",
    "                                                                               topn], axis=1)\n",
    "    print(\"corr: {}\".format(time.time() - t1))\n",
    "    pd2search['w2v_em_ocr_qual_3_4'] = pd2search.apply(found_neighbors, args=[w2v_em_ocr_qual_3_4, \n",
    "                                                                             'vocab', \n",
    "                                                                              topn], axis=1)\n",
    "    pd2search['jaccard_qual_3_4'] = \\\n",
    "        pd2search.apply(jaccard_similarity_df, args=['w2v_em_corr_qual_3_4', \n",
    "                                                     \"w2v_em_ocr_qual_3_4\", \n",
    "                                                     True], \n",
    "                        axis=1)\n",
    "    \n",
    "    neigh_jaccard_bands_3_4.append(\n",
    "        [topn, \n",
    "         pd2search['jaccard_qual_3_4'].mean(), \n",
    "         pd2search['jaccard_qual_3_4'].std()])\n",
    "    \n",
    "    print(\"total: {}\".format(time.time() - t1))\n",
    "\n",
    "neigh_jaccard_bands_3_4 = np.array(neigh_jaccard_bands_3_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality bands 1, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list of words and their frequencies in the corrected set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_corrected = []\n",
    "for item in w2v_em_corr_qual_1_2.wv.vocab:\n",
    "    words_corrected.append([item, int(w2v_em_corr_qual_1_2.wv.vocab[item].count)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_words = pd.DataFrame(words_corrected, columns=['vocab', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 439314\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>the</td>\n",
       "      <td>292602038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>of</td>\n",
       "      <td>174510683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>.</td>\n",
       "      <td>168101519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>and</td>\n",
       "      <td>139819513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>to</td>\n",
       "      <td>108989625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vocab      count\n",
       "55    the  292602038\n",
       "113    of  174510683\n",
       "11      .  168101519\n",
       "39    and  139819513\n",
       "106    to  108989625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_words = pd_words.sort_values(by=['count'], ascending=False)\n",
    "print(\"size: {}\".format(len(pd_words)))\n",
    "pd_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>the</td>\n",
       "      <td>292602038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>of</td>\n",
       "      <td>174510683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>.</td>\n",
       "      <td>168101519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>and</td>\n",
       "      <td>139819513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>to</td>\n",
       "      <td>108989625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5006</td>\n",
       "      <td>subjected</td>\n",
       "      <td>69788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2045</td>\n",
       "      <td>occupies</td>\n",
       "      <td>69787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2339</td>\n",
       "      <td>gravely</td>\n",
       "      <td>69777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5444</td>\n",
       "      <td>attendants</td>\n",
       "      <td>69770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8140</td>\n",
       "      <td>smallest</td>\n",
       "      <td>69766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           vocab      count\n",
       "55           the  292602038\n",
       "113           of  174510683\n",
       "11             .  168101519\n",
       "39           and  139819513\n",
       "106           to  108989625\n",
       "...          ...        ...\n",
       "5006   subjected      69788\n",
       "2045    occupies      69787\n",
       "2339     gravely      69777\n",
       "5444  attendants      69770\n",
       "8140    smallest      69766\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd2search = pd_words[0:5000]\n",
    "pd2search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 10:29:02,489 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topn: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "2019-11-20 10:29:54,745 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr: 52.2600359916687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 105.23348593711853\n",
      "topn: 2\n",
      "corr: 49.01822900772095\n",
      "total: 98.79225397109985\n",
      "topn: 5\n",
      "corr: 48.63156270980835\n",
      "total: 96.51067972183228\n",
      "topn: 10\n",
      "corr: 46.37088894844055\n",
      "total: 98.53879714012146\n",
      "topn: 50\n",
      "corr: 43.56462907791138\n",
      "total: 88.98045206069946\n",
      "topn: 100\n",
      "corr: 35.471516132354736\n",
      "total: 72.67836213111877\n",
      "topn: 500\n",
      "corr: 38.1435272693634\n",
      "total: 79.2555341720581\n",
      "topn: 1000\n",
      "corr: 39.5123016834259\n",
      "total: 82.0018618106842\n",
      "topn: 5000\n",
      "corr: 63.327855825424194\n",
      "total: 126.80228090286255\n",
      "topn: 10000\n",
      "corr: 77.40446305274963\n",
      "total: 159.26662611961365\n",
      "topn: 50000\n",
      "corr: 249.6574649810791\n",
      "total: 534.9824228286743\n"
     ]
    }
   ],
   "source": [
    "neigh_jaccard_bands_1_2 = []\n",
    "\n",
    "for topn in [1, 2, 5, 10, 50, 100, 500, 1000, 5000, 10000, 50000]:\n",
    "    print(\"topn: {}\".format(topn))\n",
    "    t1 = time.time()\n",
    "    \n",
    "    pd2search = pd_words[0:1000]\n",
    "    pd2search['w2v_em_corr_qual_1_2'] = pd2search.apply(found_neighbors, args=[w2v_em_corr_qual_1_2, \n",
    "                                                                               'vocab', \n",
    "                                                                               topn], axis=1)\n",
    "    print(\"corr: {}\".format(time.time() - t1))\n",
    "    pd2search['w2v_em_ocr_qual_1_2'] = pd2search.apply(found_neighbors, args=[w2v_em_ocr_qual_1_2, \n",
    "                                                                             'vocab', \n",
    "                                                                              topn], axis=1)\n",
    "    pd2search['jaccard_qual_1_2'] = \\\n",
    "        pd2search.apply(jaccard_similarity_df, args=['w2v_em_corr_qual_1_2', \n",
    "                                                     \"w2v_em_ocr_qual_1_2\", \n",
    "                                                     True], \n",
    "                        axis=1)\n",
    "    \n",
    "    neigh_jaccard_bands_1_2.append(\n",
    "        [topn, \n",
    "         pd2search['jaccard_qual_1_2'].mean(), \n",
    "         pd2search['jaccard_qual_1_2'].std()])\n",
    "    \n",
    "    print(\"total: {}\".format(time.time() - t1))\n",
    "\n",
    "neigh_jaccard_bands_1_2 = np.array(neigh_jaccard_bands_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"neigh_jaccard_bands_1_2.npy\", neigh_jaccard_bands_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "\n",
    "plt.plot(neigh_jaccard_bands_1_2[:, 0], neigh_jaccard_bands_1_2[:, 1], \n",
    "         'k-o', alpha=1.0, \n",
    "         lw=4,\n",
    "         label='Quality bands=1,2')\n",
    "\n",
    "plt.plot(neigh_jaccard_bands_3_4[:, 0], neigh_jaccard_bands_3_4[:, 1], \n",
    "         'r-o', alpha=1.0, \n",
    "         lw=4,\n",
    "         label='Quality bands=3,4')\n",
    "\n",
    "plt.grid()\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=20)\n",
    "plt.xlabel(\"#neighbours\", size=24)\n",
    "plt.ylabel(\"Jaccard similarity\", size=24)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(1.0, 100000)\n",
    "plt.ylim(0.05, 1.0)\n",
    "\n",
    "plt.legend(prop={'size': 20})\n",
    "plt.show()\n",
    "#plt.xlim(0, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
