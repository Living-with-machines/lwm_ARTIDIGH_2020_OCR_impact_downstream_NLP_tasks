{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "from time import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:20:52,898 : INFO : loading Word2Vec object from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\n",
      "2019-11-13 13:20:55,374 : INFO : loading wv recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.wv.* with mmap=None\n",
      "2019-11-13 13:20:55,378 : INFO : loading vectors from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.wv.vectors.npy with mmap=None\n",
      "2019-11-13 13:20:56,400 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-13 13:20:56,403 : INFO : loading vocabulary recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.vocabulary.* with mmap=None\n",
      "2019-11-13 13:20:56,404 : INFO : loading trainables recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.trainables.* with mmap=None\n",
      "2019-11-13 13:20:56,405 : INFO : loading syn1neg from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-13 13:20:58,854 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-13 13:20:58,866 : INFO : loaded /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=434049, size=300, alpha=0.03)\n"
     ]
    }
   ],
   "source": [
    "# model types\n",
    "# w2v: word2vec\n",
    "# ft: fasttext\n",
    "model_type = \"w2v\"   \n",
    "model_path = \"/Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\"\n",
    "\n",
    "if model_type.lower() in [\"w2v\", \"word2vec\"]:\n",
    "    # Word2Vec\n",
    "    embedding_model = Word2Vec.load(model_path)\n",
    "elif model_type.lower() in [\"ft\", \"fasttext\"]:\n",
    "    # FastText\n",
    "    embedding_model = FastText.load(model_path)\n",
    "print(embedding_model)\n",
    "embedding_model_orig = copy.deepcopy(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>articleId</th>\n",
       "      <th>articleType</th>\n",
       "      <th>year</th>\n",
       "      <th>ocrText</th>\n",
       "      <th>humanText</th>\n",
       "      <th>corrected</th>\n",
       "      <th>str_similarity</th>\n",
       "      <th>str_length_humanText</th>\n",
       "      <th>str_length_ocrText</th>\n",
       "      <th>quality_band</th>\n",
       "      <th>use_corrected</th>\n",
       "      <th>corrected_sentencizer</th>\n",
       "      <th>corrected_dict_lookup</th>\n",
       "      <th>ocr_sentencizer</th>\n",
       "      <th>ocr_dict_lookup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18378453</td>\n",
       "      <td>Article ILLUSTRATED</td>\n",
       "      <td>1953</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIÄÜ I ^PI A^H\"...</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>746</td>\n",
       "      <td>820</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[FROM, RIVER, CROSSING, TO, END, OF, TRIAL, S...</td>\n",
       "      <td>[[4, 5, 8, 2, 3, 2, 5, 6, 1, -5, -6, 8, 4, 4, ...</td>\n",
       "      <td>[[FROM, RIVER, CROSSING, TO, END, OF, TRIÄÜ, I...</td>\n",
       "      <td>[[4, 5, 8, 2, 3, 2, -5, 1, 1, 2, -3, 1, -5, -6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18363627</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>0.964119</td>\n",
       "      <td>641</td>\n",
       "      <td>630</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[Natural, Childbirth, Sir,-We, nurses, have, ...</td>\n",
       "      <td>[[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...</td>\n",
       "      <td>[[Natural, Childbirth, Sir,-We, nurses, have, ...</td>\n",
       "      <td>[[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18366055</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>FIRST CHURCH I SERVICE 1 Presbyterian I ' Anni...</td>\n",
       "      <td>FIRST CHURCH SERVICE Presbyterian Anniversary ...</td>\n",
       "      <td>FIRST CHURCH SERVICE Presbyterian Anniversary ...</td>\n",
       "      <td>0.738901</td>\n",
       "      <td>946</td>\n",
       "      <td>832</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[[FIRST, CHURCH, SERVICE, Presbyterian, Annive...</td>\n",
       "      <td>[[5, 6, 7, 12, 11, 3, 5, 11, 2, 3, 5, 12, 6, 7...</td>\n",
       "      <td>[[FIRST, CHURCH, I, SERVICE, 1, Presbyterian, ...</td>\n",
       "      <td>[[5, 6, 1, 7, 1, 12, 1, 1, 11, 1, 3, 5, -12, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18386137</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium District...</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium  Arthur ...</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium  Arthur ...</td>\n",
       "      <td>0.493898</td>\n",
       "      <td>2950</td>\n",
       "      <td>2740</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[[\", Bob, \", Lulham, 's, Fight, Against, Thall...</td>\n",
       "      <td>[[1, 3, 1, -6, 2, 5, 7, 8, 6, 6, 1, 1, 3, 1, 1...</td>\n",
       "      <td>[[\", Bob, \", Lulham, 's, Fight, Against, Thall...</td>\n",
       "      <td>[[1, 3, 1, -6, 2, 5, 7, 8, 8, 8, 1], [4, 5, 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18368961</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr Justic...</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr. Justi...</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr. Justi...</td>\n",
       "      <td>0.894176</td>\n",
       "      <td>1219</td>\n",
       "      <td>1121</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[DIVORCE, Before, The, Judge, In, Divorce, ,,...</td>\n",
       "      <td>[[7, 6, 3, 5, 2, 7, 1, 2, 1, 7, -5, 7, 4, 1, 1...</td>\n",
       "      <td>[[DIVORCE, Before, The, Judge, In, Divorce, ,,...</td>\n",
       "      <td>[[7, 6, 3, 5, 2, 7, 1, 2, 7, -5, 7, 4, 1, 1, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filePath articleId  \\\n",
       "1  ./trove_overproof/datasets/dataset1/rawTextAnd...  18378453   \n",
       "2  ./trove_overproof/datasets/dataset1/rawTextAnd...  18363627   \n",
       "3  ./trove_overproof/datasets/dataset1/rawTextAnd...  18366055   \n",
       "4  ./trove_overproof/datasets/dataset1/rawTextAnd...  18386137   \n",
       "5  ./trove_overproof/datasets/dataset1/rawTextAnd...  18368961   \n",
       "\n",
       "            articleType  year  \\\n",
       "1  Article ILLUSTRATED   1953   \n",
       "2               Article  1953   \n",
       "3               Article  1953   \n",
       "4               Article  1953   \n",
       "5               Article  1953   \n",
       "\n",
       "                                             ocrText  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIÄÜ I ^PI A^H\"...   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...   \n",
       "3  FIRST CHURCH I SERVICE 1 Presbyterian I ' Anni...   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium District...   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr Justic...   \n",
       "\n",
       "                                           humanText  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...   \n",
       "3  FIRST CHURCH SERVICE Presbyterian Anniversary ...   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium  Arthur ...   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr. Justi...   \n",
       "\n",
       "                                           corrected  str_similarity  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...        0.847561   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...        0.964119   \n",
       "3  FIRST CHURCH SERVICE Presbyterian Anniversary ...        0.738901   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium  Arthur ...        0.493898   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr. Justi...        0.894176   \n",
       "\n",
       "   str_length_humanText  str_length_ocrText  quality_band  use_corrected  \\\n",
       "1                   746                 820             2              0   \n",
       "2                   641                 630             1              0   \n",
       "3                   946                 832             3              0   \n",
       "4                  2950                2740             4              0   \n",
       "5                  1219                1121             2              0   \n",
       "\n",
       "                               corrected_sentencizer  \\\n",
       "1  [[FROM, RIVER, CROSSING, TO, END, OF, TRIAL, S...   \n",
       "2  [[Natural, Childbirth, Sir,-We, nurses, have, ...   \n",
       "3  [[FIRST, CHURCH, SERVICE, Presbyterian, Annive...   \n",
       "4  [[\", Bob, \", Lulham, 's, Fight, Against, Thall...   \n",
       "5  [[DIVORCE, Before, The, Judge, In, Divorce, ,,...   \n",
       "\n",
       "                               corrected_dict_lookup  \\\n",
       "1  [[4, 5, 8, 2, 3, 2, 5, 6, 1, -5, -6, 8, 4, 4, ...   \n",
       "2  [[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...   \n",
       "3  [[5, 6, 7, 12, 11, 3, 5, 11, 2, 3, 5, 12, 6, 7...   \n",
       "4  [[1, 3, 1, -6, 2, 5, 7, 8, 6, 6, 1, 1, 3, 1, 1...   \n",
       "5  [[7, 6, 3, 5, 2, 7, 1, 2, 1, 7, -5, 7, 4, 1, 1...   \n",
       "\n",
       "                                     ocr_sentencizer  \\\n",
       "1  [[FROM, RIVER, CROSSING, TO, END, OF, TRIÄÜ, I...   \n",
       "2  [[Natural, Childbirth, Sir,-We, nurses, have, ...   \n",
       "3  [[FIRST, CHURCH, I, SERVICE, 1, Presbyterian, ...   \n",
       "4  [[\", Bob, \", Lulham, 's, Fight, Against, Thall...   \n",
       "5  [[DIVORCE, Before, The, Judge, In, Divorce, ,,...   \n",
       "\n",
       "                                     ocr_dict_lookup  \n",
       "1  [[4, 5, 8, 2, 3, 2, -5, 1, 1, 2, -3, 1, -5, -6...  \n",
       "2  [[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...  \n",
       "3  [[5, 6, 1, 7, 1, 12, 1, 1, 11, 1, 3, 5, -12, 3...  \n",
       "4  [[1, 3, 1, -6, 2, 5, 7, 8, 8, 8, 1], [4, 5, 6,...  \n",
       "5  [[7, 6, 3, 5, 2, 7, 1, 2, 7, -5, 7, 4, 1, 1, -...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentence = pd.read_pickle(\"./db_trove_sentence_with_lookup.pkl\")\n",
    "db_sentence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef cleanup(myrow, colname=\"corrected\"):\\n    # remove all # and @§\\n    \\n    corpus = [re.sub(r\\'#\\', \\'\\', element, flags=re.IGNORECASE) for element in corpus]\\n    corpus = [re.sub(r\\'@\\', \\'\\', element, flags=re.IGNORECASE) for element in corpus]\\n    \\n    # --- remove 2 or more .\\n    corpus = [re.sub(\\'[.]{2,}\\', \\'.\\', element) for element in corpus]\\n    # --- add a space before and after a list of punctuations\\n    corpus = [re.sub(r\"([.,!?:;\"\\'])\", r\" \\x01 \", element) for element in corpus]\\n    # --- remove everything except:\\n    #corpus = [re.sub(r\"([^a-zA-Z\\\\-.:;,!?\\\\d+]+)\", r\" \", element) for element in corpus]\\n    corpus = [re.sub(r\"([^a-zA-Z\\\\d+]+)\", r\" \", element) for element in corpus]\\n    # --- replace numbers with <NUM>\\n    corpus = [re.sub(r\\'\\x08\\\\d+\\x08\\', \\'<NUM>\\', element) for element in corpus]\\n    corpus = [re.sub(\\'--\\', \\'\\', element) for element in corpus]\\n    # --- normalize white spaces\\n    corpus = [re.sub(\\'\\\\s+\\', \\' \\', element) for element in corpus]\\n    \\n    # remove multiple spaces\\n    corpus = [re.sub(r\\'\\\\s+\\', \\' \\', element, flags=re.IGNORECASE) for element in corpus]\\n    corpus = [element.strip() for element in corpus]\\n    #corpus = [element.lower() for element in corpus]\\n    return corpus\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def cleanup(myrow, colname=\"corrected\"):\n",
    "    # remove all # and @§\n",
    "    \n",
    "    corpus = [re.sub(r'#', '', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [re.sub(r'@', '', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    \n",
    "    # --- remove 2 or more .\n",
    "    corpus = [re.sub('[.]{2,}', '.', element) for element in corpus]\n",
    "    # --- add a space before and after a list of punctuations\n",
    "    corpus = [re.sub(r\"([.,!?:;\\\"\\'])\", r\" \\1 \", element) for element in corpus]\n",
    "    # --- remove everything except:\n",
    "    #corpus = [re.sub(r\"([^a-zA-Z\\-.:;,!?\\d+]+)\", r\" \", element) for element in corpus]\n",
    "    corpus = [re.sub(r\"([^a-zA-Z\\d+]+)\", r\" \", element) for element in corpus]\n",
    "    # --- replace numbers with <NUM>\n",
    "    corpus = [re.sub(r'\\b\\d+\\b', '<NUM>', element) for element in corpus]\n",
    "    corpus = [re.sub('--', '', element) for element in corpus]\n",
    "    # --- normalize white spaces\n",
    "    corpus = [re.sub('\\s+', ' ', element) for element in corpus]\n",
    "    \n",
    "    # remove multiple spaces\n",
    "    corpus = [re.sub(r'\\s+', ' ', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [element.strip() for element in corpus]\n",
    "    #corpus = [element.lower() for element in corpus]\n",
    "    return corpus\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(myrow, col_name):\n",
    "    all_clean_rows = []\n",
    "    for sent in myrow[col_name]:\n",
    "        one_clean_row = []\n",
    "        for token in sent:\n",
    "            one_clean_row.append(token.lower())\n",
    "        all_clean_rows.append(one_clean_row)\n",
    "    return all_clean_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_sentence[\"ocr_sentencizer_cleaned\"] = db_sentence.apply(cleanup, args=[\"ocr_sentencizer\"], axis=1)\n",
    "db_sentence[\"corrected_sentencizer_cleaned\"] = db_sentence.apply(cleanup, args=[\"corrected_sentencizer\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>articleId</th>\n",
       "      <th>articleType</th>\n",
       "      <th>year</th>\n",
       "      <th>ocrText</th>\n",
       "      <th>humanText</th>\n",
       "      <th>corrected</th>\n",
       "      <th>str_similarity</th>\n",
       "      <th>str_length_humanText</th>\n",
       "      <th>str_length_ocrText</th>\n",
       "      <th>quality_band</th>\n",
       "      <th>use_corrected</th>\n",
       "      <th>corrected_sentencizer</th>\n",
       "      <th>corrected_dict_lookup</th>\n",
       "      <th>ocr_sentencizer</th>\n",
       "      <th>ocr_dict_lookup</th>\n",
       "      <th>ocr_sentencizer_cleaned</th>\n",
       "      <th>corrected_sentencizer_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18378453</td>\n",
       "      <td>Article ILLUSTRATED</td>\n",
       "      <td>1953</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIÄÜ I ^PI A^H\"...</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>746</td>\n",
       "      <td>820</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[FROM, RIVER, CROSSING, TO, END, OF, TRIAL, S...</td>\n",
       "      <td>[[4, 5, 8, 2, 3, 2, 5, 6, 1, -5, -6, 8, 4, 4, ...</td>\n",
       "      <td>[[FROM, RIVER, CROSSING, TO, END, OF, TRIÄÜ, I...</td>\n",
       "      <td>[[4, 5, 8, 2, 3, 2, -5, 1, 1, 2, -3, 1, -5, -6...</td>\n",
       "      <td>[[from, river, crossing, to, end, of, triäü, i...</td>\n",
       "      <td>[[from, river, crossing, to, end, of, trial, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18363627</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>0.964119</td>\n",
       "      <td>641</td>\n",
       "      <td>630</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[[Natural, Childbirth, Sir,-We, nurses, have, ...</td>\n",
       "      <td>[[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...</td>\n",
       "      <td>[[Natural, Childbirth, Sir,-We, nurses, have, ...</td>\n",
       "      <td>[[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...</td>\n",
       "      <td>[[natural, childbirth, sir,-we, nurses, have, ...</td>\n",
       "      <td>[[natural, childbirth, sir,-we, nurses, have, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18366055</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>FIRST CHURCH I SERVICE 1 Presbyterian I ' Anni...</td>\n",
       "      <td>FIRST CHURCH SERVICE Presbyterian Anniversary ...</td>\n",
       "      <td>FIRST CHURCH SERVICE Presbyterian Anniversary ...</td>\n",
       "      <td>0.738901</td>\n",
       "      <td>946</td>\n",
       "      <td>832</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[[FIRST, CHURCH, SERVICE, Presbyterian, Annive...</td>\n",
       "      <td>[[5, 6, 7, 12, 11, 3, 5, 11, 2, 3, 5, 12, 6, 7...</td>\n",
       "      <td>[[FIRST, CHURCH, I, SERVICE, 1, Presbyterian, ...</td>\n",
       "      <td>[[5, 6, 1, 7, 1, 12, 1, 1, 11, 1, 3, 5, -12, 3...</td>\n",
       "      <td>[[first, church, i, service, 1, presbyterian, ...</td>\n",
       "      <td>[[first, church, service, presbyterian, annive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18386137</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium District...</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium  Arthur ...</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium  Arthur ...</td>\n",
       "      <td>0.493898</td>\n",
       "      <td>2950</td>\n",
       "      <td>2740</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[[\", Bob, \", Lulham, 's, Fight, Against, Thall...</td>\n",
       "      <td>[[1, 3, 1, -6, 2, 5, 7, 8, 6, 6, 1, 1, 3, 1, 1...</td>\n",
       "      <td>[[\", Bob, \", Lulham, 's, Fight, Against, Thall...</td>\n",
       "      <td>[[1, 3, 1, -6, 2, 5, 7, 8, 8, 8, 1], [4, 5, 6,...</td>\n",
       "      <td>[[\", bob, \", lulham, 's, fight, against, thall...</td>\n",
       "      <td>[[\", bob, \", lulham, 's, fight, against, thall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18368961</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr Justic...</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr. Justi...</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr. Justi...</td>\n",
       "      <td>0.894176</td>\n",
       "      <td>1219</td>\n",
       "      <td>1121</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[[DIVORCE, Before, The, Judge, In, Divorce, ,,...</td>\n",
       "      <td>[[7, 6, 3, 5, 2, 7, 1, 2, 1, 7, -5, 7, 4, 1, 1...</td>\n",
       "      <td>[[DIVORCE, Before, The, Judge, In, Divorce, ,,...</td>\n",
       "      <td>[[7, 6, 3, 5, 2, 7, 1, 2, 7, -5, 7, 4, 1, 1, -...</td>\n",
       "      <td>[[divorce, before, the, judge, in, divorce, ,,...</td>\n",
       "      <td>[[divorce, before, the, judge, in, divorce, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filePath articleId  \\\n",
       "1  ./trove_overproof/datasets/dataset1/rawTextAnd...  18378453   \n",
       "2  ./trove_overproof/datasets/dataset1/rawTextAnd...  18363627   \n",
       "3  ./trove_overproof/datasets/dataset1/rawTextAnd...  18366055   \n",
       "4  ./trove_overproof/datasets/dataset1/rawTextAnd...  18386137   \n",
       "5  ./trove_overproof/datasets/dataset1/rawTextAnd...  18368961   \n",
       "\n",
       "            articleType  year  \\\n",
       "1  Article ILLUSTRATED   1953   \n",
       "2               Article  1953   \n",
       "3               Article  1953   \n",
       "4               Article  1953   \n",
       "5               Article  1953   \n",
       "\n",
       "                                             ocrText  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIÄÜ I ^PI A^H\"...   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...   \n",
       "3  FIRST CHURCH I SERVICE 1 Presbyterian I ' Anni...   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium District...   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr Justic...   \n",
       "\n",
       "                                           humanText  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...   \n",
       "3  FIRST CHURCH SERVICE Presbyterian Anniversary ...   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium  Arthur ...   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr. Justi...   \n",
       "\n",
       "                                           corrected  str_similarity  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...        0.847561   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...        0.964119   \n",
       "3  FIRST CHURCH SERVICE Presbyterian Anniversary ...        0.738901   \n",
       "4  \"Bob\" Lulham's Fight Against Thallium  Arthur ...        0.493898   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr. Justi...        0.894176   \n",
       "\n",
       "   str_length_humanText  str_length_ocrText  quality_band  use_corrected  \\\n",
       "1                   746                 820             2              0   \n",
       "2                   641                 630             1              0   \n",
       "3                   946                 832             3              0   \n",
       "4                  2950                2740             4              0   \n",
       "5                  1219                1121             2              0   \n",
       "\n",
       "                               corrected_sentencizer  \\\n",
       "1  [[FROM, RIVER, CROSSING, TO, END, OF, TRIAL, S...   \n",
       "2  [[Natural, Childbirth, Sir,-We, nurses, have, ...   \n",
       "3  [[FIRST, CHURCH, SERVICE, Presbyterian, Annive...   \n",
       "4  [[\", Bob, \", Lulham, 's, Fight, Against, Thall...   \n",
       "5  [[DIVORCE, Before, The, Judge, In, Divorce, ,,...   \n",
       "\n",
       "                               corrected_dict_lookup  \\\n",
       "1  [[4, 5, 8, 2, 3, 2, 5, 6, 1, -5, -6, 8, 4, 4, ...   \n",
       "2  [[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...   \n",
       "3  [[5, 6, 7, 12, 11, 3, 5, 11, 2, 3, 5, 12, 6, 7...   \n",
       "4  [[1, 3, 1, -6, 2, 5, 7, 8, 6, 6, 1, 1, 3, 1, 1...   \n",
       "5  [[7, 6, 3, 5, 2, 7, 1, 2, 1, 7, -5, 7, 4, 1, 1...   \n",
       "\n",
       "                                     ocr_sentencizer  \\\n",
       "1  [[FROM, RIVER, CROSSING, TO, END, OF, TRIÄÜ, I...   \n",
       "2  [[Natural, Childbirth, Sir,-We, nurses, have, ...   \n",
       "3  [[FIRST, CHURCH, I, SERVICE, 1, Presbyterian, ...   \n",
       "4  [[\", Bob, \", Lulham, 's, Fight, Against, Thall...   \n",
       "5  [[DIVORCE, Before, The, Judge, In, Divorce, ,,...   \n",
       "\n",
       "                                     ocr_dict_lookup  \\\n",
       "1  [[4, 5, 8, 2, 3, 2, -5, 1, 1, 2, -3, 1, -5, -6...   \n",
       "2  [[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...   \n",
       "3  [[5, 6, 1, 7, 1, 12, 1, 1, 11, 1, 3, 5, -12, 3...   \n",
       "4  [[1, 3, 1, -6, 2, 5, 7, 8, 8, 8, 1], [4, 5, 6,...   \n",
       "5  [[7, 6, 3, 5, 2, 7, 1, 2, 7, -5, 7, 4, 1, 1, -...   \n",
       "\n",
       "                             ocr_sentencizer_cleaned  \\\n",
       "1  [[from, river, crossing, to, end, of, triäü, i...   \n",
       "2  [[natural, childbirth, sir,-we, nurses, have, ...   \n",
       "3  [[first, church, i, service, 1, presbyterian, ...   \n",
       "4  [[\", bob, \", lulham, 's, fight, against, thall...   \n",
       "5  [[divorce, before, the, judge, in, divorce, ,,...   \n",
       "\n",
       "                       corrected_sentencizer_cleaned  \n",
       "1  [[from, river, crossing, to, end, of, trial, s...  \n",
       "2  [[natural, childbirth, sir,-we, nurses, have, ...  \n",
       "3  [[first, church, service, presbyterian, annive...  \n",
       "4  [[\", bob, \", lulham, 's, fight, against, thall...  \n",
       "5  [[divorce, before, the, judge, in, divorce, ,,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update a pre-trained LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args for Word2Vec\n",
    "w2v_args = Namespace(\n",
    "    epochs=5, \n",
    "    # only for Word2Vec\n",
    "    compute_loss=True,                               # If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "\n",
    "#     size=100,                                        # Dimensionality of the word vectors.\n",
    "#     alpha=0.03,                                      # The initial learning rate.\n",
    "#     min_alpha=0.0007,                                # Learning rate will linearly drop to min_alpha as training progresses.\n",
    "#     sg=1,                                            # Training algorithm: skip-gram if sg=1, otherwise CBOW.\n",
    "#     hs=0,                                            # If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
    "#     negative=20,                                     # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used. \n",
    "#     min_count=5,                                    # The model ignores all words with total frequency lower than this.\n",
    "#     window=5,                                        # The maximum distance between the current and predicted word within a sentence.\n",
    "#     sample=1e-3,                                     # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "#     workers=8, \n",
    "#     cbow_mean=1,                                     # If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "#     null_word=0,                                     # \n",
    "#     trim_rule=None,                                  # \n",
    "#     sorted_vocab=1,                                  # If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
    "#     batch_words=10000,                               # Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
    "    \n",
    "#     seed=1364,                                       # Seed for the random number generator.\n",
    "#     # only for FastText (compare to word2vec)\n",
    "#     #word_ngrams=1,                                   # If 1, uses enriches word vectors with subword(n-grams) information. If 0, this is equivalent to Word2Vec. \n",
    "#     #min_n=2,                                         # Minimum length of char n-grams to be used for training word representations.\n",
    "#     #max_n=15,                                        # Max length of char ngrams to be used for training word representations. Set max_n to be lesser than min_n to avoid char ngrams being used.\n",
    "#     #bucket=2000000                                  # Character ngrams are hashed into a fixed number of buckets, in order to limit the memory usage of the model. This option specifies the number of buckets used by the model.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nw2v_model = Word2Vec(\\n                 size=w2v_args.size, \\n                 alpha=w2v_args.alpha,\\n                 min_alpha=w2v_args.min_alpha, \\n                 sg=w2v_args.sg, \\n                 hs=w2v_args.hs, \\n                 negative=w2v_args.negative, \\n                 iter=w2v_args.epochs, \\n                 min_count=w2v_args.min_count, \\n                 window=w2v_args.window, \\n                 sample=w2v_args.sample, \\n                 workers=w2v_args.workers, \\n                 cbow_mean=w2v_args.cbow_mean, \\n                 null_word=w2v_args.null_word, \\n                 trim_rule=w2v_args.trim_rule, \\n                 sorted_vocab=w2v_args.sorted_vocab, \\n                 batch_words=w2v_args.batch_words, \\n                 seed=w2v_args.seed, \\n                 compute_loss=w2v_args.compute_loss)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only if a new LM needs to be trained (from scratch)\n",
    "\"\"\"\n",
    "w2v_model = Word2Vec(\n",
    "                 size=w2v_args.size, \n",
    "                 alpha=w2v_args.alpha,\n",
    "                 min_alpha=w2v_args.min_alpha, \n",
    "                 sg=w2v_args.sg, \n",
    "                 hs=w2v_args.hs, \n",
    "                 negative=w2v_args.negative, \n",
    "                 iter=w2v_args.epochs, \n",
    "                 min_count=w2v_args.min_count, \n",
    "                 window=w2v_args.window, \n",
    "                 sample=w2v_args.sample, \n",
    "                 workers=w2v_args.workers, \n",
    "                 cbow_mean=w2v_args.cbow_mean, \n",
    "                 null_word=w2v_args.null_word, \n",
    "                 trim_rule=w2v_args.trim_rule, \n",
    "                 sorted_vocab=w2v_args.sorted_vocab, \n",
    "                 batch_words=w2v_args.batch_words, \n",
    "                 seed=w2v_args.seed, \n",
    "                 compute_loss=w2v_args.compute_loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before creating/updating LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef preprocess4LM(myrow, col_name=\"ocrText_cleaned_tokenize\"):\\n    txt = [token.lemma_ for token in nlp(myrow[col_name].lower())]\\n    return txt\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def preprocess4LM(myrow, col_name=\"ocrText_cleaned_tokenize\"):\n",
    "    txt = [token.lemma_ for token in nlp(myrow[col_name].lower())]\n",
    "    return txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndb_sentence[\"ocrText_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"ocrText_cleaned\"], axis=1)\\ndb_sentence[\"corrected_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"corrected_cleaned\"], axis=1)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "db_sentence[\"ocrText_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"ocrText_cleaned\"], axis=1)\n",
    "db_sentence[\"corrected_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"corrected_cleaned\"], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences: 30509\n"
     ]
    }
   ],
   "source": [
    "list_sentences = db_sentence[\"ocr_sentencizer_cleaned\"].to_list()\n",
    "print('#sentences: {}'.format(len(list_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_sentences = [val for sublist in list_sentences for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'river',\n",
       " 'crossing',\n",
       " 'to',\n",
       " 'end',\n",
       " 'of',\n",
       " 'triäü',\n",
       " 'i',\n",
       " '^',\n",
       " 'pi',\n",
       " 'a^h',\n",
       " '\"',\n",
       " 'pclcr',\n",
       " 'antill',\n",
       " 'ploughed',\n",
       " 'deep',\n",
       " 'into',\n",
       " 'paddy',\n",
       " \"'s\",\n",
       " 'river',\n",
       " 'in',\n",
       " 'his',\n",
       " 'chrysler',\n",
       " 'plymouth',\n",
       " 'jr',\n",
       " 'la',\n",
       " 'jil',\n",
       " '?',\n",
       " 'during',\n",
       " '{',\n",
       " '|',\n",
       " ')',\n",
       " 'c',\n",
       " 'elimination',\n",
       " 'section',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_list_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_ocr = copy.deepcopy(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `min_count` (Attribute will be removed in 4.0.0, use self.vocabulary.min_count instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embedding_model_ocr.workers = 8\n",
    "embedding_model_ocr.min_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:22:22,416 : INFO : collecting all words and their counts\n",
      "2019-11-13 13:22:22,417 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-13 13:22:22,802 : INFO : PROGRESS: at sentence #10000, processed 525154 words, keeping 85319 word types\n",
      "2019-11-13 13:22:23,027 : INFO : PROGRESS: at sentence #20000, processed 856174 words, keeping 117320 word types\n",
      "2019-11-13 13:22:23,286 : INFO : PROGRESS: at sentence #30000, processed 1262312 words, keeping 157071 word types\n",
      "2019-11-13 13:22:23,515 : INFO : PROGRESS: at sentence #40000, processed 1640602 words, keeping 191468 word types\n",
      "2019-11-13 13:22:23,709 : INFO : PROGRESS: at sentence #50000, processed 1983065 words, keeping 219352 word types\n",
      "2019-11-13 13:22:23,937 : INFO : PROGRESS: at sentence #60000, processed 2332908 words, keeping 244407 word types\n",
      "2019-11-13 13:22:24,181 : INFO : PROGRESS: at sentence #70000, processed 2683098 words, keeping 272037 word types\n",
      "2019-11-13 13:22:24,440 : INFO : PROGRESS: at sentence #80000, processed 3061759 words, keeping 300138 word types\n",
      "2019-11-13 13:22:24,682 : INFO : PROGRESS: at sentence #90000, processed 3400806 words, keeping 325517 word types\n",
      "2019-11-13 13:22:25,028 : INFO : PROGRESS: at sentence #100000, processed 3888563 words, keeping 363271 word types\n",
      "2019-11-13 13:22:25,307 : INFO : PROGRESS: at sentence #110000, processed 4305098 words, keeping 391802 word types\n",
      "2019-11-13 13:22:25,528 : INFO : PROGRESS: at sentence #120000, processed 4663026 words, keeping 415432 word types\n",
      "2019-11-13 13:22:25,858 : INFO : PROGRESS: at sentence #130000, processed 5214928 words, keeping 457311 word types\n",
      "2019-11-13 13:22:26,102 : INFO : PROGRESS: at sentence #140000, processed 5590287 words, keeping 482760 word types\n",
      "2019-11-13 13:22:26,443 : INFO : PROGRESS: at sentence #150000, processed 6128230 words, keeping 522484 word types\n",
      "2019-11-13 13:22:26,703 : INFO : PROGRESS: at sentence #160000, processed 6545889 words, keeping 548178 word types\n",
      "2019-11-13 13:22:26,956 : INFO : PROGRESS: at sentence #170000, processed 6934913 words, keeping 571470 word types\n",
      "2019-11-13 13:22:27,341 : INFO : PROGRESS: at sentence #180000, processed 7400878 words, keeping 603179 word types\n",
      "2019-11-13 13:22:27,647 : INFO : PROGRESS: at sentence #190000, processed 7822628 words, keeping 627282 word types\n",
      "2019-11-13 13:22:27,969 : INFO : PROGRESS: at sentence #200000, processed 8351540 words, keeping 660582 word types\n",
      "2019-11-13 13:22:28,177 : INFO : PROGRESS: at sentence #210000, processed 8741601 words, keeping 682611 word types\n",
      "2019-11-13 13:22:28,459 : INFO : PROGRESS: at sentence #220000, processed 9130858 words, keeping 706628 word types\n",
      "2019-11-13 13:22:28,640 : INFO : PROGRESS: at sentence #230000, processed 9470776 words, keeping 725143 word types\n",
      "2019-11-13 13:22:28,834 : INFO : PROGRESS: at sentence #240000, processed 9871436 words, keeping 749265 word types\n",
      "2019-11-13 13:22:29,080 : INFO : PROGRESS: at sentence #250000, processed 10329460 words, keeping 777098 word types\n",
      "2019-11-13 13:22:29,306 : INFO : PROGRESS: at sentence #260000, processed 10738641 words, keeping 801955 word types\n",
      "2019-11-13 13:22:29,466 : INFO : PROGRESS: at sentence #270000, processed 11043909 words, keeping 820773 word types\n",
      "2019-11-13 13:22:29,690 : INFO : PROGRESS: at sentence #280000, processed 11476758 words, keeping 848391 word types\n",
      "2019-11-13 13:22:30,021 : INFO : PROGRESS: at sentence #290000, processed 12093328 words, keeping 889865 word types\n",
      "2019-11-13 13:22:30,170 : INFO : collected 902618 word types from a corpus of 12356836 raw words and 297582 sentences\n",
      "2019-11-13 13:22:30,171 : INFO : Updating model with new vocabulary\n",
      "2019-11-13 13:22:34,169 : INFO : New added 902618 unique words (50% of original 1805236) and increased the count of 902618 pre-existing words (50% of original 1805236)\n",
      "2019-11-13 13:22:47,840 : INFO : deleting the raw counts dictionary of 902618 items\n",
      "2019-11-13 13:22:47,894 : INFO : sample=0.001 downsamples 76 most-common words\n",
      "2019-11-13 13:22:47,896 : INFO : downsampling leaves estimated 18892064 word corpus (152.9% of prior 12356836)\n",
      "2019-11-13 13:22:52,560 : INFO : estimated required memory for 1805236 words and 300 dimensions: 5235184400 bytes\n",
      "2019-11-13 13:22:52,564 : INFO : updating layer weights\n",
      "2019-11-13 13:26:52,315 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-13 13:26:52,319 : INFO : training model with 8 workers on 1226404 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=20 window=5\n",
      "2019-11-13 13:26:53,377 : INFO : EPOCH 1 - PROGRESS: at 0.78% examples, 103480 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:26:54,387 : INFO : EPOCH 1 - PROGRESS: at 1.63% examples, 112069 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-13 13:26:55,663 : INFO : EPOCH 1 - PROGRESS: at 2.93% examples, 119476 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:26:56,667 : INFO : EPOCH 1 - PROGRESS: at 5.23% examples, 132139 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:26:57,712 : INFO : EPOCH 1 - PROGRESS: at 6.86% examples, 129547 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:26:58,734 : INFO : EPOCH 1 - PROGRESS: at 8.99% examples, 135549 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:26:59,783 : INFO : EPOCH 1 - PROGRESS: at 10.21% examples, 133704 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:00,852 : INFO : EPOCH 1 - PROGRESS: at 11.71% examples, 134666 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:01,978 : INFO : EPOCH 1 - PROGRESS: at 13.71% examples, 134296 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:02,995 : INFO : EPOCH 1 - PROGRESS: at 15.58% examples, 134756 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:04,055 : INFO : EPOCH 1 - PROGRESS: at 17.58% examples, 134731 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:27:05,091 : INFO : EPOCH 1 - PROGRESS: at 19.24% examples, 134518 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:06,112 : INFO : EPOCH 1 - PROGRESS: at 20.81% examples, 134338 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:07,213 : INFO : EPOCH 1 - PROGRESS: at 22.87% examples, 134889 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:08,254 : INFO : EPOCH 1 - PROGRESS: at 24.63% examples, 134935 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:09,339 : INFO : EPOCH 1 - PROGRESS: at 26.19% examples, 135325 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:10,352 : INFO : EPOCH 1 - PROGRESS: at 28.66% examples, 135489 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:11,421 : INFO : EPOCH 1 - PROGRESS: at 30.06% examples, 135116 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:12,424 : INFO : EPOCH 1 - PROGRESS: at 31.32% examples, 135147 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:13,505 : INFO : EPOCH 1 - PROGRESS: at 32.55% examples, 135004 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:14,518 : INFO : EPOCH 1 - PROGRESS: at 33.88% examples, 134940 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:15,533 : INFO : EPOCH 1 - PROGRESS: at 35.27% examples, 134978 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:16,584 : INFO : EPOCH 1 - PROGRESS: at 36.80% examples, 134803 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:17,659 : INFO : EPOCH 1 - PROGRESS: at 38.34% examples, 134842 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:18,726 : INFO : EPOCH 1 - PROGRESS: at 40.22% examples, 134749 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:19,780 : INFO : EPOCH 1 - PROGRESS: at 42.13% examples, 135002 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:20,824 : INFO : EPOCH 1 - PROGRESS: at 42.88% examples, 134832 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:21,952 : INFO : EPOCH 1 - PROGRESS: at 43.64% examples, 134368 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:22,976 : INFO : EPOCH 1 - PROGRESS: at 45.40% examples, 135011 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:27:24,056 : INFO : EPOCH 1 - PROGRESS: at 47.19% examples, 134866 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:27:25,111 : INFO : EPOCH 1 - PROGRESS: at 48.59% examples, 134556 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:26,152 : INFO : EPOCH 1 - PROGRESS: at 49.44% examples, 134187 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:27:27,185 : INFO : EPOCH 1 - PROGRESS: at 50.29% examples, 134294 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:27:28,193 : INFO : EPOCH 1 - PROGRESS: at 51.82% examples, 134812 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:29,236 : INFO : EPOCH 1 - PROGRESS: at 53.12% examples, 134512 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:30,267 : INFO : EPOCH 1 - PROGRESS: at 54.47% examples, 134179 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:31,338 : INFO : EPOCH 1 - PROGRESS: at 56.52% examples, 134846 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:32,449 : INFO : EPOCH 1 - PROGRESS: at 57.95% examples, 134341 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:33,479 : INFO : EPOCH 1 - PROGRESS: at 59.29% examples, 134515 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:34,537 : INFO : EPOCH 1 - PROGRESS: at 60.52% examples, 134548 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:35,646 : INFO : EPOCH 1 - PROGRESS: at 62.11% examples, 134425 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:36,659 : INFO : EPOCH 1 - PROGRESS: at 63.51% examples, 134410 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:37,666 : INFO : EPOCH 1 - PROGRESS: at 64.54% examples, 134429 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:38,675 : INFO : EPOCH 1 - PROGRESS: at 65.57% examples, 134447 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:39,756 : INFO : EPOCH 1 - PROGRESS: at 66.91% examples, 134382 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:40,769 : INFO : EPOCH 1 - PROGRESS: at 68.34% examples, 134195 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:41,780 : INFO : EPOCH 1 - PROGRESS: at 69.92% examples, 134529 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:42,808 : INFO : EPOCH 1 - PROGRESS: at 71.75% examples, 134631 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:43,959 : INFO : EPOCH 1 - PROGRESS: at 73.06% examples, 134143 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:27:44,977 : INFO : EPOCH 1 - PROGRESS: at 74.58% examples, 133985 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:45,977 : INFO : EPOCH 1 - PROGRESS: at 76.34% examples, 134240 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:27:47,167 : INFO : EPOCH 1 - PROGRESS: at 78.34% examples, 133875 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:48,217 : INFO : EPOCH 1 - PROGRESS: at 79.99% examples, 134037 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:27:49,225 : INFO : EPOCH 1 - PROGRESS: at 81.57% examples, 134060 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:27:50,368 : INFO : EPOCH 1 - PROGRESS: at 82.73% examples, 133788 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:51,432 : INFO : EPOCH 1 - PROGRESS: at 84.05% examples, 133821 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:27:52,529 : INFO : EPOCH 1 - PROGRESS: at 85.69% examples, 133760 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:27:53,544 : INFO : EPOCH 1 - PROGRESS: at 86.98% examples, 133633 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:54,637 : INFO : EPOCH 1 - PROGRESS: at 89.28% examples, 133786 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:55,686 : INFO : EPOCH 1 - PROGRESS: at 90.97% examples, 133676 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:56,708 : INFO : EPOCH 1 - PROGRESS: at 92.50% examples, 133550 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:57,756 : INFO : EPOCH 1 - PROGRESS: at 93.85% examples, 133615 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:27:58,794 : INFO : EPOCH 1 - PROGRESS: at 94.85% examples, 133617 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:27:59,833 : INFO : EPOCH 1 - PROGRESS: at 95.79% examples, 133492 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:28:00,866 : INFO : EPOCH 1 - PROGRESS: at 96.79% examples, 133502 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:01,872 : INFO : EPOCH 1 - PROGRESS: at 97.91% examples, 133553 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:02,689 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:28:02,746 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:28:02,785 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:28:02,792 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:28:02,891 : INFO : EPOCH 1 - PROGRESS: at 99.70% examples, 133662 words/s, in_qsize 3, out_qsize 1\n",
      "2019-11-13 13:28:02,892 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:28:02,916 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:28:02,917 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:28:03,016 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:28:03,017 : INFO : EPOCH - 1 : training on 12356836 raw words (9445793 effective words) took 70.6s, 133714 effective words/s\n",
      "2019-11-13 13:28:04,055 : INFO : EPOCH 2 - PROGRESS: at 1.00% examples, 125284 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:05,407 : INFO : EPOCH 2 - PROGRESS: at 1.87% examples, 112215 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:06,431 : INFO : EPOCH 2 - PROGRESS: at 3.24% examples, 122396 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:28:07,591 : INFO : EPOCH 2 - PROGRESS: at 5.33% examples, 126168 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:08,592 : INFO : EPOCH 2 - PROGRESS: at 7.13% examples, 128517 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:09,762 : INFO : EPOCH 2 - PROGRESS: at 9.06% examples, 129327 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:10,772 : INFO : EPOCH 2 - PROGRESS: at 10.27% examples, 129077 words/s, in_qsize 16, out_qsize 2\n",
      "2019-11-13 13:28:11,797 : INFO : EPOCH 2 - PROGRESS: at 12.01% examples, 132885 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:12,831 : INFO : EPOCH 2 - PROGRESS: at 13.71% examples, 131692 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:13,832 : INFO : EPOCH 2 - PROGRESS: at 15.58% examples, 132579 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:28:14,947 : INFO : EPOCH 2 - PROGRESS: at 17.74% examples, 133410 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:15,999 : INFO : EPOCH 2 - PROGRESS: at 19.39% examples, 133156 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:17,116 : INFO : EPOCH 2 - PROGRESS: at 21.18% examples, 133740 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:18,159 : INFO : EPOCH 2 - PROGRESS: at 23.18% examples, 133834 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:19,196 : INFO : EPOCH 2 - PROGRESS: at 24.94% examples, 134502 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:28:20,196 : INFO : EPOCH 2 - PROGRESS: at 26.34% examples, 134238 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:21,247 : INFO : EPOCH 2 - PROGRESS: at 28.72% examples, 134221 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:22,263 : INFO : EPOCH 2 - PROGRESS: at 30.29% examples, 135124 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:23,312 : INFO : EPOCH 2 - PROGRESS: at 31.52% examples, 134801 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:24,386 : INFO : EPOCH 2 - PROGRESS: at 32.68% examples, 134386 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:25,443 : INFO : EPOCH 2 - PROGRESS: at 34.09% examples, 134754 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:28:26,458 : INFO : EPOCH 2 - PROGRESS: at 35.83% examples, 135091 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:28:27,473 : INFO : EPOCH 2 - PROGRESS: at 36.97% examples, 134589 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:28,514 : INFO : EPOCH 2 - PROGRESS: at 38.62% examples, 134722 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:29,550 : INFO : EPOCH 2 - PROGRESS: at 40.62% examples, 135071 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:28:30,639 : INFO : EPOCH 2 - PROGRESS: at 42.24% examples, 134616 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:28:31,742 : INFO : EPOCH 2 - PROGRESS: at 42.97% examples, 134201 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:28:32,798 : INFO : EPOCH 2 - PROGRESS: at 43.80% examples, 134340 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:28:33,800 : INFO : EPOCH 2 - PROGRESS: at 45.58% examples, 134810 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:34,861 : INFO : EPOCH 2 - PROGRESS: at 47.29% examples, 134524 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:35,904 : INFO : EPOCH 2 - PROGRESS: at 48.65% examples, 134295 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:28:37,010 : INFO : EPOCH 2 - PROGRESS: at 49.44% examples, 133441 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:28:38,093 : INFO : EPOCH 2 - PROGRESS: at 50.20% examples, 132934 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:28:39,138 : INFO : EPOCH 2 - PROGRESS: at 51.31% examples, 132527 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:28:40,175 : INFO : EPOCH 2 - PROGRESS: at 52.56% examples, 132114 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:41,212 : INFO : EPOCH 2 - PROGRESS: at 53.94% examples, 131854 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:42,231 : INFO : EPOCH 2 - PROGRESS: at 55.68% examples, 132592 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:43,260 : INFO : EPOCH 2 - PROGRESS: at 57.32% examples, 132352 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:44,326 : INFO : EPOCH 2 - PROGRESS: at 58.65% examples, 132279 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:45,363 : INFO : EPOCH 2 - PROGRESS: at 59.91% examples, 132377 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:28:46,545 : INFO : EPOCH 2 - PROGRESS: at 61.51% examples, 132321 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:47,598 : INFO : EPOCH 2 - PROGRESS: at 63.05% examples, 132589 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:48,741 : INFO : EPOCH 2 - PROGRESS: at 64.30% examples, 132567 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:49,843 : INFO : EPOCH 2 - PROGRESS: at 65.43% examples, 132693 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:50,844 : INFO : EPOCH 2 - PROGRESS: at 66.76% examples, 132888 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:51,883 : INFO : EPOCH 2 - PROGRESS: at 68.25% examples, 132805 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:52,901 : INFO : EPOCH 2 - PROGRESS: at 69.59% examples, 132696 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:28:53,933 : INFO : EPOCH 2 - PROGRESS: at 71.44% examples, 132805 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:54,975 : INFO : EPOCH 2 - PROGRESS: at 73.00% examples, 133083 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:28:55,977 : INFO : EPOCH 2 - PROGRESS: at 74.52% examples, 132984 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:56,982 : INFO : EPOCH 2 - PROGRESS: at 76.13% examples, 133112 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:58,054 : INFO : EPOCH 2 - PROGRESS: at 78.34% examples, 133324 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:28:59,111 : INFO : EPOCH 2 - PROGRESS: at 79.82% examples, 133197 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:00,120 : INFO : EPOCH 2 - PROGRESS: at 81.57% examples, 133508 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:01,133 : INFO : EPOCH 2 - PROGRESS: at 82.73% examples, 133543 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:02,248 : INFO : EPOCH 2 - PROGRESS: at 83.96% examples, 133335 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:03,249 : INFO : EPOCH 2 - PROGRESS: at 85.62% examples, 133499 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:04,267 : INFO : EPOCH 2 - PROGRESS: at 87.08% examples, 133613 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:05,277 : INFO : EPOCH 2 - PROGRESS: at 89.19% examples, 133709 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:06,286 : INFO : EPOCH 2 - PROGRESS: at 90.90% examples, 133680 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:07,313 : INFO : EPOCH 2 - PROGRESS: at 92.68% examples, 133890 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:08,374 : INFO : EPOCH 2 - PROGRESS: at 93.92% examples, 133811 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:09,412 : INFO : EPOCH 2 - PROGRESS: at 94.85% examples, 133698 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:10,435 : INFO : EPOCH 2 - PROGRESS: at 95.80% examples, 133604 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:11,450 : INFO : EPOCH 2 - PROGRESS: at 96.82% examples, 133757 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:12,487 : INFO : EPOCH 2 - PROGRESS: at 98.00% examples, 133746 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:13,238 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:29:13,325 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:29:13,350 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:29:13,377 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:29:13,493 : INFO : EPOCH 2 - PROGRESS: at 99.68% examples, 133731 words/s, in_qsize 3, out_qsize 1\n",
      "2019-11-13 13:29:13,494 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:29:13,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:29:13,553 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:29:13,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:29:13,636 : INFO : EPOCH - 2 : training on 12356836 raw words (9444682 effective words) took 70.6s, 133785 effective words/s\n",
      "2019-11-13 13:29:15,020 : INFO : EPOCH 3 - PROGRESS: at 1.08% examples, 99093 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:16,032 : INFO : EPOCH 3 - PROGRESS: at 2.07% examples, 121853 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:17,293 : INFO : EPOCH 3 - PROGRESS: at 3.81% examples, 124315 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:18,337 : INFO : EPOCH 3 - PROGRESS: at 6.20% examples, 133558 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:19,410 : INFO : EPOCH 3 - PROGRESS: at 7.55% examples, 130520 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:20,412 : INFO : EPOCH 3 - PROGRESS: at 9.13% examples, 129775 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:21,451 : INFO : EPOCH 3 - PROGRESS: at 10.63% examples, 132844 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:22,471 : INFO : EPOCH 3 - PROGRESS: at 12.11% examples, 132819 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:23,489 : INFO : EPOCH 3 - PROGRESS: at 13.88% examples, 132599 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:24,574 : INFO : EPOCH 3 - PROGRESS: at 16.22% examples, 134346 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:25,598 : INFO : EPOCH 3 - PROGRESS: at 17.95% examples, 134228 words/s, in_qsize 15, out_qsize 1\n",
      "2019-11-13 13:29:26,611 : INFO : EPOCH 3 - PROGRESS: at 19.81% examples, 135492 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:27,615 : INFO : EPOCH 3 - PROGRESS: at 21.30% examples, 135363 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:28,634 : INFO : EPOCH 3 - PROGRESS: at 23.18% examples, 135072 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:29:29,704 : INFO : EPOCH 3 - PROGRESS: at 24.94% examples, 135375 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:30,837 : INFO : EPOCH 3 - PROGRESS: at 26.68% examples, 135338 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:31,904 : INFO : EPOCH 3 - PROGRESS: at 29.03% examples, 135572 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:32,919 : INFO : EPOCH 3 - PROGRESS: at 30.58% examples, 135995 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:29:33,941 : INFO : EPOCH 3 - PROGRESS: at 31.63% examples, 135441 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:35,009 : INFO : EPOCH 3 - PROGRESS: at 32.97% examples, 135735 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:29:36,031 : INFO : EPOCH 3 - PROGRESS: at 34.32% examples, 135912 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:37,068 : INFO : EPOCH 3 - PROGRESS: at 36.07% examples, 135718 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:38,124 : INFO : EPOCH 3 - PROGRESS: at 37.18% examples, 135344 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:29:39,159 : INFO : EPOCH 3 - PROGRESS: at 39.00% examples, 135733 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:40,293 : INFO : EPOCH 3 - PROGRESS: at 41.03% examples, 135523 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:41,335 : INFO : EPOCH 3 - PROGRESS: at 42.48% examples, 135837 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:29:42,404 : INFO : EPOCH 3 - PROGRESS: at 43.26% examples, 135588 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:43,444 : INFO : EPOCH 3 - PROGRESS: at 44.17% examples, 135482 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:44,467 : INFO : EPOCH 3 - PROGRESS: at 45.89% examples, 135532 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:29:45,469 : INFO : EPOCH 3 - PROGRESS: at 47.85% examples, 135728 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:46,475 : INFO : EPOCH 3 - PROGRESS: at 48.94% examples, 135702 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:29:47,505 : INFO : EPOCH 3 - PROGRESS: at 49.74% examples, 135565 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:48,612 : INFO : EPOCH 3 - PROGRESS: at 50.66% examples, 135115 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:49,640 : INFO : EPOCH 3 - PROGRESS: at 52.31% examples, 135717 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:29:50,824 : INFO : EPOCH 3 - PROGRESS: at 53.80% examples, 135032 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:51,858 : INFO : EPOCH 3 - PROGRESS: at 55.50% examples, 135658 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:52,972 : INFO : EPOCH 3 - PROGRESS: at 57.24% examples, 135219 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:53,979 : INFO : EPOCH 3 - PROGRESS: at 58.57% examples, 135272 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:55,018 : INFO : EPOCH 3 - PROGRESS: at 59.84% examples, 135285 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:56,124 : INFO : EPOCH 3 - PROGRESS: at 61.43% examples, 135393 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:57,145 : INFO : EPOCH 3 - PROGRESS: at 62.71% examples, 135192 words/s, in_qsize 15, out_qsize 1\n",
      "2019-11-13 13:29:58,157 : INFO : EPOCH 3 - PROGRESS: at 64.15% examples, 135657 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:29:59,235 : INFO : EPOCH 3 - PROGRESS: at 65.06% examples, 135277 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:00,269 : INFO : EPOCH 3 - PROGRESS: at 66.23% examples, 135019 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:01,291 : INFO : EPOCH 3 - PROGRESS: at 67.41% examples, 134799 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:02,340 : INFO : EPOCH 3 - PROGRESS: at 69.14% examples, 135152 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:03,444 : INFO : EPOCH 3 - PROGRESS: at 70.95% examples, 135023 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:04,464 : INFO : EPOCH 3 - PROGRESS: at 72.47% examples, 135010 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:30:05,483 : INFO : EPOCH 3 - PROGRESS: at 74.07% examples, 135143 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:06,570 : INFO : EPOCH 3 - PROGRESS: at 75.74% examples, 135170 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:07,631 : INFO : EPOCH 3 - PROGRESS: at 77.93% examples, 135233 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:08,763 : INFO : EPOCH 3 - PROGRESS: at 79.58% examples, 135149 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:09,826 : INFO : EPOCH 3 - PROGRESS: at 81.16% examples, 135178 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:30:10,876 : INFO : EPOCH 3 - PROGRESS: at 82.52% examples, 135220 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:11,979 : INFO : EPOCH 3 - PROGRESS: at 83.67% examples, 135015 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-13 13:30:13,021 : INFO : EPOCH 3 - PROGRESS: at 85.55% examples, 135309 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:14,058 : INFO : EPOCH 3 - PROGRESS: at 86.89% examples, 135227 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:15,062 : INFO : EPOCH 3 - PROGRESS: at 88.99% examples, 135315 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:16,113 : INFO : EPOCH 3 - PROGRESS: at 90.84% examples, 135292 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:17,238 : INFO : EPOCH 3 - PROGRESS: at 92.57% examples, 135149 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:18,241 : INFO : EPOCH 3 - PROGRESS: at 93.85% examples, 135287 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:19,302 : INFO : EPOCH 3 - PROGRESS: at 94.85% examples, 135217 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:20,443 : INFO : EPOCH 3 - PROGRESS: at 95.94% examples, 135091 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:30:21,454 : INFO : EPOCH 3 - PROGRESS: at 96.82% examples, 135003 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:22,503 : INFO : EPOCH 3 - PROGRESS: at 98.16% examples, 135169 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:23,177 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:30:23,221 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:30:23,231 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:30:23,260 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:30:23,344 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:30:23,355 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:30:23,423 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:30:23,429 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:30:23,430 : INFO : EPOCH - 3 : training on 12356836 raw words (9446831 effective words) took 69.8s, 135395 effective words/s\n",
      "2019-11-13 13:30:24,453 : INFO : EPOCH 4 - PROGRESS: at 1.00% examples, 125905 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:25,800 : INFO : EPOCH 4 - PROGRESS: at 1.87% examples, 112781 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:26,802 : INFO : EPOCH 4 - PROGRESS: at 3.57% examples, 130221 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:27,977 : INFO : EPOCH 4 - PROGRESS: at 5.33% examples, 126674 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:29,031 : INFO : EPOCH 4 - PROGRESS: at 7.41% examples, 133071 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:30,067 : INFO : EPOCH 4 - PROGRESS: at 9.06% examples, 131233 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:31,103 : INFO : EPOCH 4 - PROGRESS: at 10.29% examples, 130219 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:30:32,228 : INFO : EPOCH 4 - PROGRESS: at 12.11% examples, 133246 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:33,243 : INFO : EPOCH 4 - PROGRESS: at 13.97% examples, 133774 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:34,326 : INFO : EPOCH 4 - PROGRESS: at 16.22% examples, 134746 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:35,366 : INFO : EPOCH 4 - PROGRESS: at 17.95% examples, 134388 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:36,444 : INFO : EPOCH 4 - PROGRESS: at 19.90% examples, 135539 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:37,535 : INFO : EPOCH 4 - PROGRESS: at 21.42% examples, 134571 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:38,647 : INFO : EPOCH 4 - PROGRESS: at 23.64% examples, 135463 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:39,760 : INFO : EPOCH 4 - PROGRESS: at 25.27% examples, 134958 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:40,807 : INFO : EPOCH 4 - PROGRESS: at 27.36% examples, 136006 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:41,884 : INFO : EPOCH 4 - PROGRESS: at 29.31% examples, 135795 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:42,941 : INFO : EPOCH 4 - PROGRESS: at 30.89% examples, 136246 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:30:43,959 : INFO : EPOCH 4 - PROGRESS: at 32.03% examples, 136098 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:45,044 : INFO : EPOCH 4 - PROGRESS: at 33.29% examples, 135550 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:46,046 : INFO : EPOCH 4 - PROGRESS: at 34.72% examples, 135844 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:47,068 : INFO : EPOCH 4 - PROGRESS: at 36.46% examples, 136128 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:48,116 : INFO : EPOCH 4 - PROGRESS: at 37.76% examples, 136022 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:49,121 : INFO : EPOCH 4 - PROGRESS: at 39.42% examples, 136276 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:50,194 : INFO : EPOCH 4 - PROGRESS: at 41.68% examples, 136589 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:51,298 : INFO : EPOCH 4 - PROGRESS: at 42.65% examples, 136069 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:52,353 : INFO : EPOCH 4 - PROGRESS: at 43.44% examples, 136144 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:53,416 : INFO : EPOCH 4 - PROGRESS: at 44.74% examples, 136119 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:54,432 : INFO : EPOCH 4 - PROGRESS: at 46.43% examples, 136188 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:30:55,466 : INFO : EPOCH 4 - PROGRESS: at 48.21% examples, 136060 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:30:56,506 : INFO : EPOCH 4 - PROGRESS: at 49.27% examples, 136132 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:57,593 : INFO : EPOCH 4 - PROGRESS: at 50.13% examples, 136212 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:30:58,614 : INFO : EPOCH 4 - PROGRESS: at 51.31% examples, 136017 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:30:59,691 : INFO : EPOCH 4 - PROGRESS: at 52.73% examples, 135759 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:31:00,696 : INFO : EPOCH 4 - PROGRESS: at 54.39% examples, 136293 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:01,826 : INFO : EPOCH 4 - PROGRESS: at 55.99% examples, 135943 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:02,879 : INFO : EPOCH 4 - PROGRESS: at 57.89% examples, 136341 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:03,925 : INFO : EPOCH 4 - PROGRESS: at 59.07% examples, 136032 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:31:05,006 : INFO : EPOCH 4 - PROGRESS: at 60.28% examples, 135952 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:06,017 : INFO : EPOCH 4 - PROGRESS: at 61.97% examples, 136300 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:31:07,079 : INFO : EPOCH 4 - PROGRESS: at 63.36% examples, 136086 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:08,107 : INFO : EPOCH 4 - PROGRESS: at 64.42% examples, 136006 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:31:09,131 : INFO : EPOCH 4 - PROGRESS: at 65.46% examples, 135939 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:10,176 : INFO : EPOCH 4 - PROGRESS: at 66.84% examples, 136109 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:11,253 : INFO : EPOCH 4 - PROGRESS: at 68.34% examples, 135852 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:31:12,270 : INFO : EPOCH 4 - PROGRESS: at 69.83% examples, 135982 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:13,284 : INFO : EPOCH 4 - PROGRESS: at 71.75% examples, 136242 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:31:14,569 : INFO : EPOCH 4 - PROGRESS: at 73.06% examples, 135358 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:15,576 : INFO : EPOCH 4 - PROGRESS: at 74.97% examples, 135910 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:16,741 : INFO : EPOCH 4 - PROGRESS: at 76.71% examples, 135424 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:31:17,744 : INFO : EPOCH 4 - PROGRESS: at 78.86% examples, 135898 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:18,766 : INFO : EPOCH 4 - PROGRESS: at 79.99% examples, 135292 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:19,767 : INFO : EPOCH 4 - PROGRESS: at 81.65% examples, 135448 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:20,791 : INFO : EPOCH 4 - PROGRESS: at 82.73% examples, 135290 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:21,844 : INFO : EPOCH 4 - PROGRESS: at 83.96% examples, 135191 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:22,850 : INFO : EPOCH 4 - PROGRESS: at 85.62% examples, 135314 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:23,869 : INFO : EPOCH 4 - PROGRESS: at 86.80% examples, 135022 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:24,943 : INFO : EPOCH 4 - PROGRESS: at 88.79% examples, 134841 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:26,009 : INFO : EPOCH 4 - PROGRESS: at 90.84% examples, 135027 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:27,134 : INFO : EPOCH 4 - PROGRESS: at 92.57% examples, 134888 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:28,197 : INFO : EPOCH 4 - PROGRESS: at 93.97% examples, 135142 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:29,395 : INFO : EPOCH 4 - PROGRESS: at 95.06% examples, 134910 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:30,403 : INFO : EPOCH 4 - PROGRESS: at 96.05% examples, 134947 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-13 13:31:31,412 : INFO : EPOCH 4 - PROGRESS: at 97.00% examples, 135090 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:32,462 : INFO : EPOCH 4 - PROGRESS: at 98.34% examples, 135031 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:33,048 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:31:33,086 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:31:33,102 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:31:33,115 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:31:33,178 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:31:33,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:31:33,296 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:31:33,335 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:31:33,336 : INFO : EPOCH - 4 : training on 12356836 raw words (9445707 effective words) took 69.9s, 135144 effective words/s\n",
      "2019-11-13 13:31:34,720 : INFO : EPOCH 5 - PROGRESS: at 1.08% examples, 98404 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:35,736 : INFO : EPOCH 5 - PROGRESS: at 2.07% examples, 121242 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:37,008 : INFO : EPOCH 5 - PROGRESS: at 3.81% examples, 123478 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:31:38,011 : INFO : EPOCH 5 - PROGRESS: at 6.20% examples, 134038 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:39,139 : INFO : EPOCH 5 - PROGRESS: at 7.55% examples, 129686 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:40,180 : INFO : EPOCH 5 - PROGRESS: at 9.41% examples, 132780 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:41,343 : INFO : EPOCH 5 - PROGRESS: at 10.82% examples, 131403 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:42,369 : INFO : EPOCH 5 - PROGRESS: at 12.68% examples, 134629 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:43,490 : INFO : EPOCH 5 - PROGRESS: at 14.27% examples, 132973 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:44,499 : INFO : EPOCH 5 - PROGRESS: at 16.63% examples, 134981 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:31:45,589 : INFO : EPOCH 5 - PROGRESS: at 18.56% examples, 134614 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:31:46,619 : INFO : EPOCH 5 - PROGRESS: at 20.17% examples, 134492 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:47,715 : INFO : EPOCH 5 - PROGRESS: at 22.04% examples, 135120 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:48,756 : INFO : EPOCH 5 - PROGRESS: at 23.95% examples, 135103 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:49,795 : INFO : EPOCH 5 - PROGRESS: at 25.59% examples, 135710 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:50,914 : INFO : EPOCH 5 - PROGRESS: at 27.77% examples, 135714 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:51,980 : INFO : EPOCH 5 - PROGRESS: at 29.61% examples, 136034 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:52,982 : INFO : EPOCH 5 - PROGRESS: at 31.00% examples, 136094 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:54,059 : INFO : EPOCH 5 - PROGRESS: at 32.19% examples, 135570 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:55,149 : INFO : EPOCH 5 - PROGRESS: at 33.55% examples, 135701 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:31:56,174 : INFO : EPOCH 5 - PROGRESS: at 35.01% examples, 135940 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:57,174 : INFO : EPOCH 5 - PROGRESS: at 36.62% examples, 135989 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:58,212 : INFO : EPOCH 5 - PROGRESS: at 38.03% examples, 135901 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:31:59,239 : INFO : EPOCH 5 - PROGRESS: at 39.74% examples, 136021 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:00,321 : INFO : EPOCH 5 - PROGRESS: at 41.88% examples, 136027 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:32:01,336 : INFO : EPOCH 5 - PROGRESS: at 42.72% examples, 135968 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:02,336 : INFO : EPOCH 5 - PROGRESS: at 43.44% examples, 135787 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:03,338 : INFO : EPOCH 5 - PROGRESS: at 44.82% examples, 136304 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:04,373 : INFO : EPOCH 5 - PROGRESS: at 46.43% examples, 136032 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:32:05,424 : INFO : EPOCH 5 - PROGRESS: at 48.15% examples, 135581 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:06,463 : INFO : EPOCH 5 - PROGRESS: at 49.20% examples, 135687 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:07,469 : INFO : EPOCH 5 - PROGRESS: at 50.03% examples, 135856 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:32:08,521 : INFO : EPOCH 5 - PROGRESS: at 51.23% examples, 135783 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:09,610 : INFO : EPOCH 5 - PROGRESS: at 52.56% examples, 135294 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:10,610 : INFO : EPOCH 5 - PROGRESS: at 54.31% examples, 136072 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:11,768 : INFO : EPOCH 5 - PROGRESS: at 55.75% examples, 135440 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:12,832 : INFO : EPOCH 5 - PROGRESS: at 57.61% examples, 135395 words/s, in_qsize 16, out_qsize 2\n",
      "2019-11-13 13:32:14,049 : INFO : EPOCH 5 - PROGRESS: at 59.07% examples, 135310 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:15,084 : INFO : EPOCH 5 - PROGRESS: at 60.36% examples, 135579 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:16,294 : INFO : EPOCH 5 - PROGRESS: at 62.11% examples, 135470 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:17,326 : INFO : EPOCH 5 - PROGRESS: at 63.65% examples, 135695 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:18,377 : INFO : EPOCH 5 - PROGRESS: at 64.65% examples, 135561 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:32:19,413 : INFO : EPOCH 5 - PROGRESS: at 65.75% examples, 135469 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:20,436 : INFO : EPOCH 5 - PROGRESS: at 66.99% examples, 135388 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:21,474 : INFO : EPOCH 5 - PROGRESS: at 68.53% examples, 135435 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:22,504 : INFO : EPOCH 5 - PROGRESS: at 70.23% examples, 135672 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:23,521 : INFO : EPOCH 5 - PROGRESS: at 71.81% examples, 135490 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:32:24,604 : INFO : EPOCH 5 - PROGRESS: at 73.30% examples, 135460 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:25,631 : INFO : EPOCH 5 - PROGRESS: at 75.04% examples, 135663 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:32:26,711 : INFO : EPOCH 5 - PROGRESS: at 77.21% examples, 135678 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:27,743 : INFO : EPOCH 5 - PROGRESS: at 78.92% examples, 135801 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:28,768 : INFO : EPOCH 5 - PROGRESS: at 80.27% examples, 135617 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:29,780 : INFO : EPOCH 5 - PROGRESS: at 81.91% examples, 135596 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:30,782 : INFO : EPOCH 5 - PROGRESS: at 83.07% examples, 135888 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:31,809 : INFO : EPOCH 5 - PROGRESS: at 84.50% examples, 135834 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:32,834 : INFO : EPOCH 5 - PROGRESS: at 86.00% examples, 135768 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:33,861 : INFO : EPOCH 5 - PROGRESS: at 87.52% examples, 135832 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:34,862 : INFO : EPOCH 5 - PROGRESS: at 89.67% examples, 135889 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:35,899 : INFO : EPOCH 5 - PROGRESS: at 91.44% examples, 135907 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:36,985 : INFO : EPOCH 5 - PROGRESS: at 92.93% examples, 135724 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:32:38,003 : INFO : EPOCH 5 - PROGRESS: at 94.19% examples, 135835 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:39,091 : INFO : EPOCH 5 - PROGRESS: at 95.21% examples, 135692 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:40,168 : INFO : EPOCH 5 - PROGRESS: at 96.33% examples, 135692 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:32:41,169 : INFO : EPOCH 5 - PROGRESS: at 97.08% examples, 135729 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:32:42,212 : INFO : EPOCH 5 - PROGRESS: at 98.61% examples, 135671 words/s, in_qsize 14, out_qsize 0\n",
      "2019-11-13 13:32:42,530 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:32:42,635 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:32:42,682 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:32:42,719 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:32:42,754 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:32:42,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:32:42,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:32:42,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:32:42,899 : INFO : EPOCH - 5 : training on 12356836 raw words (9446493 effective words) took 69.6s, 135814 effective words/s\n",
      "2019-11-13 13:32:42,900 : INFO : training on a 61784180 raw words (47229506 effective words) took 350.6s, 134721 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47229506, 61784180)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_ocr.build_vocab(flattened_list_sentences, update=True)\n",
    "embedding_model_ocr.train(flattened_list_sentences, \n",
    "                          total_examples=embedding_model_ocr.corpus_count,\n",
    "                          epochs=w2v_args.epochs,  \n",
    "                          compute_loss=w2v_args.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:32:42,913 : INFO : saving Word2Vec object under ./w2v_005_embedding_model_ocr.model, separately None\n",
      "2019-11-13 13:32:42,914 : INFO : storing np array 'vectors' to ./w2v_005_embedding_model_ocr.model.wv.vectors.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] Save the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:32:45,335 : INFO : not storing attribute vectors_norm\n",
      "2019-11-13 13:32:45,336 : INFO : storing np array 'syn1neg' to ./w2v_005_embedding_model_ocr.model.trainables.syn1neg.npy\n",
      "2019-11-13 13:32:47,731 : INFO : not storing attribute cum_table\n",
      "2019-11-13 13:32:50,802 : INFO : saved ./w2v_005_embedding_model_ocr.model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n[INFO] Save the model\")\n",
    "embedding_model_ocr.save(\"./w2v_005_embedding_model_ocr.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences: 30509\n"
     ]
    }
   ],
   "source": [
    "list_sentences = db_sentence[\"corrected_sentencizer_cleaned\"].to_list()\n",
    "print('#sentences: {}'.format(len(list_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_sentences = [val for sublist in list_sentences for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'river',\n",
       " 'crossing',\n",
       " 'to',\n",
       " 'end',\n",
       " 'of',\n",
       " 'trial',\n",
       " 'splash',\n",
       " ':',\n",
       " 'peler',\n",
       " 'antill',\n",
       " 'ploughed',\n",
       " 'deep',\n",
       " 'into',\n",
       " 'paddy',\n",
       " \"'s\",\n",
       " 'river',\n",
       " 'in',\n",
       " 'his',\n",
       " 'chrysler',\n",
       " 'plymouth',\n",
       " 'during',\n",
       " 'the',\n",
       " 'elimination',\n",
       " 'section',\n",
       " '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_list_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_corrected = copy.deepcopy(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `min_count` (Attribute will be removed in 4.0.0, use self.vocabulary.min_count instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embedding_model_corrected.workers = 8\n",
    "embedding_model_corrected.min_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:33:01,833 : INFO : collecting all words and their counts\n",
      "2019-11-13 13:33:01,834 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-13 13:33:01,876 : INFO : PROGRESS: at sentence #10000, processed 238920 words, keeping 21363 word types\n",
      "2019-11-13 13:33:01,913 : INFO : PROGRESS: at sentence #20000, processed 462924 words, keeping 30432 word types\n",
      "2019-11-13 13:33:01,952 : INFO : PROGRESS: at sentence #30000, processed 706176 words, keeping 39228 word types\n",
      "2019-11-13 13:33:01,996 : INFO : PROGRESS: at sentence #40000, processed 967747 words, keeping 46390 word types\n",
      "2019-11-13 13:33:02,038 : INFO : PROGRESS: at sentence #50000, processed 1206328 words, keeping 52319 word types\n",
      "2019-11-13 13:33:02,083 : INFO : PROGRESS: at sentence #60000, processed 1454073 words, keeping 57131 word types\n",
      "2019-11-13 13:33:02,125 : INFO : PROGRESS: at sentence #70000, processed 1709317 words, keeping 61561 word types\n",
      "2019-11-13 13:33:02,166 : INFO : PROGRESS: at sentence #80000, processed 1953711 words, keeping 66452 word types\n",
      "2019-11-13 13:33:02,206 : INFO : PROGRESS: at sentence #90000, processed 2193623 words, keeping 70605 word types\n",
      "2019-11-13 13:33:02,247 : INFO : PROGRESS: at sentence #100000, processed 2442433 words, keeping 74867 word types\n",
      "2019-11-13 13:33:02,292 : INFO : PROGRESS: at sentence #110000, processed 2714060 words, keeping 78874 word types\n",
      "2019-11-13 13:33:02,332 : INFO : PROGRESS: at sentence #120000, processed 2958402 words, keeping 82525 word types\n",
      "2019-11-13 13:33:02,371 : INFO : PROGRESS: at sentence #130000, processed 3188124 words, keeping 85969 word types\n",
      "2019-11-13 13:33:02,414 : INFO : PROGRESS: at sentence #140000, processed 3437823 words, keeping 89967 word types\n",
      "2019-11-13 13:33:02,459 : INFO : PROGRESS: at sentence #150000, processed 3695828 words, keeping 94014 word types\n",
      "2019-11-13 13:33:02,503 : INFO : PROGRESS: at sentence #160000, processed 3941960 words, keeping 97060 word types\n",
      "2019-11-13 13:33:02,544 : INFO : PROGRESS: at sentence #170000, processed 4173711 words, keeping 100500 word types\n",
      "2019-11-13 13:33:02,583 : INFO : PROGRESS: at sentence #180000, processed 4407953 words, keeping 103674 word types\n",
      "2019-11-13 13:33:02,625 : INFO : PROGRESS: at sentence #190000, processed 4653900 words, keeping 106938 word types\n",
      "2019-11-13 13:33:02,668 : INFO : PROGRESS: at sentence #200000, processed 4894380 words, keeping 109826 word types\n",
      "2019-11-13 13:33:02,711 : INFO : PROGRESS: at sentence #210000, processed 5143048 words, keeping 112348 word types\n",
      "2019-11-13 13:33:02,756 : INFO : PROGRESS: at sentence #220000, processed 5407809 words, keeping 116127 word types\n",
      "2019-11-13 13:33:02,800 : INFO : PROGRESS: at sentence #230000, processed 5632256 words, keeping 118732 word types\n",
      "2019-11-13 13:33:02,839 : INFO : PROGRESS: at sentence #240000, processed 5857441 words, keeping 121618 word types\n",
      "2019-11-13 13:33:02,882 : INFO : PROGRESS: at sentence #250000, processed 6071938 words, keeping 123752 word types\n",
      "2019-11-13 13:33:02,922 : INFO : PROGRESS: at sentence #260000, processed 6302770 words, keeping 126469 word types\n",
      "2019-11-13 13:33:02,965 : INFO : PROGRESS: at sentence #270000, processed 6555040 words, keeping 129136 word types\n",
      "2019-11-13 13:33:03,007 : INFO : PROGRESS: at sentence #280000, processed 6802156 words, keeping 131406 word types\n",
      "2019-11-13 13:33:03,047 : INFO : PROGRESS: at sentence #290000, processed 7036644 words, keeping 133727 word types\n",
      "2019-11-13 13:33:03,088 : INFO : PROGRESS: at sentence #300000, processed 7259334 words, keeping 136544 word types\n",
      "2019-11-13 13:33:03,129 : INFO : PROGRESS: at sentence #310000, processed 7496943 words, keeping 139064 word types\n",
      "2019-11-13 13:33:03,171 : INFO : PROGRESS: at sentence #320000, processed 7746685 words, keeping 141765 word types\n",
      "2019-11-13 13:33:03,217 : INFO : PROGRESS: at sentence #330000, processed 7982859 words, keeping 143854 word types\n",
      "2019-11-13 13:33:03,256 : INFO : PROGRESS: at sentence #340000, processed 8213783 words, keeping 145598 word types\n",
      "2019-11-13 13:33:03,305 : INFO : PROGRESS: at sentence #350000, processed 8475485 words, keeping 148076 word types\n",
      "2019-11-13 13:33:03,346 : INFO : PROGRESS: at sentence #360000, processed 8710121 words, keeping 150482 word types\n",
      "2019-11-13 13:33:03,391 : INFO : PROGRESS: at sentence #370000, processed 8941968 words, keeping 152372 word types\n",
      "2019-11-13 13:33:03,437 : INFO : PROGRESS: at sentence #380000, processed 9193157 words, keeping 154924 word types\n",
      "2019-11-13 13:33:03,477 : INFO : PROGRESS: at sentence #390000, processed 9426266 words, keeping 156789 word types\n",
      "2019-11-13 13:33:03,521 : INFO : PROGRESS: at sentence #400000, processed 9684734 words, keeping 159635 word types\n",
      "2019-11-13 13:33:03,558 : INFO : PROGRESS: at sentence #410000, processed 9911276 words, keeping 161735 word types\n",
      "2019-11-13 13:33:03,593 : INFO : PROGRESS: at sentence #420000, processed 10141232 words, keeping 163536 word types\n",
      "2019-11-13 13:33:03,631 : INFO : PROGRESS: at sentence #430000, processed 10383723 words, keeping 165716 word types\n",
      "2019-11-13 13:33:03,670 : INFO : PROGRESS: at sentence #440000, processed 10622947 words, keeping 168137 word types\n",
      "2019-11-13 13:33:03,718 : INFO : PROGRESS: at sentence #450000, processed 10897590 words, keeping 170776 word types\n",
      "2019-11-13 13:33:03,756 : INFO : PROGRESS: at sentence #460000, processed 11139912 words, keeping 172884 word types\n",
      "2019-11-13 13:33:03,796 : INFO : PROGRESS: at sentence #470000, processed 11392785 words, keeping 174354 word types\n",
      "2019-11-13 13:33:03,844 : INFO : PROGRESS: at sentence #480000, processed 11643314 words, keeping 175435 word types\n",
      "2019-11-13 13:33:03,884 : INFO : PROGRESS: at sentence #490000, processed 11885014 words, keeping 176881 word types\n",
      "2019-11-13 13:33:03,924 : INFO : collected 179735 word types from a corpus of 12124087 raw words and 499269 sentences\n",
      "2019-11-13 13:33:03,925 : INFO : Updating model with new vocabulary\n",
      "2019-11-13 13:33:04,285 : INFO : New added 179735 unique words (50% of original 359470) and increased the count of 179735 pre-existing words (50% of original 359470)\n",
      "2019-11-13 13:33:05,392 : INFO : deleting the raw counts dictionary of 179735 items\n",
      "2019-11-13 13:33:05,397 : INFO : sample=0.001 downsamples 68 most-common words\n",
      "2019-11-13 13:33:05,407 : INFO : downsampling leaves estimated 16819762 word corpus (138.7% of prior 12124087)\n",
      "2019-11-13 13:33:06,696 : INFO : estimated required memory for 359470 words and 300 dimensions: 1042463000 bytes\n",
      "2019-11-13 13:33:06,697 : INFO : updating layer weights\n",
      "2019-11-13 13:33:21,365 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-13 13:33:21,366 : INFO : training model with 8 workers on 528940 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=20 window=5\n",
      "2019-11-13 13:33:22,477 : INFO : EPOCH 1 - PROGRESS: at 1.34% examples, 111326 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:23,558 : INFO : EPOCH 1 - PROGRESS: at 3.57% examples, 135129 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:24,569 : INFO : EPOCH 1 - PROGRESS: at 5.51% examples, 144478 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:25,597 : INFO : EPOCH 1 - PROGRESS: at 7.39% examples, 148373 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:26,624 : INFO : EPOCH 1 - PROGRESS: at 9.35% examples, 150775 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:27,663 : INFO : EPOCH 1 - PROGRESS: at 11.32% examples, 152154 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:28,695 : INFO : EPOCH 1 - PROGRESS: at 13.27% examples, 153261 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:29,803 : INFO : EPOCH 1 - PROGRESS: at 15.15% examples, 152770 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:30,910 : INFO : EPOCH 1 - PROGRESS: at 17.15% examples, 152316 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:31,980 : INFO : EPOCH 1 - PROGRESS: at 19.13% examples, 152421 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:32,990 : INFO : EPOCH 1 - PROGRESS: at 20.84% examples, 152813 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:34,017 : INFO : EPOCH 1 - PROGRESS: at 22.33% examples, 151281 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:35,049 : INFO : EPOCH 1 - PROGRESS: at 24.28% examples, 151403 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:33:36,106 : INFO : EPOCH 1 - PROGRESS: at 26.24% examples, 151265 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:37,142 : INFO : EPOCH 1 - PROGRESS: at 28.07% examples, 151362 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:38,268 : INFO : EPOCH 1 - PROGRESS: at 29.94% examples, 151063 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:39,358 : INFO : EPOCH 1 - PROGRESS: at 31.87% examples, 151133 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:40,422 : INFO : EPOCH 1 - PROGRESS: at 33.92% examples, 151424 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:41,513 : INFO : EPOCH 1 - PROGRESS: at 35.99% examples, 151415 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:42,585 : INFO : EPOCH 1 - PROGRESS: at 37.94% examples, 151523 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:43,616 : INFO : EPOCH 1 - PROGRESS: at 39.91% examples, 151841 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:44,658 : INFO : EPOCH 1 - PROGRESS: at 41.69% examples, 151581 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:45,671 : INFO : EPOCH 1 - PROGRESS: at 43.30% examples, 151229 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:46,734 : INFO : EPOCH 1 - PROGRESS: at 45.26% examples, 151433 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:47,756 : INFO : EPOCH 1 - PROGRESS: at 47.33% examples, 151659 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:48,779 : INFO : EPOCH 1 - PROGRESS: at 49.48% examples, 151959 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:49,811 : INFO : EPOCH 1 - PROGRESS: at 51.31% examples, 151600 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:50,817 : INFO : EPOCH 1 - PROGRESS: at 53.06% examples, 151396 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:51,822 : INFO : EPOCH 1 - PROGRESS: at 54.85% examples, 151621 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:52,830 : INFO : EPOCH 1 - PROGRESS: at 56.79% examples, 151801 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:53,848 : INFO : EPOCH 1 - PROGRESS: at 58.47% examples, 151330 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:54,974 : INFO : EPOCH 1 - PROGRESS: at 60.47% examples, 150889 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:33:56,148 : INFO : EPOCH 1 - PROGRESS: at 62.44% examples, 150598 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:33:57,303 : INFO : EPOCH 1 - PROGRESS: at 64.37% examples, 150381 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:58,319 : INFO : EPOCH 1 - PROGRESS: at 66.00% examples, 149781 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:33:59,360 : INFO : EPOCH 1 - PROGRESS: at 68.09% examples, 150005 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:00,467 : INFO : EPOCH 1 - PROGRESS: at 69.64% examples, 149497 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:01,483 : INFO : EPOCH 1 - PROGRESS: at 70.97% examples, 148459 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:02,508 : INFO : EPOCH 1 - PROGRESS: at 72.99% examples, 148634 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:03,540 : INFO : EPOCH 1 - PROGRESS: at 74.42% examples, 147796 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:04,543 : INFO : EPOCH 1 - PROGRESS: at 76.03% examples, 147560 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:05,663 : INFO : EPOCH 1 - PROGRESS: at 77.45% examples, 146324 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:06,748 : INFO : EPOCH 1 - PROGRESS: at 78.50% examples, 144949 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:07,765 : INFO : EPOCH 1 - PROGRESS: at 79.67% examples, 144150 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:08,794 : INFO : EPOCH 1 - PROGRESS: at 81.11% examples, 143360 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:09,982 : INFO : EPOCH 1 - PROGRESS: at 82.61% examples, 142282 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:11,075 : INFO : EPOCH 1 - PROGRESS: at 84.15% examples, 141624 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:12,165 : INFO : EPOCH 1 - PROGRESS: at 85.57% examples, 140917 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:13,166 : INFO : EPOCH 1 - PROGRESS: at 87.13% examples, 140721 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:14,184 : INFO : EPOCH 1 - PROGRESS: at 88.67% examples, 140492 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:15,249 : INFO : EPOCH 1 - PROGRESS: at 90.10% examples, 140282 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:16,258 : INFO : EPOCH 1 - PROGRESS: at 91.90% examples, 140454 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:17,280 : INFO : EPOCH 1 - PROGRESS: at 93.76% examples, 140711 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:18,283 : INFO : EPOCH 1 - PROGRESS: at 95.39% examples, 140790 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:19,302 : INFO : EPOCH 1 - PROGRESS: at 97.14% examples, 140803 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:20,351 : INFO : EPOCH 1 - PROGRESS: at 98.78% examples, 140656 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:20,893 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:34:21,011 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:34:21,036 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:34:21,059 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:34:21,136 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:34:21,158 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:34:21,164 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:34:21,251 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:34:21,252 : INFO : EPOCH - 1 : training on 12124087 raw words (8410137 effective words) took 59.9s, 140471 effective words/s\n",
      "2019-11-13 13:34:22,461 : INFO : EPOCH 2 - PROGRESS: at 1.34% examples, 102114 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:23,480 : INFO : EPOCH 2 - PROGRESS: at 3.24% examples, 119990 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:34:24,771 : INFO : EPOCH 2 - PROGRESS: at 4.90% examples, 115986 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:25,772 : INFO : EPOCH 2 - PROGRESS: at 6.60% examples, 122114 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:26,962 : INFO : EPOCH 2 - PROGRESS: at 8.04% examples, 119616 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:28,107 : INFO : EPOCH 2 - PROGRESS: at 9.35% examples, 115673 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:29,138 : INFO : EPOCH 2 - PROGRESS: at 10.90% examples, 117130 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:30,186 : INFO : EPOCH 2 - PROGRESS: at 11.96% examples, 113400 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:31,234 : INFO : EPOCH 2 - PROGRESS: at 12.58% examples, 107024 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:32,274 : INFO : EPOCH 2 - PROGRESS: at 13.93% examples, 107528 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:33,288 : INFO : EPOCH 2 - PROGRESS: at 15.23% examples, 107601 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:34:34,599 : INFO : EPOCH 2 - PROGRESS: at 15.80% examples, 100535 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-13 13:34:35,627 : INFO : EPOCH 2 - PROGRESS: at 17.15% examples, 101070 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:34:36,687 : INFO : EPOCH 2 - PROGRESS: at 18.59% examples, 101629 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:37,694 : INFO : EPOCH 2 - PROGRESS: at 19.80% examples, 102100 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:38,723 : INFO : EPOCH 2 - PROGRESS: at 21.42% examples, 104753 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:39,988 : INFO : EPOCH 2 - PROGRESS: at 22.11% examples, 100964 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:41,030 : INFO : EPOCH 2 - PROGRESS: at 23.35% examples, 101246 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:34:42,895 : INFO : EPOCH 2 - PROGRESS: at 24.78% examples, 97574 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:43,947 : INFO : EPOCH 2 - PROGRESS: at 25.58% examples, 95755 words/s, in_qsize 15, out_qsize 1\n",
      "2019-11-13 13:34:45,756 : INFO : EPOCH 2 - PROGRESS: at 26.16% examples, 90669 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:46,942 : INFO : EPOCH 2 - PROGRESS: at 26.81% examples, 88624 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:34:47,974 : INFO : EPOCH 2 - PROGRESS: at 27.77% examples, 88295 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:48,985 : INFO : EPOCH 2 - PROGRESS: at 28.93% examples, 88546 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:50,039 : INFO : EPOCH 2 - PROGRESS: at 29.94% examples, 88668 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:34:51,238 : INFO : EPOCH 2 - PROGRESS: at 31.50% examples, 89493 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:34:52,286 : INFO : EPOCH 2 - PROGRESS: at 33.26% examples, 91188 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:53,422 : INFO : EPOCH 2 - PROGRESS: at 34.91% examples, 92053 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:54,479 : INFO : EPOCH 2 - PROGRESS: at 36.59% examples, 93247 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:55,496 : INFO : EPOCH 2 - PROGRESS: at 37.78% examples, 93487 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:56,568 : INFO : EPOCH 2 - PROGRESS: at 38.83% examples, 93158 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:57,613 : INFO : EPOCH 2 - PROGRESS: at 39.83% examples, 92730 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:58,618 : INFO : EPOCH 2 - PROGRESS: at 40.90% examples, 92644 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:34:59,669 : INFO : EPOCH 2 - PROGRESS: at 42.18% examples, 92980 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:00,679 : INFO : EPOCH 2 - PROGRESS: at 43.37% examples, 93402 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:01,872 : INFO : EPOCH 2 - PROGRESS: at 44.75% examples, 93553 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:02,884 : INFO : EPOCH 2 - PROGRESS: at 46.71% examples, 94935 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:04,157 : INFO : EPOCH 2 - PROGRESS: at 48.34% examples, 95110 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:05,470 : INFO : EPOCH 2 - PROGRESS: at 50.54% examples, 96121 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:06,475 : INFO : EPOCH 2 - PROGRESS: at 52.28% examples, 97070 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:07,558 : INFO : EPOCH 2 - PROGRESS: at 53.82% examples, 97790 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:08,558 : INFO : EPOCH 2 - PROGRESS: at 55.24% examples, 98345 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:09,589 : INFO : EPOCH 2 - PROGRESS: at 56.96% examples, 99097 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:10,600 : INFO : EPOCH 2 - PROGRESS: at 58.65% examples, 99892 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:11,663 : INFO : EPOCH 2 - PROGRESS: at 60.56% examples, 100732 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:12,691 : INFO : EPOCH 2 - PROGRESS: at 62.13% examples, 101286 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:13,771 : INFO : EPOCH 2 - PROGRESS: at 63.31% examples, 101189 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:14,784 : INFO : EPOCH 2 - PROGRESS: at 65.01% examples, 101839 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:15,831 : INFO : EPOCH 2 - PROGRESS: at 66.74% examples, 102412 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:16,890 : INFO : EPOCH 2 - PROGRESS: at 68.52% examples, 103175 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:17,934 : INFO : EPOCH 2 - PROGRESS: at 69.97% examples, 103606 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:35:18,974 : INFO : EPOCH 2 - PROGRESS: at 71.21% examples, 103535 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:20,048 : INFO : EPOCH 2 - PROGRESS: at 72.99% examples, 103996 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:21,243 : INFO : EPOCH 2 - PROGRESS: at 74.62% examples, 104120 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:22,314 : INFO : EPOCH 2 - PROGRESS: at 75.86% examples, 104106 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:23,355 : INFO : EPOCH 2 - PROGRESS: at 77.25% examples, 104142 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:24,434 : INFO : EPOCH 2 - PROGRESS: at 78.50% examples, 104107 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:25,441 : INFO : EPOCH 2 - PROGRESS: at 80.13% examples, 104735 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:26,448 : INFO : EPOCH 2 - PROGRESS: at 81.57% examples, 104829 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:27,479 : INFO : EPOCH 2 - PROGRESS: at 83.31% examples, 105267 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:28,497 : INFO : EPOCH 2 - PROGRESS: at 84.62% examples, 105311 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:29,573 : INFO : EPOCH 2 - PROGRESS: at 86.48% examples, 105885 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:30,595 : INFO : EPOCH 2 - PROGRESS: at 87.88% examples, 106012 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:31,596 : INFO : EPOCH 2 - PROGRESS: at 89.40% examples, 106464 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:35:32,602 : INFO : EPOCH 2 - PROGRESS: at 90.91% examples, 106905 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:33,708 : INFO : EPOCH 2 - PROGRESS: at 92.82% examples, 107436 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:35:34,709 : INFO : EPOCH 2 - PROGRESS: at 94.39% examples, 107853 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:35,734 : INFO : EPOCH 2 - PROGRESS: at 95.96% examples, 108225 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:36,743 : INFO : EPOCH 2 - PROGRESS: at 97.80% examples, 108790 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:37,764 : INFO : EPOCH 2 - PROGRESS: at 99.36% examples, 109069 words/s, in_qsize 10, out_qsize 0\n",
      "2019-11-13 13:35:37,888 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:35:37,938 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:35:37,953 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:35:37,995 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:35:38,061 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:35:38,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:35:38,130 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:35:38,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:35:38,238 : INFO : EPOCH - 2 : training on 12124087 raw words (8410611 effective words) took 77.0s, 109270 effective words/s\n",
      "2019-11-13 13:35:39,545 : INFO : EPOCH 3 - PROGRESS: at 1.34% examples, 95444 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:40,755 : INFO : EPOCH 3 - PROGRESS: at 3.59% examples, 118289 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:41,927 : INFO : EPOCH 3 - PROGRESS: at 5.51% examples, 126036 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:42,977 : INFO : EPOCH 3 - PROGRESS: at 7.05% examples, 125655 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:44,123 : INFO : EPOCH 3 - PROGRESS: at 8.72% examples, 125776 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:35:45,211 : INFO : EPOCH 3 - PROGRESS: at 10.65% examples, 129882 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:46,232 : INFO : EPOCH 3 - PROGRESS: at 12.18% examples, 129673 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:47,308 : INFO : EPOCH 3 - PROGRESS: at 13.93% examples, 130993 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:48,359 : INFO : EPOCH 3 - PROGRESS: at 15.80% examples, 132906 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:49,378 : INFO : EPOCH 3 - PROGRESS: at 17.65% examples, 134445 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:50,434 : INFO : EPOCH 3 - PROGRESS: at 19.34% examples, 134546 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:35:51,484 : INFO : EPOCH 3 - PROGRESS: at 21.05% examples, 135825 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:52,563 : INFO : EPOCH 3 - PROGRESS: at 22.65% examples, 135679 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:53,608 : INFO : EPOCH 3 - PROGRESS: at 24.35% examples, 135380 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:35:54,616 : INFO : EPOCH 3 - PROGRESS: at 26.24% examples, 136287 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:55,628 : INFO : EPOCH 3 - PROGRESS: at 27.91% examples, 136680 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:56,789 : INFO : EPOCH 3 - PROGRESS: at 29.53% examples, 135532 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:35:57,947 : INFO : EPOCH 3 - PROGRESS: at 31.17% examples, 134910 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:58,947 : INFO : EPOCH 3 - PROGRESS: at 32.82% examples, 135133 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:35:59,972 : INFO : EPOCH 3 - PROGRESS: at 34.81% examples, 136075 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:01,025 : INFO : EPOCH 3 - PROGRESS: at 36.84% examples, 137012 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:02,037 : INFO : EPOCH 3 - PROGRESS: at 38.65% examples, 137815 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:03,219 : INFO : EPOCH 3 - PROGRESS: at 40.09% examples, 135920 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:04,286 : INFO : EPOCH 3 - PROGRESS: at 41.77% examples, 135928 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:05,292 : INFO : EPOCH 3 - PROGRESS: at 43.23% examples, 135728 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:06,660 : INFO : EPOCH 3 - PROGRESS: at 44.49% examples, 133075 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:07,775 : INFO : EPOCH 3 - PROGRESS: at 45.88% examples, 131810 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:08,874 : INFO : EPOCH 3 - PROGRESS: at 47.33% examples, 130730 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:09,880 : INFO : EPOCH 3 - PROGRESS: at 49.38% examples, 131519 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:10,884 : INFO : EPOCH 3 - PROGRESS: at 50.70% examples, 130695 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:11,897 : INFO : EPOCH 3 - PROGRESS: at 52.28% examples, 130504 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:12,914 : INFO : EPOCH 3 - PROGRESS: at 53.52% examples, 129872 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:13,938 : INFO : EPOCH 3 - PROGRESS: at 54.92% examples, 129628 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:14,953 : INFO : EPOCH 3 - PROGRESS: at 56.79% examples, 130179 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:15,998 : INFO : EPOCH 3 - PROGRESS: at 58.31% examples, 129899 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:17,006 : INFO : EPOCH 3 - PROGRESS: at 59.44% examples, 128875 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:18,050 : INFO : EPOCH 3 - PROGRESS: at 60.82% examples, 128159 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:19,177 : INFO : EPOCH 3 - PROGRESS: at 62.44% examples, 128020 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:20,192 : INFO : EPOCH 3 - PROGRESS: at 64.13% examples, 128399 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:21,209 : INFO : EPOCH 3 - PROGRESS: at 65.72% examples, 128397 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:22,212 : INFO : EPOCH 3 - PROGRESS: at 67.75% examples, 129056 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:23,219 : INFO : EPOCH 3 - PROGRESS: at 69.32% examples, 129402 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:24,284 : INFO : EPOCH 3 - PROGRESS: at 70.97% examples, 129417 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:25,338 : INFO : EPOCH 3 - PROGRESS: at 72.99% examples, 129898 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:26,370 : INFO : EPOCH 3 - PROGRESS: at 74.70% examples, 129989 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:27,420 : INFO : EPOCH 3 - PROGRESS: at 76.31% examples, 130174 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:28,427 : INFO : EPOCH 3 - PROGRESS: at 78.19% examples, 130576 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:29,557 : INFO : EPOCH 3 - PROGRESS: at 79.84% examples, 130658 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:30,563 : INFO : EPOCH 3 - PROGRESS: at 81.73% examples, 130932 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:31,614 : INFO : EPOCH 3 - PROGRESS: at 83.80% examples, 131429 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:32,653 : INFO : EPOCH 3 - PROGRESS: at 85.74% examples, 131859 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:33,659 : INFO : EPOCH 3 - PROGRESS: at 87.63% examples, 132313 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:34,692 : INFO : EPOCH 3 - PROGRESS: at 89.10% examples, 132222 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:35,744 : INFO : EPOCH 3 - PROGRESS: at 90.41% examples, 131974 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:36,784 : INFO : EPOCH 3 - PROGRESS: at 92.32% examples, 132311 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:37,808 : INFO : EPOCH 3 - PROGRESS: at 93.84% examples, 132232 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:38,875 : INFO : EPOCH 3 - PROGRESS: at 95.63% examples, 132527 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:39,967 : INFO : EPOCH 3 - PROGRESS: at 97.63% examples, 132854 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:40,981 : INFO : EPOCH 3 - PROGRESS: at 99.46% examples, 133149 words/s, in_qsize 9, out_qsize 0\n",
      "2019-11-13 13:36:41,138 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:36:41,207 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:36:41,208 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:36:41,249 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:36:41,311 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:36:41,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:36:41,335 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:36:41,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:36:41,396 : INFO : EPOCH - 3 : training on 12124087 raw words (8410294 effective words) took 63.1s, 133225 effective words/s\n",
      "2019-11-13 13:36:42,455 : INFO : EPOCH 4 - PROGRESS: at 0.90% examples, 75981 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:43,859 : INFO : EPOCH 4 - PROGRESS: at 2.85% examples, 96961 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:44,877 : INFO : EPOCH 4 - PROGRESS: at 4.81% examples, 115544 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:45,946 : INFO : EPOCH 4 - PROGRESS: at 6.19% examples, 113886 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:47,178 : INFO : EPOCH 4 - PROGRESS: at 8.01% examples, 118176 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:48,322 : INFO : EPOCH 4 - PROGRESS: at 10.02% examples, 122510 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:49,547 : INFO : EPOCH 4 - PROGRESS: at 11.96% examples, 124355 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:50,555 : INFO : EPOCH 4 - PROGRESS: at 13.67% examples, 126445 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:36:51,637 : INFO : EPOCH 4 - PROGRESS: at 15.15% examples, 125889 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:52,688 : INFO : EPOCH 4 - PROGRESS: at 17.15% examples, 128744 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:53,727 : INFO : EPOCH 4 - PROGRESS: at 19.05% examples, 130633 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:36:54,749 : INFO : EPOCH 4 - PROGRESS: at 20.71% examples, 131969 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:55,770 : INFO : EPOCH 4 - PROGRESS: at 22.19% examples, 132162 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:56,789 : INFO : EPOCH 4 - PROGRESS: at 23.98% examples, 133262 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:57,794 : INFO : EPOCH 4 - PROGRESS: at 25.92% examples, 134290 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:58,819 : INFO : EPOCH 4 - PROGRESS: at 27.46% examples, 133941 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:36:59,869 : INFO : EPOCH 4 - PROGRESS: at 29.39% examples, 135249 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:00,925 : INFO : EPOCH 4 - PROGRESS: at 31.26% examples, 136399 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:01,949 : INFO : EPOCH 4 - PROGRESS: at 33.26% examples, 137738 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:02,971 : INFO : EPOCH 4 - PROGRESS: at 35.18% examples, 138264 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:03,980 : INFO : EPOCH 4 - PROGRESS: at 37.07% examples, 139074 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:05,006 : INFO : EPOCH 4 - PROGRESS: at 38.75% examples, 139098 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:06,013 : INFO : EPOCH 4 - PROGRESS: at 40.65% examples, 139814 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:37:07,052 : INFO : EPOCH 4 - PROGRESS: at 42.18% examples, 139273 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:08,106 : INFO : EPOCH 4 - PROGRESS: at 43.37% examples, 137915 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:09,268 : INFO : EPOCH 4 - PROGRESS: at 45.18% examples, 137625 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:37:10,328 : INFO : EPOCH 4 - PROGRESS: at 46.80% examples, 136889 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:11,447 : INFO : EPOCH 4 - PROGRESS: at 48.81% examples, 136991 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:12,466 : INFO : EPOCH 4 - PROGRESS: at 50.70% examples, 137261 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:13,694 : INFO : EPOCH 4 - PROGRESS: at 52.37% examples, 136146 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:14,753 : INFO : EPOCH 4 - PROGRESS: at 53.77% examples, 135547 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:15,789 : INFO : EPOCH 4 - PROGRESS: at 55.68% examples, 136275 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:16,844 : INFO : EPOCH 4 - PROGRESS: at 57.72% examples, 136894 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:17,952 : INFO : EPOCH 4 - PROGRESS: at 59.83% examples, 137366 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:19,064 : INFO : EPOCH 4 - PROGRESS: at 61.89% examples, 137767 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:20,168 : INFO : EPOCH 4 - PROGRESS: at 63.79% examples, 138146 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:21,286 : INFO : EPOCH 4 - PROGRESS: at 65.72% examples, 138229 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:22,331 : INFO : EPOCH 4 - PROGRESS: at 67.44% examples, 137878 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:37:23,399 : INFO : EPOCH 4 - PROGRESS: at 68.87% examples, 137492 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:24,458 : INFO : EPOCH 4 - PROGRESS: at 70.23% examples, 136854 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:25,480 : INFO : EPOCH 4 - PROGRESS: at 71.83% examples, 136663 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:26,486 : INFO : EPOCH 4 - PROGRESS: at 73.67% examples, 136826 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:27,556 : INFO : EPOCH 4 - PROGRESS: at 75.06% examples, 136051 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:28,580 : INFO : EPOCH 4 - PROGRESS: at 76.70% examples, 136321 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:37:29,610 : INFO : EPOCH 4 - PROGRESS: at 78.50% examples, 136414 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:30,632 : INFO : EPOCH 4 - PROGRESS: at 79.76% examples, 135965 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:31,633 : INFO : EPOCH 4 - PROGRESS: at 81.11% examples, 135323 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:32,766 : INFO : EPOCH 4 - PROGRESS: at 82.25% examples, 134092 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:37:33,770 : INFO : EPOCH 4 - PROGRESS: at 84.15% examples, 134395 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:34,834 : INFO : EPOCH 4 - PROGRESS: at 85.67% examples, 134064 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:35,876 : INFO : EPOCH 4 - PROGRESS: at 86.96% examples, 133517 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:36,893 : INFO : EPOCH 4 - PROGRESS: at 88.47% examples, 133309 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:37,907 : INFO : EPOCH 4 - PROGRESS: at 90.01% examples, 133611 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:38,946 : INFO : EPOCH 4 - PROGRESS: at 91.42% examples, 133232 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:40,243 : INFO : EPOCH 4 - PROGRESS: at 93.38% examples, 133090 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:37:41,340 : INFO : EPOCH 4 - PROGRESS: at 95.25% examples, 133422 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:42,387 : INFO : EPOCH 4 - PROGRESS: at 97.22% examples, 133845 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:43,462 : INFO : EPOCH 4 - PROGRESS: at 99.21% examples, 134218 words/s, in_qsize 12, out_qsize 0\n",
      "2019-11-13 13:37:43,623 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:37:43,743 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:37:43,763 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:37:43,764 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:37:43,857 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:37:43,864 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:37:43,887 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:37:43,937 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:37:43,938 : INFO : EPOCH - 4 : training on 12124087 raw words (8408547 effective words) took 62.5s, 134487 effective words/s\n",
      "2019-11-13 13:37:45,085 : INFO : EPOCH 5 - PROGRESS: at 1.34% examples, 108122 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:37:46,225 : INFO : EPOCH 5 - PROGRESS: at 3.59% examples, 129690 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:47,295 : INFO : EPOCH 5 - PROGRESS: at 5.51% examples, 138045 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:48,455 : INFO : EPOCH 5 - PROGRESS: at 7.39% examples, 139140 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:49,497 : INFO : EPOCH 5 - PROGRESS: at 9.35% examples, 142775 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:50,680 : INFO : EPOCH 5 - PROGRESS: at 11.32% examples, 142222 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:51,692 : INFO : EPOCH 5 - PROGRESS: at 12.65% examples, 138792 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:52,712 : INFO : EPOCH 5 - PROGRESS: at 14.47% examples, 140700 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:53,793 : INFO : EPOCH 5 - PROGRESS: at 16.47% examples, 141920 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:54,816 : INFO : EPOCH 5 - PROGRESS: at 17.89% examples, 139367 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:55,826 : INFO : EPOCH 5 - PROGRESS: at 19.50% examples, 139013 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:57,005 : INFO : EPOCH 5 - PROGRESS: at 20.40% examples, 132798 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:37:58,005 : INFO : EPOCH 5 - PROGRESS: at 22.11% examples, 134565 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:37:59,073 : INFO : EPOCH 5 - PROGRESS: at 24.08% examples, 135991 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:00,075 : INFO : EPOCH 5 - PROGRESS: at 25.99% examples, 136908 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:01,084 : INFO : EPOCH 5 - PROGRESS: at 27.39% examples, 135695 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:38:02,136 : INFO : EPOCH 5 - PROGRESS: at 28.75% examples, 134244 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:03,244 : INFO : EPOCH 5 - PROGRESS: at 30.01% examples, 132640 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:04,267 : INFO : EPOCH 5 - PROGRESS: at 31.81% examples, 133443 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:05,373 : INFO : EPOCH 5 - PROGRESS: at 33.34% examples, 132382 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:06,428 : INFO : EPOCH 5 - PROGRESS: at 35.26% examples, 132938 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:07,467 : INFO : EPOCH 5 - PROGRESS: at 36.84% examples, 132577 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:08,477 : INFO : EPOCH 5 - PROGRESS: at 38.65% examples, 133545 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:09,484 : INFO : EPOCH 5 - PROGRESS: at 40.57% examples, 134437 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:10,486 : INFO : EPOCH 5 - PROGRESS: at 42.40% examples, 135357 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:11,503 : INFO : EPOCH 5 - PROGRESS: at 43.98% examples, 135607 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:12,533 : INFO : EPOCH 5 - PROGRESS: at 45.99% examples, 136317 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:13,628 : INFO : EPOCH 5 - PROGRESS: at 47.97% examples, 136506 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:14,677 : INFO : EPOCH 5 - PROGRESS: at 49.57% examples, 135778 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:15,687 : INFO : EPOCH 5 - PROGRESS: at 50.62% examples, 134099 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:38:16,809 : INFO : EPOCH 5 - PROGRESS: at 52.45% examples, 133987 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:17,813 : INFO : EPOCH 5 - PROGRESS: at 54.23% examples, 134697 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:18,902 : INFO : EPOCH 5 - PROGRESS: at 56.09% examples, 135027 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:20,074 : INFO : EPOCH 5 - PROGRESS: at 57.46% examples, 133715 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:21,090 : INFO : EPOCH 5 - PROGRESS: at 58.91% examples, 133256 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:22,148 : INFO : EPOCH 5 - PROGRESS: at 60.47% examples, 132721 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:38:23,161 : INFO : EPOCH 5 - PROGRESS: at 62.05% examples, 132668 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:38:24,206 : INFO : EPOCH 5 - PROGRESS: at 63.16% examples, 131642 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:25,207 : INFO : EPOCH 5 - PROGRESS: at 65.01% examples, 132121 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:38:26,304 : INFO : EPOCH 5 - PROGRESS: at 66.75% examples, 131948 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-13 13:38:27,378 : INFO : EPOCH 5 - PROGRESS: at 68.45% examples, 132006 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-13 13:38:28,385 : INFO : EPOCH 5 - PROGRESS: at 70.04% examples, 132301 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:29,401 : INFO : EPOCH 5 - PROGRESS: at 71.67% examples, 132242 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:30,440 : INFO : EPOCH 5 - PROGRESS: at 73.78% examples, 132848 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:31,515 : INFO : EPOCH 5 - PROGRESS: at 75.68% examples, 133339 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:38:32,523 : INFO : EPOCH 5 - PROGRESS: at 77.58% examples, 133704 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:33,662 : INFO : EPOCH 5 - PROGRESS: at 79.21% examples, 133692 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:34,752 : INFO : EPOCH 5 - PROGRESS: at 81.31% examples, 134094 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:35,889 : INFO : EPOCH 5 - PROGRESS: at 83.39% examples, 134345 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:36,913 : INFO : EPOCH 5 - PROGRESS: at 84.87% examples, 134093 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:37,957 : INFO : EPOCH 5 - PROGRESS: at 86.48% examples, 133935 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-13 13:38:38,968 : INFO : EPOCH 5 - PROGRESS: at 88.15% examples, 133985 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:40,030 : INFO : EPOCH 5 - PROGRESS: at 89.58% examples, 133916 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:41,073 : INFO : EPOCH 5 - PROGRESS: at 91.00% examples, 133642 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:42,163 : INFO : EPOCH 5 - PROGRESS: at 92.99% examples, 133958 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:43,202 : INFO : EPOCH 5 - PROGRESS: at 94.79% examples, 134290 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:44,210 : INFO : EPOCH 5 - PROGRESS: at 96.54% examples, 134562 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:45,228 : INFO : EPOCH 5 - PROGRESS: at 98.28% examples, 134699 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-13 13:38:45,971 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-13 13:38:46,046 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-13 13:38:46,052 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-13 13:38:46,094 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-13 13:38:46,117 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-13 13:38:46,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-13 13:38:46,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-13 13:38:46,298 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 134910 words/s, in_qsize 0, out_qsize 1\n",
      "2019-11-13 13:38:46,299 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-13 13:38:46,299 : INFO : EPOCH - 5 : training on 12124087 raw words (8410292 effective words) took 62.3s, 134907 effective words/s\n",
      "2019-11-13 13:38:46,302 : INFO : training on a 60620435 raw words (42049881 effective words) took 324.9s, 129411 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42049881, 60620435)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_corrected.build_vocab(flattened_list_sentences, update=True)\n",
    "embedding_model_corrected.train(flattened_list_sentences, \n",
    "                                total_examples=embedding_model_corrected.corpus_count,\n",
    "                                epochs=w2v_args.epochs,  \n",
    "                                compute_loss=w2v_args.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:38:46,350 : INFO : saving Word2Vec object under ./w2v_005_embedding_model_corrected.model, separately None\n",
      "2019-11-13 13:38:46,354 : INFO : storing np array 'vectors' to ./w2v_005_embedding_model_corrected.model.wv.vectors.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] Save the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-13 13:38:48,917 : INFO : not storing attribute vectors_norm\n",
      "2019-11-13 13:38:48,918 : INFO : storing np array 'syn1neg' to ./w2v_005_embedding_model_corrected.model.trainables.syn1neg.npy\n",
      "2019-11-13 13:38:50,261 : INFO : not storing attribute cum_table\n",
      "2019-11-13 13:38:51,857 : INFO : saved ./w2v_005_embedding_model_corrected.model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n[INFO] Save the model\")\n",
    "embedding_model_corrected.save(\"./w2v_005_embedding_model_corrected.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
