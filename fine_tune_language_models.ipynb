{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import copy\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:40:47,233 : INFO : loading Word2Vec object from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\n",
      "2019-11-20 09:40:48,248 : INFO : loading wv recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.wv.* with mmap=None\n",
      "2019-11-20 09:40:48,249 : INFO : loading vectors from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.wv.vectors.npy with mmap=None\n",
      "2019-11-20 09:40:48,544 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-20 09:40:48,545 : INFO : loading vocabulary recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.vocabulary.* with mmap=None\n",
      "2019-11-20 09:40:48,545 : INFO : loading trainables recursively from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.trainables.* with mmap=None\n",
      "2019-11-20 09:40:48,546 : INFO : loading syn1neg from /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-11-20 09:40:48,842 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-20 09:40:48,843 : INFO : loaded /Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=434049, size=300, alpha=0.03)\n"
     ]
    }
   ],
   "source": [
    "# model types\n",
    "# w2v: word2vec\n",
    "# ft: fasttext\n",
    "model_type = \"w2v\"   \n",
    "model_path = \"/Users/khosseini/myJobs/ATI/Projects/2019/Living-with-Machines-code/language-lab-mro/lexicon_expansion/interactive_expansion/models/all_books/w2v_005/w2v_words.model\"\n",
    "\n",
    "if model_type.lower() in [\"w2v\", \"word2vec\"]:\n",
    "    # Word2Vec\n",
    "    embedding_model_orig = Word2Vec.load(model_path)\n",
    "elif model_type.lower() in [\"ft\", \"fasttext\"]:\n",
    "    # FastText\n",
    "    embedding_model_orig = FastText.load(model_path)\n",
    "print(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007000009816451012"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_alpha_yet_reached = embedding_model_orig.min_alpha_yet_reached\n",
    "min_alpha_yet_reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_sentence_orig = pd.read_pickle(\"./db_trove_v002.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality bands 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    3600\n",
       "4    1495\n",
       "Name: quality_band, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentence = db_sentence_orig[(db_sentence_orig['quality_band'] == 3) | \n",
    "                               (db_sentence_orig['quality_band'] == 4)]\n",
    "db_sentence['quality_band'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef cleanup(myrow, colname=\"corrected\"):\\n    # remove all # and @§\\n    \\n    corpus = [re.sub(r\\'#\\', \\'\\', element, flags=re.IGNORECASE) for element in corpus]\\n    corpus = [re.sub(r\\'@\\', \\'\\', element, flags=re.IGNORECASE) for element in corpus]\\n    \\n    # --- remove 2 or more .\\n    corpus = [re.sub(\\'[.]{2,}\\', \\'.\\', element) for element in corpus]\\n    # --- add a space before and after a list of punctuations\\n    corpus = [re.sub(r\"([.,!?:;\"\\'])\", r\" \\x01 \", element) for element in corpus]\\n    # --- remove everything except:\\n    #corpus = [re.sub(r\"([^a-zA-Z\\\\-.:;,!?\\\\d+]+)\", r\" \", element) for element in corpus]\\n    corpus = [re.sub(r\"([^a-zA-Z\\\\d+]+)\", r\" \", element) for element in corpus]\\n    # --- replace numbers with <NUM>\\n    corpus = [re.sub(r\\'\\x08\\\\d+\\x08\\', \\'<NUM>\\', element) for element in corpus]\\n    corpus = [re.sub(\\'--\\', \\'\\', element) for element in corpus]\\n    # --- normalize white spaces\\n    corpus = [re.sub(\\'\\\\s+\\', \\' \\', element) for element in corpus]\\n    \\n    # remove multiple spaces\\n    corpus = [re.sub(r\\'\\\\s+\\', \\' \\', element, flags=re.IGNORECASE) for element in corpus]\\n    corpus = [element.strip() for element in corpus]\\n    #corpus = [element.lower() for element in corpus]\\n    return corpus\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def cleanup(myrow, colname=\"corrected\"):\n",
    "    # remove all # and @§\n",
    "    \n",
    "    corpus = [re.sub(r'#', '', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [re.sub(r'@', '', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    \n",
    "    # --- remove 2 or more .\n",
    "    corpus = [re.sub('[.]{2,}', '.', element) for element in corpus]\n",
    "    # --- add a space before and after a list of punctuations\n",
    "    corpus = [re.sub(r\"([.,!?:;\\\"\\'])\", r\" \\1 \", element) for element in corpus]\n",
    "    # --- remove everything except:\n",
    "    #corpus = [re.sub(r\"([^a-zA-Z\\-.:;,!?\\d+]+)\", r\" \", element) for element in corpus]\n",
    "    corpus = [re.sub(r\"([^a-zA-Z\\d+]+)\", r\" \", element) for element in corpus]\n",
    "    # --- replace numbers with <NUM>\n",
    "    corpus = [re.sub(r'\\b\\d+\\b', '<NUM>', element) for element in corpus]\n",
    "    corpus = [re.sub('--', '', element) for element in corpus]\n",
    "    # --- normalize white spaces\n",
    "    corpus = [re.sub('\\s+', ' ', element) for element in corpus]\n",
    "    \n",
    "    # remove multiple spaces\n",
    "    corpus = [re.sub(r'\\s+', ' ', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [element.strip() for element in corpus]\n",
    "    #corpus = [element.lower() for element in corpus]\n",
    "    return corpus\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(myrow, col_name):\n",
    "    all_clean_rows = []\n",
    "    for sent in myrow[col_name]:\n",
    "        one_clean_row = []\n",
    "        for token in sent:\n",
    "            one_clean_row.append(token.lower())\n",
    "        all_clean_rows.append(one_clean_row)\n",
    "    return all_clean_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "db_sentence[\"ocr_sentencizer_cleaned\"] = db_sentence.apply(cleanup, args=[\"ocr_sentencizer\"], axis=1)\n",
    "db_sentence[\"corrected_sentencizer_cleaned\"] = db_sentence.apply(cleanup, args=[\"corrected_sentencizer\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5095, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>articleId</th>\n",
       "      <th>articleType</th>\n",
       "      <th>year</th>\n",
       "      <th>ocrText</th>\n",
       "      <th>humanText</th>\n",
       "      <th>corrected</th>\n",
       "      <th>str_similarity</th>\n",
       "      <th>str_length_humanText</th>\n",
       "      <th>str_length_ocrText</th>\n",
       "      <th>...</th>\n",
       "      <th>ocr_dict_lookup</th>\n",
       "      <th>ocr_dict_lookup_list</th>\n",
       "      <th>ocr_dict_perc</th>\n",
       "      <th>corrected_dict_lookup_list</th>\n",
       "      <th>corr_dict_perc</th>\n",
       "      <th>corrected_sentencizer_list</th>\n",
       "      <th>ocr_sentencizer_list</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <th>ocr_sentencizer_cleaned</th>\n",
       "      <th>corrected_sentencizer_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18366055</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>FIRST CHURCH I SERVICE 1 Presbyterian I ' Anni...</td>\n",
       "      <td>FIRST CHURCH SERVICE Presbyterian Anniversary ...</td>\n",
       "      <td>FIRST CHURCH SERVICE Presbyterian Anniversary ...</td>\n",
       "      <td>0.738901</td>\n",
       "      <td>946</td>\n",
       "      <td>832</td>\n",
       "      <td>...</td>\n",
       "      <td>[[5, 6, 1, 7, 1, 12, 1, 1, 11, 1, 3, 5, -12, 3...</td>\n",
       "      <td>[5, 6, 1, 7, 1, 12, 1, 1, 11, 1, 3, 5, -12, 3,...</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>[5, 6, 7, 12, 11, 3, 5, 11, 2, 3, 5, 12, 6, 7,...</td>\n",
       "      <td>99.206349</td>\n",
       "      <td>[FIRST, CHURCH, SERVICE, Presbyterian, Anniver...</td>\n",
       "      <td>[FIRST, CHURCH, I, SERVICE, 1, Presbyterian, I...</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>[[first, church, i, service, 1, presbyterian, ...</td>\n",
       "      <td>[[first, church, service, presbyterian, annive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18386137</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium District...</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium  Arthur ...</td>\n",
       "      <td>\"Bob\" Lulham's Fight Against Thallium  Arthur ...</td>\n",
       "      <td>0.493898</td>\n",
       "      <td>2950</td>\n",
       "      <td>2740</td>\n",
       "      <td>...</td>\n",
       "      <td>[[1, 3, 1, -6, 2, 5, 7, 8, 8, 8, 1], [4, 5, 6,...</td>\n",
       "      <td>[1, 3, 1, -6, 2, 5, 7, 8, 8, 8, 1, 4, 5, 6, 6,...</td>\n",
       "      <td>95.580110</td>\n",
       "      <td>[1, 3, 1, -6, 2, 5, 7, 8, 6, 6, 1, 1, 3, 1, 1,...</td>\n",
       "      <td>95.652174</td>\n",
       "      <td>[\", Bob, \", Lulham, 's, Fight, Against, Thalli...</td>\n",
       "      <td>[\", Bob, \", Lulham, 's, Fight, Against, Thalli...</td>\n",
       "      <td>0.282682</td>\n",
       "      <td>[[\", bob, \", lulham, 's, fight, against, thall...</td>\n",
       "      <td>[[\", bob, \", lulham, 's, fight, against, thall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18391223</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>rchitect For lympics aid £31,665 - ?? MELBOURN...</td>\n",
       "      <td>Architect For Olympics paid £31,665  MELBOURNE...</td>\n",
       "      <td>Architect For Olympics paid £31,665  MELBOURNE...</td>\n",
       "      <td>0.793510</td>\n",
       "      <td>678</td>\n",
       "      <td>585</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-8, 3, 7, 3, 1, -6, 1, 2, 9, 1, 6], [1, 1, 5...</td>\n",
       "      <td>[-8, 3, 7, 3, 1, -6, 1, 2, 9, 1, 6, 1, 1, 5, 5...</td>\n",
       "      <td>88.607595</td>\n",
       "      <td>[9, 3, 8, 4, 1, -6, 9, 1, 6, 2, 5, 5, 1, 3, 9,...</td>\n",
       "      <td>94.680851</td>\n",
       "      <td>[Architect, For, Olympics, paid, £, 31,665, ME...</td>\n",
       "      <td>[rchitect, For, lympics, aid, £, 31,665, -, ??...</td>\n",
       "      <td>0.302885</td>\n",
       "      <td>[[rchitect, for, lympics, aid, £, 31,665, -, ?...</td>\n",
       "      <td>[[architect, for, olympics, paid, £, 31,665, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18392087</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>Mint Building i Sur,--Tb* cía .\\-fint Build- i...</td>\n",
       "      <td>Mint Building Sir, - The old Mint Build- ing i...</td>\n",
       "      <td>Mint Building Sir, - The old Mint Build- ing i...</td>\n",
       "      <td>0.687574</td>\n",
       "      <td>813</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>[[4, 8, 1, -8, 1, 3, 1, 1, 1, 4, 5, 1, 3, 2, 9...</td>\n",
       "      <td>[4, 8, 1, -8, 1, 3, 1, 1, 1, 4, 5, 1, 3, 2, 9,...</td>\n",
       "      <td>64.347826</td>\n",
       "      <td>[4, 8, 3, 1, 1, 3, 3, 4, 5, 1, 3, 2, 9, 6, 1, ...</td>\n",
       "      <td>98.333333</td>\n",
       "      <td>[Mint, Building, Sir, ,, -, The, old, Mint, Bu...</td>\n",
       "      <td>[Mint, Building, i, Sur,--Tb, *, cía, ., \\, -,...</td>\n",
       "      <td>0.163399</td>\n",
       "      <td>[[mint, building, i, sur,--tb, *, cía, ., \\, -...</td>\n",
       "      <td>[[mint, building, sir, ,, -, the, old, mint, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>27904257</td>\n",
       "      <td>Article ILLUSTRATED</td>\n",
       "      <td>1947</td>\n",
       "      <td>PEARL TOWN PAINTED BY ELIZABETH DURACK \"The st...</td>\n",
       "      <td>PEARL TOWN PAINTED BY ELIZABETH DURACK \"The st...</td>\n",
       "      <td>PEARL TOWN PAINTED BY ELIZABETH DURACK \"The st...</td>\n",
       "      <td>0.744510</td>\n",
       "      <td>2732</td>\n",
       "      <td>2722</td>\n",
       "      <td>...</td>\n",
       "      <td>[[5, 4, 7, 2, 9, 6, 1, 3, 5, 2, 3, 5, 4, 8, 4,...</td>\n",
       "      <td>[5, 4, 7, 2, 9, 6, 1, 3, 5, 2, 3, 5, 4, 8, 4, ...</td>\n",
       "      <td>96.800000</td>\n",
       "      <td>[5, 4, 7, 2, 9, 6, 1, 3, 5, 2, 3, 5, 4, 8, 4, ...</td>\n",
       "      <td>99.726776</td>\n",
       "      <td>[PEARL, TOWN, PAINTED, BY, ELIZABETH, DURACK, ...</td>\n",
       "      <td>[PEARL, TOWN, PAINTED, BY, ELIZABETH, DURACK, ...</td>\n",
       "      <td>0.236032</td>\n",
       "      <td>[[pearl, town, painted, by, elizabeth, durack,...</td>\n",
       "      <td>[[pearl, town, painted, by, elizabeth, durack,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filePath articleId  \\\n",
       "3   ./trove_overproof/datasets/dataset1/rawTextAnd...  18366055   \n",
       "4   ./trove_overproof/datasets/dataset1/rawTextAnd...  18386137   \n",
       "10  ./trove_overproof/datasets/dataset1/rawTextAnd...  18391223   \n",
       "21  ./trove_overproof/datasets/dataset1/rawTextAnd...  18392087   \n",
       "25  ./trove_overproof/datasets/dataset1/rawTextAnd...  27904257   \n",
       "\n",
       "             articleType  year  \\\n",
       "3                Article  1953   \n",
       "4                Article  1953   \n",
       "10               Article  1953   \n",
       "21               Article  1953   \n",
       "25  Article ILLUSTRATED   1947   \n",
       "\n",
       "                                              ocrText  \\\n",
       "3   FIRST CHURCH I SERVICE 1 Presbyterian I ' Anni...   \n",
       "4   \"Bob\" Lulham's Fight Against Thallium District...   \n",
       "10  rchitect For lympics aid £31,665 - ?? MELBOURN...   \n",
       "21  Mint Building i Sur,--Tb* cía .\\-fint Build- i...   \n",
       "25  PEARL TOWN PAINTED BY ELIZABETH DURACK \"The st...   \n",
       "\n",
       "                                            humanText  \\\n",
       "3   FIRST CHURCH SERVICE Presbyterian Anniversary ...   \n",
       "4   \"Bob\" Lulham's Fight Against Thallium  Arthur ...   \n",
       "10  Architect For Olympics paid £31,665  MELBOURNE...   \n",
       "21  Mint Building Sir, - The old Mint Build- ing i...   \n",
       "25  PEARL TOWN PAINTED BY ELIZABETH DURACK \"The st...   \n",
       "\n",
       "                                            corrected  str_similarity  \\\n",
       "3   FIRST CHURCH SERVICE Presbyterian Anniversary ...        0.738901   \n",
       "4   \"Bob\" Lulham's Fight Against Thallium  Arthur ...        0.493898   \n",
       "10  Architect For Olympics paid £31,665  MELBOURNE...        0.793510   \n",
       "21  Mint Building Sir, - The old Mint Build- ing i...        0.687574   \n",
       "25  PEARL TOWN PAINTED BY ELIZABETH DURACK \"The st...        0.744510   \n",
       "\n",
       "    str_length_humanText  str_length_ocrText  ...  \\\n",
       "3                    946                 832  ...   \n",
       "4                   2950                2740  ...   \n",
       "10                   678                 585  ...   \n",
       "21                   813                 845  ...   \n",
       "25                  2732                2722  ...   \n",
       "\n",
       "                                      ocr_dict_lookup  \\\n",
       "3   [[5, 6, 1, 7, 1, 12, 1, 1, 11, 1, 3, 5, -12, 3...   \n",
       "4   [[1, 3, 1, -6, 2, 5, 7, 8, 8, 8, 1], [4, 5, 6,...   \n",
       "10  [[-8, 3, 7, 3, 1, -6, 1, 2, 9, 1, 6], [1, 1, 5...   \n",
       "21  [[4, 8, 1, -8, 1, 3, 1, 1, 1, 4, 5, 1, 3, 2, 9...   \n",
       "25  [[5, 4, 7, 2, 9, 6, 1, 3, 5, 2, 3, 5, 4, 8, 4,...   \n",
       "\n",
       "                                 ocr_dict_lookup_list ocr_dict_perc  \\\n",
       "3   [5, 6, 1, 7, 1, 12, 1, 1, 11, 1, 3, 5, -12, 3,...     82.000000   \n",
       "4   [1, 3, 1, -6, 2, 5, 7, 8, 8, 8, 1, 4, 5, 6, 6,...     95.580110   \n",
       "10  [-8, 3, 7, 3, 1, -6, 1, 2, 9, 1, 6, 1, 1, 5, 5...     88.607595   \n",
       "21  [4, 8, 1, -8, 1, 3, 1, 1, 1, 4, 5, 1, 3, 2, 9,...     64.347826   \n",
       "25  [5, 4, 7, 2, 9, 6, 1, 3, 5, 2, 3, 5, 4, 8, 4, ...     96.800000   \n",
       "\n",
       "                           corrected_dict_lookup_list corr_dict_perc  \\\n",
       "3   [5, 6, 7, 12, 11, 3, 5, 11, 2, 3, 5, 12, 6, 7,...      99.206349   \n",
       "4   [1, 3, 1, -6, 2, 5, 7, 8, 6, 6, 1, 1, 3, 1, 1,...      95.652174   \n",
       "10  [9, 3, 8, 4, 1, -6, 9, 1, 6, 2, 5, 5, 1, 3, 9,...      94.680851   \n",
       "21  [4, 8, 3, 1, 1, 3, 3, 4, 5, 1, 3, 2, 9, 6, 1, ...      98.333333   \n",
       "25  [5, 4, 7, 2, 9, 6, 1, 3, 5, 2, 3, 5, 4, 8, 4, ...      99.726776   \n",
       "\n",
       "                           corrected_sentencizer_list  \\\n",
       "3   [FIRST, CHURCH, SERVICE, Presbyterian, Anniver...   \n",
       "4   [\", Bob, \", Lulham, 's, Fight, Against, Thalli...   \n",
       "10  [Architect, For, Olympics, paid, £, 31,665, ME...   \n",
       "21  [Mint, Building, Sir, ,, -, The, old, Mint, Bu...   \n",
       "25  [PEARL, TOWN, PAINTED, BY, ELIZABETH, DURACK, ...   \n",
       "\n",
       "                                 ocr_sentencizer_list  jaccard_similarity  \\\n",
       "3   [FIRST, CHURCH, I, SERVICE, 1, Presbyterian, I...            0.191919   \n",
       "4   [\", Bob, \", Lulham, 's, Fight, Against, Thalli...            0.282682   \n",
       "10  [rchitect, For, lympics, aid, £, 31,665, -, ??...            0.302885   \n",
       "21  [Mint, Building, i, Sur,--Tb, *, cía, ., \\, -,...            0.163399   \n",
       "25  [PEARL, TOWN, PAINTED, BY, ELIZABETH, DURACK, ...            0.236032   \n",
       "\n",
       "                              ocr_sentencizer_cleaned  \\\n",
       "3   [[first, church, i, service, 1, presbyterian, ...   \n",
       "4   [[\", bob, \", lulham, 's, fight, against, thall...   \n",
       "10  [[rchitect, for, lympics, aid, £, 31,665, -, ?...   \n",
       "21  [[mint, building, i, sur,--tb, *, cía, ., \\, -...   \n",
       "25  [[pearl, town, painted, by, elizabeth, durack,...   \n",
       "\n",
       "                        corrected_sentencizer_cleaned  \n",
       "3   [[first, church, service, presbyterian, annive...  \n",
       "4   [[\", bob, \", lulham, 's, fight, against, thall...  \n",
       "10  [[architect, for, olympics, paid, £, 31,665, m...  \n",
       "21  [[mint, building, sir, ,, -, the, old, mint, b...  \n",
       "25  [[pearl, town, painted, by, elizabeth, durack,...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(db_sentence.shape)\n",
    "db_sentence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update a pre-trained LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args for Word2Vec\n",
    "w2v_args = Namespace(\n",
    "    epochs=5, \n",
    "    # only for Word2Vec\n",
    "    compute_loss=True,                               # If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "\n",
    "#     size=100,                                        # Dimensionality of the word vectors.\n",
    "#     alpha=0.03,                                      # The initial learning rate.\n",
    "#     min_alpha=0.0007,                                # Learning rate will linearly drop to min_alpha as training progresses.\n",
    "#     sg=1,                                            # Training algorithm: skip-gram if sg=1, otherwise CBOW.\n",
    "#     hs=0,                                            # If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
    "#     negative=20,                                     # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used. \n",
    "#     min_count=5,                                    # The model ignores all words with total frequency lower than this.\n",
    "#     window=5,                                        # The maximum distance between the current and predicted word within a sentence.\n",
    "#     sample=1e-3,                                     # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "#     workers=8, \n",
    "#     cbow_mean=1,                                     # If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "#     null_word=0,                                     # \n",
    "#     trim_rule=None,                                  # \n",
    "#     sorted_vocab=1,                                  # If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
    "#     batch_words=10000,                               # Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
    "    \n",
    "#     seed=1364,                                       # Seed for the random number generator.\n",
    "#     # only for FastText (compare to word2vec)\n",
    "#     #word_ngrams=1,                                   # If 1, uses enriches word vectors with subword(n-grams) information. If 0, this is equivalent to Word2Vec. \n",
    "#     #min_n=2,                                         # Minimum length of char n-grams to be used for training word representations.\n",
    "#     #max_n=15,                                        # Max length of char ngrams to be used for training word representations. Set max_n to be lesser than min_n to avoid char ngrams being used.\n",
    "#     #bucket=2000000                                  # Character ngrams are hashed into a fixed number of buckets, in order to limit the memory usage of the model. This option specifies the number of buckets used by the model.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before creating/updating LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef preprocess4LM(myrow, col_name=\"ocrText_cleaned_tokenize\"):\\n    txt = [token.lemma_ for token in nlp(myrow[col_name].lower())]\\n    return txt\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def preprocess4LM(myrow, col_name=\"ocrText_cleaned_tokenize\"):\n",
    "    txt = [token.lemma_ for token in nlp(myrow[col_name].lower())]\n",
    "    return txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndb_sentence[\"ocrText_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"ocrText_cleaned\"], axis=1)\\ndb_sentence[\"corrected_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"corrected_cleaned\"], axis=1)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "db_sentence[\"ocrText_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"ocrText_cleaned\"], axis=1)\n",
    "db_sentence[\"corrected_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"corrected_cleaned\"], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences: 5095\n"
     ]
    }
   ],
   "source": [
    "list_sentences = db_sentence[\"ocr_sentencizer_cleaned\"].to_list()\n",
    "print('#sentences: {}'.format(len(list_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_sentences = [val for sublist in list_sentences for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'church',\n",
       " 'i',\n",
       " 'service',\n",
       " '1',\n",
       " 'presbyterian',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'anniversary',\n",
       " '1',\n",
       " 'the',\n",
       " '150th',\n",
       " 'anniveisarjj',\n",
       " 'the',\n",
       " 'first',\n",
       " 'presb',\n",
       " '>',\n",
       " 'terian',\n",
       " 'chad',\n",
       " 'service',\n",
       " 'm',\n",
       " 'australia',\n",
       " 'will',\n",
       " 'k',\n",
       " 'celebrated',\n",
       " 'to',\n",
       " 'morrow',\n",
       " 'special',\n",
       " 'services',\n",
       " 'will',\n",
       " 'be',\n",
       " 'be',\n",
       " 'at',\n",
       " 'the',\n",
       " 'ebenezer',\n",
       " 'p«sbjt«j',\n",
       " 'church',\n",
       " ',',\n",
       " 'six',\n",
       " 'miles',\n",
       " 'from',\n",
       " 'wmdso',\n",
       " 'the',\n",
       " 'ebenezer',\n",
       " 'church',\n",
       " 'is',\n",
       " '»',\n",
       " 'oldest',\n",
       " 'church',\n",
       " 'in',\n",
       " 'australia',\n",
       " '?',\n",
       " 'which',\n",
       " 'services',\n",
       " 'are',\n",
       " 'still',\n",
       " '«',\n",
       " 'ia',\n",
       " 'the',\n",
       " 'sen',\n",
       " 'ice',\n",
       " 'v',\n",
       " '.',\n",
       " '.',\n",
       " '11',\n",
       " 'start',\n",
       " 'at',\n",
       " 'lp',\n",
       " '£',\n",
       " 'with',\n",
       " 'sacred',\n",
       " 'mus.c',\n",
       " 'ttajjl',\n",
       " '*',\n",
       " 'followed',\n",
       " 'at',\n",
       " '130',\n",
       " 'p',\n",
       " 'm',\n",
       " '«*£',\n",
       " 'dedication',\n",
       " 'of',\n",
       " 'commemorate',\n",
       " 'g',\n",
       " 'tue',\n",
       " 'moderator',\n",
       " 'general',\n",
       " 'of',\n",
       " 'j',\n",
       " '«',\n",
       " 'presbyterian',\n",
       " 'church',\n",
       " 'of',\n",
       " 'au',\n",
       " '»',\n",
       " 'the',\n",
       " 'right',\n",
       " 'rev',\n",
       " '^',\n",
       " '¿',\n",
       " 'bz',\n",
       " 'will',\n",
       " 'conduct',\n",
       " 'a',\n",
       " 'special',\n",
       " 'atfhe',\n",
       " 'pä',\n",
       " 'ws«.llbe^r',\n",
       " 'the',\n",
       " 'modentor',\n",
       " 'of',\n",
       " 'the',\n",
       " 'general',\n",
       " 'assemblv',\n",
       " 'the',\n",
       " 'rig',\n",
       " '«',\n",
       " \"'\",\n",
       " 'frank',\n",
       " 'hinlin',\n",
       " 'of',\n",
       " 'manly',\n",
       " 'the',\n",
       " 'police',\n",
       " 'p',\n",
       " \"'p\",\n",
       " '=',\n",
       " 'ft',\n",
       " 'se',\n",
       " '«',\n",
       " '?',\n",
       " 'present',\n",
       " 'at',\n",
       " 'the',\n",
       " 'churc',\n",
       " '«',\n",
       " ',',\n",
       " 'and',\n",
       " 'm',\n",
       " 'college',\n",
       " 'and',\n",
       " 'knox',\n",
       " 'ui',\n",
       " 'y',\n",
       " '_',\n",
       " 'school',\n",
       " 'students',\n",
       " 'will',\n",
       " 'prouo',\n",
       " '?',\n",
       " 'guard',\n",
       " 'of',\n",
       " 'hçnour',\n",
       " 'fm',\n",
       " 'm',\n",
       " 'special',\n",
       " 'trains',\n",
       " '»',\n",
       " \"'\",\n",
       " '»',\n",
       " '¿',\n",
       " 't«»,tl',\n",
       " 'h',\n",
       " 'central',\n",
       " 'to',\n",
       " 'windsor',\n",
       " 'bu',\n",
       " '«',\n",
       " 'h',\n",
       " 'take',\n",
       " 'people',\n",
       " 'on',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wen',\n",
       " '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_list_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_ocr = copy.deepcopy(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_ocr.workers = 8\n",
    "embedding_model_ocr.vocabulary.min_count = 5\n",
    "embedding_model_ocr.alpha = min_alpha_yet_reached*10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:41:06,575 : INFO : collecting all words and their counts\n",
      "2019-11-20 09:41:06,576 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-20 09:41:06,674 : INFO : PROGRESS: at sentence #10000, processed 434822 words, keeping 90410 word types\n",
      "2019-11-20 09:41:06,791 : INFO : PROGRESS: at sentence #20000, processed 886062 words, keeping 160063 word types\n",
      "2019-11-20 09:41:06,911 : INFO : PROGRESS: at sentence #30000, processed 1366349 words, keeping 227866 word types\n",
      "2019-11-20 09:41:07,012 : INFO : PROGRESS: at sentence #40000, processed 1789264 words, keeping 284758 word types\n",
      "2019-11-20 09:41:07,035 : INFO : collected 298566 word types from a corpus of 1896341 raw words and 42059 sentences\n",
      "2019-11-20 09:41:07,036 : INFO : Updating model with new vocabulary\n",
      "2019-11-20 09:41:07,184 : INFO : New added 19776 unique words (6% of original 318342) and increased the count of 19776 pre-existing words (6% of original 318342)\n",
      "2019-11-20 09:41:07,319 : INFO : deleting the raw counts dictionary of 298566 items\n",
      "2019-11-20 09:41:07,328 : INFO : sample=0.001 downsamples 100 most-common words\n",
      "2019-11-20 09:41:07,328 : INFO : downsampling leaves estimated 2218004 word corpus (141.6% of prior 1566450)\n",
      "2019-11-20 09:41:08,450 : INFO : estimated required memory for 39552 words and 300 dimensions: 114700800 bytes\n",
      "2019-11-20 09:41:08,451 : INFO : updating layer weights\n",
      "2019-11-20 09:41:09,518 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-20 09:41:09,519 : INFO : training model with 8 workers on 436964 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=20 window=5\n",
      "2019-11-20 09:41:10,908 : INFO : EPOCH 1 - PROGRESS: at 7.98% examples, 77602 words/s, in_qsize 16, out_qsize 2\n",
      "2019-11-20 09:41:11,988 : INFO : EPOCH 1 - PROGRESS: at 22.32% examples, 102786 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:13,059 : INFO : EPOCH 1 - PROGRESS: at 33.02% examples, 104328 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:14,147 : INFO : EPOCH 1 - PROGRESS: at 41.31% examples, 100989 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:15,180 : INFO : EPOCH 1 - PROGRESS: at 49.45% examples, 100672 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:16,211 : INFO : EPOCH 1 - PROGRESS: at 59.15% examples, 103908 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:17,229 : INFO : EPOCH 1 - PROGRESS: at 66.68% examples, 103210 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:18,230 : INFO : EPOCH 1 - PROGRESS: at 77.08% examples, 104838 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:19,232 : INFO : EPOCH 1 - PROGRESS: at 85.26% examples, 104273 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:20,210 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:41:20,335 : INFO : EPOCH 1 - PROGRESS: at 97.24% examples, 105509 words/s, in_qsize 6, out_qsize 1\n",
      "2019-11-20 09:41:20,336 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:41:20,387 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:41:20,464 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:41:20,501 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:41:20,506 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:41:20,533 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:41:20,534 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:41:20,535 : INFO : EPOCH - 1 : training on 1896341 raw words (1177830 effective words) took 11.0s, 107027 effective words/s\n",
      "2019-11-20 09:41:21,790 : INFO : EPOCH 2 - PROGRESS: at 8.15% examples, 85892 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:22,852 : INFO : EPOCH 2 - PROGRESS: at 22.32% examples, 109328 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:23,920 : INFO : EPOCH 2 - PROGRESS: at 31.64% examples, 103658 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:41:24,933 : INFO : EPOCH 2 - PROGRESS: at 43.26% examples, 110398 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:25,948 : INFO : EPOCH 2 - PROGRESS: at 51.68% examples, 109947 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:27,079 : INFO : EPOCH 2 - PROGRESS: at 62.72% examples, 114022 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:28,201 : INFO : EPOCH 2 - PROGRESS: at 74.70% examples, 116765 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:29,240 : INFO : EPOCH 2 - PROGRESS: at 86.20% examples, 117730 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:30,201 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:41:30,305 : INFO : EPOCH 2 - PROGRESS: at 97.24% examples, 116817 words/s, in_qsize 6, out_qsize 1\n",
      "2019-11-20 09:41:30,307 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:41:30,322 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:41:30,337 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:41:30,372 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:41:30,465 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:41:30,495 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:41:30,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:41:30,511 : INFO : EPOCH - 2 : training on 1896341 raw words (1178357 effective words) took 10.0s, 118201 effective words/s\n",
      "2019-11-20 09:41:31,706 : INFO : EPOCH 3 - PROGRESS: at 8.15% examples, 90190 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:32,881 : INFO : EPOCH 3 - PROGRESS: at 22.32% examples, 106912 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:33,898 : INFO : EPOCH 3 - PROGRESS: at 34.34% examples, 114274 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:34,958 : INFO : EPOCH 3 - PROGRESS: at 45.03% examples, 114521 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:36,033 : INFO : EPOCH 3 - PROGRESS: at 54.89% examples, 116925 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:37,055 : INFO : EPOCH 3 - PROGRESS: at 65.35% examples, 118754 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:38,089 : INFO : EPOCH 3 - PROGRESS: at 73.45% examples, 115569 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:39,149 : INFO : EPOCH 3 - PROGRESS: at 83.64% examples, 114969 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:40,146 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:41:40,262 : INFO : EPOCH 3 - PROGRESS: at 97.24% examples, 116963 words/s, in_qsize 6, out_qsize 1\n",
      "2019-11-20 09:41:40,263 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:41:40,295 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:41:40,297 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:41:40,311 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:41:40,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:41:40,394 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:41:40,428 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:41:40,429 : INFO : EPOCH - 3 : training on 1896341 raw words (1177280 effective words) took 9.9s, 118800 effective words/s\n",
      "2019-11-20 09:41:41,657 : INFO : EPOCH 4 - PROGRESS: at 8.29% examples, 87573 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:41:42,694 : INFO : EPOCH 4 - PROGRESS: at 22.32% examples, 111954 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:43,775 : INFO : EPOCH 4 - PROGRESS: at 35.02% examples, 119401 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:44,927 : INFO : EPOCH 4 - PROGRESS: at 47.35% examples, 121340 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:46,097 : INFO : EPOCH 4 - PROGRESS: at 59.15% examples, 122617 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:41:47,099 : INFO : EPOCH 4 - PROGRESS: at 69.53% examples, 123757 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:41:48,118 : INFO : EPOCH 4 - PROGRESS: at 79.75% examples, 122698 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:41:49,139 : INFO : EPOCH 4 - PROGRESS: at 90.92% examples, 121600 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:41:49,629 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:41:49,745 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:41:49,802 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:41:49,843 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:41:49,871 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:41:49,876 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:41:49,914 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:41:49,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:41:49,920 : INFO : EPOCH - 4 : training on 1896341 raw words (1177341 effective words) took 9.5s, 124139 effective words/s\n",
      "2019-11-20 09:41:51,166 : INFO : EPOCH 5 - PROGRESS: at 8.29% examples, 86378 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:52,251 : INFO : EPOCH 5 - PROGRESS: at 22.32% examples, 108724 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:41:53,254 : INFO : EPOCH 5 - PROGRESS: at 34.73% examples, 118016 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:54,313 : INFO : EPOCH 5 - PROGRESS: at 44.32% examples, 113148 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:55,315 : INFO : EPOCH 5 - PROGRESS: at 52.74% examples, 113844 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:41:56,390 : INFO : EPOCH 5 - PROGRESS: at 63.21% examples, 116297 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:57,392 : INFO : EPOCH 5 - PROGRESS: at 74.33% examples, 118940 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:58,394 : INFO : EPOCH 5 - PROGRESS: at 83.61% examples, 117232 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:41:59,403 : INFO : EPOCH 5 - PROGRESS: at 96.32% examples, 118887 words/s, in_qsize 9, out_qsize 0\n",
      "2019-11-20 09:41:59,508 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:41:59,638 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:41:59,640 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:41:59,643 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:41:59,699 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:41:59,756 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:41:59,802 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:41:59,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:41:59,806 : INFO : EPOCH - 5 : training on 1896341 raw words (1177506 effective words) took 9.9s, 119203 effective words/s\n",
      "2019-11-20 09:41:59,807 : INFO : training on a 9481705 raw words (5888314 effective words) took 50.3s, 117094 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5888314, 9481705)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_ocr.build_vocab(flattened_list_sentences, update=True)\n",
    "embedding_model_ocr.train(flattened_list_sentences, \n",
    "                          total_examples=embedding_model_ocr.corpus_count,\n",
    "                          epochs=w2v_args.epochs,  \n",
    "                          compute_loss=w2v_args.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:41:59,817 : INFO : saving Word2Vec object under ./w2v_005_EM_ocr_qual_3_4.model, separately None\n",
      "2019-11-20 09:41:59,817 : INFO : storing np array 'vectors' to ./w2v_005_EM_ocr_qual_3_4.model.wv.vectors.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] Save the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:42:00,553 : INFO : not storing attribute vectors_norm\n",
      "2019-11-20 09:42:00,554 : INFO : storing np array 'syn1neg' to ./w2v_005_EM_ocr_qual_3_4.model.trainables.syn1neg.npy\n",
      "2019-11-20 09:42:01,260 : INFO : not storing attribute cum_table\n",
      "2019-11-20 09:42:02,329 : INFO : saved ./w2v_005_EM_ocr_qual_3_4.model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n[INFO] Save the model\")\n",
    "embedding_model_ocr.save(\"./w2v_005_EM_ocr_qual_3_4.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences: 5095\n"
     ]
    }
   ],
   "source": [
    "list_sentences = db_sentence[\"corrected_sentencizer_cleaned\"].to_list()\n",
    "print('#sentences: {}'.format(len(list_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_sentences = [val for sublist in list_sentences for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'church',\n",
       " 'service',\n",
       " 'presbyterian',\n",
       " 'anniversary',\n",
       " 'the',\n",
       " '150th',\n",
       " 'anniversary',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'presbyterian',\n",
       " 'church',\n",
       " 'service',\n",
       " 'in',\n",
       " 'australia',\n",
       " 'will',\n",
       " 'be',\n",
       " 'celebrated',\n",
       " 'to',\n",
       " 'morrow',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_list_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_corrected = copy.deepcopy(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_corrected.workers = 8\n",
    "embedding_model_corrected.vocabulary.min_count = 5\n",
    "embedding_model_corrected.alpha = min_alpha_yet_reached*10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:42:10,527 : INFO : collecting all words and their counts\n",
      "2019-11-20 09:42:10,529 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-20 09:42:10,577 : INFO : PROGRESS: at sentence #10000, processed 217550 words, keeping 21037 word types\n",
      "2019-11-20 09:42:10,634 : INFO : PROGRESS: at sentence #20000, processed 455085 words, keeping 31368 word types\n",
      "2019-11-20 09:42:10,679 : INFO : PROGRESS: at sentence #30000, processed 695671 words, keeping 40549 word types\n",
      "2019-11-20 09:42:10,721 : INFO : PROGRESS: at sentence #40000, processed 923693 words, keeping 47531 word types\n",
      "2019-11-20 09:42:10,760 : INFO : PROGRESS: at sentence #50000, processed 1136699 words, keeping 53209 word types\n",
      "2019-11-20 09:42:10,802 : INFO : PROGRESS: at sentence #60000, processed 1361413 words, keeping 58750 word types\n",
      "2019-11-20 09:42:10,838 : INFO : PROGRESS: at sentence #70000, processed 1582999 words, keeping 63795 word types\n",
      "2019-11-20 09:42:10,874 : INFO : collected 68578 word types from a corpus of 1807730 raw words and 79187 sentences\n",
      "2019-11-20 09:42:10,875 : INFO : Updating model with new vocabulary\n",
      "2019-11-20 09:42:10,944 : INFO : New added 16933 unique words (19% of original 85511) and increased the count of 16933 pre-existing words (19% of original 85511)\n",
      "2019-11-20 09:42:11,058 : INFO : deleting the raw counts dictionary of 68578 items\n",
      "2019-11-20 09:42:11,060 : INFO : sample=0.001 downsamples 72 most-common words\n",
      "2019-11-20 09:42:11,060 : INFO : downsampling leaves estimated 2333759 word corpus (134.9% of prior 1730412)\n",
      "2019-11-20 09:42:12,276 : INFO : estimated required memory for 33866 words and 300 dimensions: 98211400 bytes\n",
      "2019-11-20 09:42:12,277 : INFO : updating layer weights\n",
      "2019-11-20 09:42:13,271 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-20 09:42:13,272 : INFO : training model with 8 workers on 435768 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=20 window=5\n",
      "2019-11-20 09:42:14,316 : INFO : EPOCH 1 - PROGRESS: at 9.25% examples, 106099 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:15,610 : INFO : EPOCH 1 - PROGRESS: at 18.77% examples, 95778 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:16,636 : INFO : EPOCH 1 - PROGRESS: at 30.40% examples, 110215 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:17,752 : INFO : EPOCH 1 - PROGRESS: at 39.69% examples, 109203 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:42:18,938 : INFO : EPOCH 1 - PROGRESS: at 52.75% examples, 113595 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:19,954 : INFO : EPOCH 1 - PROGRESS: at 62.66% examples, 113563 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:42:21,061 : INFO : EPOCH 1 - PROGRESS: at 71.87% examples, 111230 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:22,134 : INFO : EPOCH 1 - PROGRESS: at 80.88% examples, 109863 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:23,142 : INFO : EPOCH 1 - PROGRESS: at 92.33% examples, 112932 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:23,624 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:42:23,652 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:42:23,696 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:42:23,794 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:42:23,866 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:42:23,891 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:42:23,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:42:23,917 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:42:23,917 : INFO : EPOCH - 1 : training on 1807730 raw words (1216079 effective words) took 10.6s, 114392 effective words/s\n",
      "2019-11-20 09:42:24,954 : INFO : EPOCH 2 - PROGRESS: at 7.79% examples, 86972 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:42:26,185 : INFO : EPOCH 2 - PROGRESS: at 18.77% examples, 98691 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:27,283 : INFO : EPOCH 2 - PROGRESS: at 27.18% examples, 98100 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:28,349 : INFO : EPOCH 2 - PROGRESS: at 39.03% examples, 109009 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:29,381 : INFO : EPOCH 2 - PROGRESS: at 48.57% examples, 108915 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:30,386 : INFO : EPOCH 2 - PROGRESS: at 61.49% examples, 115134 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:31,460 : INFO : EPOCH 2 - PROGRESS: at 71.87% examples, 114792 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:32,557 : INFO : EPOCH 2 - PROGRESS: at 84.85% examples, 118093 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:33,569 : INFO : EPOCH 2 - PROGRESS: at 94.89% examples, 118885 words/s, in_qsize 11, out_qsize 0\n",
      "2019-11-20 09:42:33,668 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:42:33,712 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:42:33,776 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:42:33,831 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:42:33,882 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:42:33,931 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:42:33,951 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:42:33,989 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:42:33,990 : INFO : EPOCH - 2 : training on 1807730 raw words (1215245 effective words) took 10.1s, 120830 effective words/s\n",
      "2019-11-20 09:42:35,131 : INFO : EPOCH 3 - PROGRESS: at 9.82% examples, 103137 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:36,431 : INFO : EPOCH 3 - PROGRESS: at 23.02% examples, 113848 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:37,568 : INFO : EPOCH 3 - PROGRESS: at 35.38% examples, 122485 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:38,669 : INFO : EPOCH 3 - PROGRESS: at 48.57% examples, 127417 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:39,729 : INFO : EPOCH 3 - PROGRESS: at 62.60% examples, 132294 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:40,787 : INFO : EPOCH 3 - PROGRESS: at 76.09% examples, 135274 words/s, in_qsize 15, out_qsize 2\n",
      "2019-11-20 09:42:41,808 : INFO : EPOCH 3 - PROGRESS: at 89.66% examples, 138298 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:42,351 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:42:42,371 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:42:42,372 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:42:42,479 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:42:42,545 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:42:42,581 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:42:42,596 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:42:42,607 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:42:42,607 : INFO : EPOCH - 3 : training on 1807730 raw words (1215767 effective words) took 8.6s, 141357 effective words/s\n",
      "2019-11-20 09:42:43,721 : INFO : EPOCH 4 - PROGRESS: at 9.82% examples, 105451 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:44,892 : INFO : EPOCH 4 - PROGRESS: at 23.02% examples, 121462 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:46,013 : INFO : EPOCH 4 - PROGRESS: at 35.38% examples, 128589 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:47,073 : INFO : EPOCH 4 - PROGRESS: at 48.57% examples, 133435 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:48,161 : INFO : EPOCH 4 - PROGRESS: at 62.60% examples, 136691 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:49,225 : INFO : EPOCH 4 - PROGRESS: at 76.09% examples, 138923 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:42:50,276 : INFO : EPOCH 4 - PROGRESS: at 89.66% examples, 140942 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:42:50,909 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:42:50,939 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:42:51,040 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:42:51,050 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:42:51,114 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:42:51,188 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:42:51,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:42:51,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:42:51,207 : INFO : EPOCH - 4 : training on 1807730 raw words (1215530 effective words) took 8.6s, 141573 effective words/s\n",
      "2019-11-20 09:42:52,401 : INFO : EPOCH 5 - PROGRESS: at 9.82% examples, 98167 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:42:53,644 : INFO : EPOCH 5 - PROGRESS: at 23.02% examples, 113841 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:54,850 : INFO : EPOCH 5 - PROGRESS: at 35.38% examples, 120065 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:56,089 : INFO : EPOCH 5 - PROGRESS: at 48.57% examples, 121921 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:57,351 : INFO : EPOCH 5 - PROGRESS: at 62.60% examples, 123428 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:42:58,622 : INFO : EPOCH 5 - PROGRESS: at 76.09% examples, 123864 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:42:59,639 : INFO : EPOCH 5 - PROGRESS: at 88.68% examples, 126532 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:00,423 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:43:00,427 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:43:00,446 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:43:00,549 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:43:00,592 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:43:00,687 : INFO : EPOCH 5 - PROGRESS: at 99.15% examples, 126940 words/s, in_qsize 2, out_qsize 1\n",
      "2019-11-20 09:43:00,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:43:00,700 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:43:00,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:43:00,703 : INFO : EPOCH - 5 : training on 1807730 raw words (1215075 effective words) took 9.5s, 128137 effective words/s\n",
      "2019-11-20 09:43:00,704 : INFO : training on a 9038650 raw words (6077696 effective words) took 47.4s, 128136 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6077696, 9038650)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_corrected.build_vocab(flattened_list_sentences, update=True)\n",
    "embedding_model_corrected.train(flattened_list_sentences, \n",
    "                                total_examples=embedding_model_corrected.corpus_count,\n",
    "                                epochs=w2v_args.epochs,  \n",
    "                                compute_loss=w2v_args.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:43:00,712 : INFO : saving Word2Vec object under ./w2v_005_EM_corr_qual_3_4.model, separately None\n",
      "2019-11-20 09:43:00,713 : INFO : storing np array 'vectors' to ./w2v_005_EM_corr_qual_3_4.model.wv.vectors.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] Save the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:43:01,413 : INFO : not storing attribute vectors_norm\n",
      "2019-11-20 09:43:01,413 : INFO : storing np array 'syn1neg' to ./w2v_005_EM_corr_qual_3_4.model.trainables.syn1neg.npy\n",
      "2019-11-20 09:43:02,225 : INFO : not storing attribute cum_table\n",
      "2019-11-20 09:43:03,251 : INFO : saved ./w2v_005_EM_corr_qual_3_4.model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n[INFO] Save the model\")\n",
    "embedding_model_corrected.save(\"./w2v_005_EM_corr_qual_3_4.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality bands 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    13953\n",
       "1    11461\n",
       "Name: quality_band, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sentence = db_sentence_orig[(db_sentence_orig['quality_band'] == 1) | \n",
    "                               (db_sentence_orig['quality_band'] == 2)]\n",
    "db_sentence['quality_band'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef cleanup(myrow, colname=\"corrected\"):\\n    # remove all # and @§\\n    \\n    corpus = [re.sub(r\\'#\\', \\'\\', element, flags=re.IGNORECASE) for element in corpus]\\n    corpus = [re.sub(r\\'@\\', \\'\\', element, flags=re.IGNORECASE) for element in corpus]\\n    \\n    # --- remove 2 or more .\\n    corpus = [re.sub(\\'[.]{2,}\\', \\'.\\', element) for element in corpus]\\n    # --- add a space before and after a list of punctuations\\n    corpus = [re.sub(r\"([.,!?:;\"\\'])\", r\" \\x01 \", element) for element in corpus]\\n    # --- remove everything except:\\n    #corpus = [re.sub(r\"([^a-zA-Z\\\\-.:;,!?\\\\d+]+)\", r\" \", element) for element in corpus]\\n    corpus = [re.sub(r\"([^a-zA-Z\\\\d+]+)\", r\" \", element) for element in corpus]\\n    # --- replace numbers with <NUM>\\n    corpus = [re.sub(r\\'\\x08\\\\d+\\x08\\', \\'<NUM>\\', element) for element in corpus]\\n    corpus = [re.sub(\\'--\\', \\'\\', element) for element in corpus]\\n    # --- normalize white spaces\\n    corpus = [re.sub(\\'\\\\s+\\', \\' \\', element) for element in corpus]\\n    \\n    # remove multiple spaces\\n    corpus = [re.sub(r\\'\\\\s+\\', \\' \\', element, flags=re.IGNORECASE) for element in corpus]\\n    corpus = [element.strip() for element in corpus]\\n    #corpus = [element.lower() for element in corpus]\\n    return corpus\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def cleanup(myrow, colname=\"corrected\"):\n",
    "    # remove all # and @§\n",
    "    \n",
    "    corpus = [re.sub(r'#', '', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [re.sub(r'@', '', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    \n",
    "    # --- remove 2 or more .\n",
    "    corpus = [re.sub('[.]{2,}', '.', element) for element in corpus]\n",
    "    # --- add a space before and after a list of punctuations\n",
    "    corpus = [re.sub(r\"([.,!?:;\\\"\\'])\", r\" \\1 \", element) for element in corpus]\n",
    "    # --- remove everything except:\n",
    "    #corpus = [re.sub(r\"([^a-zA-Z\\-.:;,!?\\d+]+)\", r\" \", element) for element in corpus]\n",
    "    corpus = [re.sub(r\"([^a-zA-Z\\d+]+)\", r\" \", element) for element in corpus]\n",
    "    # --- replace numbers with <NUM>\n",
    "    corpus = [re.sub(r'\\b\\d+\\b', '<NUM>', element) for element in corpus]\n",
    "    corpus = [re.sub('--', '', element) for element in corpus]\n",
    "    # --- normalize white spaces\n",
    "    corpus = [re.sub('\\s+', ' ', element) for element in corpus]\n",
    "    \n",
    "    # remove multiple spaces\n",
    "    corpus = [re.sub(r'\\s+', ' ', element, flags=re.IGNORECASE) for element in corpus]\n",
    "    corpus = [element.strip() for element in corpus]\n",
    "    #corpus = [element.lower() for element in corpus]\n",
    "    return corpus\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(myrow, col_name):\n",
    "    all_clean_rows = []\n",
    "    for sent in myrow[col_name]:\n",
    "        one_clean_row = []\n",
    "        for token in sent:\n",
    "            one_clean_row.append(token.lower())\n",
    "        all_clean_rows.append(one_clean_row)\n",
    "    return all_clean_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/khosseini/anaconda3/envs/py37torch/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "db_sentence[\"ocr_sentencizer_cleaned\"] = db_sentence.apply(cleanup, args=[\"ocr_sentencizer\"], axis=1)\n",
    "db_sentence[\"corrected_sentencizer_cleaned\"] = db_sentence.apply(cleanup, args=[\"corrected_sentencizer\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25414, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>articleId</th>\n",
       "      <th>articleType</th>\n",
       "      <th>year</th>\n",
       "      <th>ocrText</th>\n",
       "      <th>humanText</th>\n",
       "      <th>corrected</th>\n",
       "      <th>str_similarity</th>\n",
       "      <th>str_length_humanText</th>\n",
       "      <th>str_length_ocrText</th>\n",
       "      <th>...</th>\n",
       "      <th>ocr_dict_lookup</th>\n",
       "      <th>ocr_dict_lookup_list</th>\n",
       "      <th>ocr_dict_perc</th>\n",
       "      <th>corrected_dict_lookup_list</th>\n",
       "      <th>corr_dict_perc</th>\n",
       "      <th>corrected_sentencizer_list</th>\n",
       "      <th>ocr_sentencizer_list</th>\n",
       "      <th>jaccard_similarity</th>\n",
       "      <th>ocr_sentencizer_cleaned</th>\n",
       "      <th>corrected_sentencizer_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18378453</td>\n",
       "      <td>Article ILLUSTRATED</td>\n",
       "      <td>1953</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIÄÜ I ^PI A^H\"...</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...</td>\n",
       "      <td>FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>746</td>\n",
       "      <td>820</td>\n",
       "      <td>...</td>\n",
       "      <td>[[4, 5, 8, 2, 3, 2, -5, 1, 1, 2, -3, 1, -5, -6...</td>\n",
       "      <td>[4, 5, 8, 2, 3, 2, -5, 1, 1, 2, -3, 1, -5, -6,...</td>\n",
       "      <td>79.439252</td>\n",
       "      <td>[4, 5, 8, 2, 3, 2, 5, 6, 1, -5, -6, 8, 4, 4, 5...</td>\n",
       "      <td>92.079208</td>\n",
       "      <td>[FROM, RIVER, CROSSING, TO, END, OF, TRIAL, SP...</td>\n",
       "      <td>[FROM, RIVER, CROSSING, TO, END, OF, TRIÄÜ, I,...</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>[[from, river, crossing, to, end, of, triäü, i...</td>\n",
       "      <td>[[from, river, crossing, to, end, of, trial, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18363627</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>Natural Childbirth Sir,-We nurses have seen fa...</td>\n",
       "      <td>0.964119</td>\n",
       "      <td>641</td>\n",
       "      <td>630</td>\n",
       "      <td>...</td>\n",
       "      <td>[[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...</td>\n",
       "      <td>[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, 2...</td>\n",
       "      <td>96.590909</td>\n",
       "      <td>[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, 2...</td>\n",
       "      <td>98.876404</td>\n",
       "      <td>[Natural, Childbirth, Sir,-We, nurses, have, s...</td>\n",
       "      <td>[Natural, Childbirth, Sir,-We, nurses, have, s...</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>[[natural, childbirth, sir,-we, nurses, have, ...</td>\n",
       "      <td>[[natural, childbirth, sir,-we, nurses, have, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18368961</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr Justic...</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr. Justi...</td>\n",
       "      <td>DIVORCE Before The Judge In Divorce, Mr. Justi...</td>\n",
       "      <td>0.894176</td>\n",
       "      <td>1219</td>\n",
       "      <td>1121</td>\n",
       "      <td>...</td>\n",
       "      <td>[[7, 6, 3, 5, 2, 7, 1, 2, 7, -5, 7, 4, 1, 1, -...</td>\n",
       "      <td>[7, 6, 3, 5, 2, 7, 1, 2, 7, -5, 7, 4, 1, 1, -6...</td>\n",
       "      <td>82.882883</td>\n",
       "      <td>[7, 6, 3, 5, 2, 7, 1, 2, 1, 7, -5, 7, 4, 1, 1,...</td>\n",
       "      <td>86.486486</td>\n",
       "      <td>[DIVORCE, Before, The, Judge, In, Divorce, ,, ...</td>\n",
       "      <td>[DIVORCE, Before, The, Judge, In, Divorce, ,, ...</td>\n",
       "      <td>0.147513</td>\n",
       "      <td>[[divorce, before, the, judge, in, divorce, ,,...</td>\n",
       "      <td>[[divorce, before, the, judge, in, divorce, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18355541</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>BRITISH AIRLINER I MISSING ¡HALIFAX, Feb. 2 M....</td>\n",
       "      <td>BRITISH AIRLINER MISSING HALIFAX, Feb. 2 (A.A....</td>\n",
       "      <td>BRITISH AIRLINER MISSING HALIFAX, Feb. 2 (A.A....</td>\n",
       "      <td>0.829670</td>\n",
       "      <td>1274</td>\n",
       "      <td>1139</td>\n",
       "      <td>...</td>\n",
       "      <td>[[7, 8, 1, 7, 1, 7, 1, 3, 1, 1, 4, 1, -3, 7, 3...</td>\n",
       "      <td>[7, 8, 1, 7, 1, 7, 1, 3, 1, 1, 4, 1, -3, 7, 3,...</td>\n",
       "      <td>88.194444</td>\n",
       "      <td>[7, 8, 7, 7, 1, 3, 1, 1, 1, -6, 1, -3, 7, 3, 1...</td>\n",
       "      <td>95.731707</td>\n",
       "      <td>[BRITISH, AIRLINER, MISSING, HALIFAX, ,, Feb, ...</td>\n",
       "      <td>[BRITISH, AIRLINER, I, MISSING, ¡, HALIFAX, ,,...</td>\n",
       "      <td>0.240449</td>\n",
       "      <td>[[british, airliner, i, missing, ¡, halifax, ,...</td>\n",
       "      <td>[[british, airliner, missing, halifax, ,, feb,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>18381450</td>\n",
       "      <td>Article</td>\n",
       "      <td>1953</td>\n",
       "      <td>I SCHOOL CHESS * Homebush Increased Ils lead o...</td>\n",
       "      <td>SCHOOL CHESS  Homebush increased its lead over...</td>\n",
       "      <td>SCHOOL CHESS  Homebush increased its lead over...</td>\n",
       "      <td>0.918264</td>\n",
       "      <td>991</td>\n",
       "      <td>955</td>\n",
       "      <td>...</td>\n",
       "      <td>[[1, 6, 5, 1, 8, 9, 3, 4, 4, 8, 2, 3, 5, 5, 2,...</td>\n",
       "      <td>[1, 6, 5, 1, 8, 9, 3, 4, 4, 8, 2, 3, 5, 5, 2, ...</td>\n",
       "      <td>92.307692</td>\n",
       "      <td>[6, 5, 8, 9, 3, 4, 4, 8, 2, 3, 5, 5, 2, 3, 1, ...</td>\n",
       "      <td>96.598639</td>\n",
       "      <td>[SCHOOL, CHESS, Homebush, increased, its, lead...</td>\n",
       "      <td>[I, SCHOOL, CHESS, *, Homebush, Increased, Ils...</td>\n",
       "      <td>0.277612</td>\n",
       "      <td>[[i, school, chess, *, homebush, increased, il...</td>\n",
       "      <td>[[school, chess, homebush, increased, its, lea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filePath articleId  \\\n",
       "1  ./trove_overproof/datasets/dataset1/rawTextAnd...  18378453   \n",
       "2  ./trove_overproof/datasets/dataset1/rawTextAnd...  18363627   \n",
       "5  ./trove_overproof/datasets/dataset1/rawTextAnd...  18368961   \n",
       "6  ./trove_overproof/datasets/dataset1/rawTextAnd...  18355541   \n",
       "7  ./trove_overproof/datasets/dataset1/rawTextAnd...  18381450   \n",
       "\n",
       "            articleType  year  \\\n",
       "1  Article ILLUSTRATED   1953   \n",
       "2               Article  1953   \n",
       "5               Article  1953   \n",
       "6               Article  1953   \n",
       "7               Article  1953   \n",
       "\n",
       "                                             ocrText  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIÄÜ I ^PI A^H\"...   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr Justic...   \n",
       "6  BRITISH AIRLINER I MISSING ¡HALIFAX, Feb. 2 M....   \n",
       "7  I SCHOOL CHESS * Homebush Increased Ils lead o...   \n",
       "\n",
       "                                           humanText  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr. Justi...   \n",
       "6  BRITISH AIRLINER MISSING HALIFAX, Feb. 2 (A.A....   \n",
       "7  SCHOOL CHESS  Homebush increased its lead over...   \n",
       "\n",
       "                                           corrected  str_similarity  \\\n",
       "1  FROM RIVER CROSSING TO END OF TRIAL SPLASH: Pe...        0.847561   \n",
       "2  Natural Childbirth Sir,-We nurses have seen fa...        0.964119   \n",
       "5  DIVORCE Before The Judge In Divorce, Mr. Justi...        0.894176   \n",
       "6  BRITISH AIRLINER MISSING HALIFAX, Feb. 2 (A.A....        0.829670   \n",
       "7  SCHOOL CHESS  Homebush increased its lead over...        0.918264   \n",
       "\n",
       "   str_length_humanText  str_length_ocrText  ...  \\\n",
       "1                   746                 820  ...   \n",
       "2                   641                 630  ...   \n",
       "5                  1219                1121  ...   \n",
       "6                  1274                1139  ...   \n",
       "7                   991                 955  ...   \n",
       "\n",
       "                                     ocr_dict_lookup  \\\n",
       "1  [[4, 5, 8, 2, 3, 2, -5, 1, 1, 2, -3, 1, -5, -6...   \n",
       "2  [[7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, ...   \n",
       "5  [[7, 6, 3, 5, 2, 7, 1, 2, 7, -5, 7, 4, 1, 1, -...   \n",
       "6  [[7, 8, 1, 7, 1, 7, 1, 3, 1, 1, 4, 1, -3, 7, 3...   \n",
       "7  [[1, 6, 5, 1, 8, 9, 3, 4, 4, 8, 2, 3, 5, 5, 2,...   \n",
       "\n",
       "                                ocr_dict_lookup_list ocr_dict_perc  \\\n",
       "1  [4, 5, 8, 2, 3, 2, -5, 1, 1, 2, -3, 1, -5, -6,...     79.439252   \n",
       "2  [7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, 2...     96.590909   \n",
       "5  [7, 6, 3, 5, 2, 7, 1, 2, 7, -5, 7, 4, 1, 1, -6...     82.882883   \n",
       "6  [7, 8, 1, 7, 1, 7, 1, 3, 1, 1, 4, 1, -3, 7, 3,...     88.194444   \n",
       "7  [1, 6, 5, 1, 8, 9, 3, 4, 4, 8, 2, 3, 5, 5, 2, ...     92.307692   \n",
       "\n",
       "                          corrected_dict_lookup_list corr_dict_perc  \\\n",
       "1  [4, 5, 8, 2, 3, 2, 5, 6, 1, -5, -6, 8, 4, 4, 5...      92.079208   \n",
       "2  [7, 10, -7, 6, 4, 4, 3, 3, 4, 5, 6, 4, 6, 5, 2...      98.876404   \n",
       "5  [7, 6, 3, 5, 2, 7, 1, 2, 1, 7, -5, 7, 4, 1, 1,...      86.486486   \n",
       "6  [7, 8, 7, 7, 1, 3, 1, 1, 1, -6, 1, -3, 7, 3, 1...      95.731707   \n",
       "7  [6, 5, 8, 9, 3, 4, 4, 8, 2, 3, 5, 5, 2, 3, 1, ...      96.598639   \n",
       "\n",
       "                          corrected_sentencizer_list  \\\n",
       "1  [FROM, RIVER, CROSSING, TO, END, OF, TRIAL, SP...   \n",
       "2  [Natural, Childbirth, Sir,-We, nurses, have, s...   \n",
       "5  [DIVORCE, Before, The, Judge, In, Divorce, ,, ...   \n",
       "6  [BRITISH, AIRLINER, MISSING, HALIFAX, ,, Feb, ...   \n",
       "7  [SCHOOL, CHESS, Homebush, increased, its, lead...   \n",
       "\n",
       "                                ocr_sentencizer_list  jaccard_similarity  \\\n",
       "1  [FROM, RIVER, CROSSING, TO, END, OF, TRIÄÜ, I,...            0.305882   \n",
       "2  [Natural, Childbirth, Sir,-We, nurses, have, s...            0.523810   \n",
       "5  [DIVORCE, Before, The, Judge, In, Divorce, ,, ...            0.147513   \n",
       "6  [BRITISH, AIRLINER, I, MISSING, ¡, HALIFAX, ,,...            0.240449   \n",
       "7  [I, SCHOOL, CHESS, *, Homebush, Increased, Ils...            0.277612   \n",
       "\n",
       "                             ocr_sentencizer_cleaned  \\\n",
       "1  [[from, river, crossing, to, end, of, triäü, i...   \n",
       "2  [[natural, childbirth, sir,-we, nurses, have, ...   \n",
       "5  [[divorce, before, the, judge, in, divorce, ,,...   \n",
       "6  [[british, airliner, i, missing, ¡, halifax, ,...   \n",
       "7  [[i, school, chess, *, homebush, increased, il...   \n",
       "\n",
       "                       corrected_sentencizer_cleaned  \n",
       "1  [[from, river, crossing, to, end, of, trial, s...  \n",
       "2  [[natural, childbirth, sir,-we, nurses, have, ...  \n",
       "5  [[divorce, before, the, judge, in, divorce, ,,...  \n",
       "6  [[british, airliner, missing, halifax, ,, feb,...  \n",
       "7  [[school, chess, homebush, increased, its, lea...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(db_sentence.shape)\n",
    "db_sentence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update a pre-trained LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args for Word2Vec\n",
    "w2v_args = Namespace(\n",
    "    epochs=5, \n",
    "    # only for Word2Vec\n",
    "    compute_loss=True,                               # If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "\n",
    "#     size=100,                                        # Dimensionality of the word vectors.\n",
    "#     alpha=0.03,                                      # The initial learning rate.\n",
    "#     min_alpha=0.0007,                                # Learning rate will linearly drop to min_alpha as training progresses.\n",
    "#     sg=1,                                            # Training algorithm: skip-gram if sg=1, otherwise CBOW.\n",
    "#     hs=0,                                            # If 1, hierarchical softmax will be used for model training. If set to 0, and negative is non-zero, negative sampling will be used.\n",
    "#     negative=20,                                     # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used. \n",
    "#     min_count=5,                                    # The model ignores all words with total frequency lower than this.\n",
    "#     window=5,                                        # The maximum distance between the current and predicted word within a sentence.\n",
    "#     sample=1e-3,                                     # The threshold for configuring which higher-frequency words are randomly downsampled, useful range is (0, 1e-5).\n",
    "#     workers=8, \n",
    "#     cbow_mean=1,                                     # If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
    "#     null_word=0,                                     # \n",
    "#     trim_rule=None,                                  # \n",
    "#     sorted_vocab=1,                                  # If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
    "#     batch_words=10000,                               # Target size (in words) for batches of examples passed to worker threads (and thus cython routines).(Larger batches will be passed if individual texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
    "    \n",
    "#     seed=1364,                                       # Seed for the random number generator.\n",
    "#     # only for FastText (compare to word2vec)\n",
    "#     #word_ngrams=1,                                   # If 1, uses enriches word vectors with subword(n-grams) information. If 0, this is equivalent to Word2Vec. \n",
    "#     #min_n=2,                                         # Minimum length of char n-grams to be used for training word representations.\n",
    "#     #max_n=15,                                        # Max length of char ngrams to be used for training word representations. Set max_n to be lesser than min_n to avoid char ngrams being used.\n",
    "#     #bucket=2000000                                  # Character ngrams are hashed into a fixed number of buckets, in order to limit the memory usage of the model. This option specifies the number of buckets used by the model.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before creating/updating LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef preprocess4LM(myrow, col_name=\"ocrText_cleaned_tokenize\"):\\n    txt = [token.lemma_ for token in nlp(myrow[col_name].lower())]\\n    return txt\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def preprocess4LM(myrow, col_name=\"ocrText_cleaned_tokenize\"):\n",
    "    txt = [token.lemma_ for token in nlp(myrow[col_name].lower())]\n",
    "    return txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndb_sentence[\"ocrText_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"ocrText_cleaned\"], axis=1)\\ndb_sentence[\"corrected_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"corrected_cleaned\"], axis=1)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "db_sentence[\"ocrText_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"ocrText_cleaned\"], axis=1)\n",
    "db_sentence[\"corrected_cleaned_tokenize\"] = db_sentence[0:10].apply(preprocess4LM, args=[\"corrected_cleaned\"], axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences: 25414\n"
     ]
    }
   ],
   "source": [
    "list_sentences = db_sentence[\"ocr_sentencizer_cleaned\"].to_list()\n",
    "print('#sentences: {}'.format(len(list_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_sentences = [val for sublist in list_sentences for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'river',\n",
       " 'crossing',\n",
       " 'to',\n",
       " 'end',\n",
       " 'of',\n",
       " 'triäü',\n",
       " 'i',\n",
       " '^',\n",
       " 'pi',\n",
       " 'a^h',\n",
       " '\"',\n",
       " 'pclcr',\n",
       " 'antill',\n",
       " 'ploughed',\n",
       " 'deep',\n",
       " 'into',\n",
       " 'paddy',\n",
       " \"'s\",\n",
       " 'river',\n",
       " 'in',\n",
       " 'his',\n",
       " 'chrysler',\n",
       " 'plymouth',\n",
       " 'jr',\n",
       " 'la',\n",
       " 'jil',\n",
       " '?',\n",
       " 'during',\n",
       " '{',\n",
       " '|',\n",
       " ')',\n",
       " 'c',\n",
       " 'elimination',\n",
       " 'section',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_list_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_ocr = copy.deepcopy(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_ocr.workers = 8\n",
    "embedding_model_ocr.vocabulary.min_count = 5\n",
    "embedding_model_ocr.alpha = min_alpha_yet_reached*10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:43:16,592 : INFO : collecting all words and their counts\n",
      "2019-11-20 09:43:16,593 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-20 09:43:16,697 : INFO : PROGRESS: at sentence #10000, processed 486420 words, keeping 71569 word types\n",
      "2019-11-20 09:43:16,778 : INFO : PROGRESS: at sentence #20000, processed 818079 words, keeping 102132 word types\n",
      "2019-11-20 09:43:16,859 : INFO : PROGRESS: at sentence #30000, processed 1228071 words, keeping 139597 word types\n",
      "2019-11-20 09:43:16,922 : INFO : PROGRESS: at sentence #40000, processed 1547465 words, keeping 163436 word types\n",
      "2019-11-20 09:43:17,010 : INFO : PROGRESS: at sentence #50000, processed 1901060 words, keeping 187025 word types\n",
      "2019-11-20 09:43:17,082 : INFO : PROGRESS: at sentence #60000, processed 2249261 words, keeping 212735 word types\n",
      "2019-11-20 09:43:17,153 : INFO : PROGRESS: at sentence #70000, processed 2602920 words, keeping 237265 word types\n",
      "2019-11-20 09:43:17,233 : INFO : PROGRESS: at sentence #80000, processed 2993681 words, keeping 263814 word types\n",
      "2019-11-20 09:43:17,324 : INFO : PROGRESS: at sentence #90000, processed 3434924 words, keeping 293701 word types\n",
      "2019-11-20 09:43:17,403 : INFO : PROGRESS: at sentence #100000, processed 3826238 words, keeping 318025 word types\n",
      "2019-11-20 09:43:17,514 : INFO : PROGRESS: at sentence #110000, processed 4317753 words, keeping 350579 word types\n",
      "2019-11-20 09:43:17,595 : INFO : PROGRESS: at sentence #120000, processed 4689535 words, keeping 372619 word types\n",
      "2019-11-20 09:43:17,715 : INFO : PROGRESS: at sentence #130000, processed 5219652 words, keeping 409481 word types\n",
      "2019-11-20 09:43:17,805 : INFO : PROGRESS: at sentence #140000, processed 5632480 words, keeping 431591 word types\n",
      "2019-11-20 09:43:17,889 : INFO : PROGRESS: at sentence #150000, processed 6028476 words, keeping 453720 word types\n",
      "2019-11-20 09:43:17,988 : INFO : PROGRESS: at sentence #160000, processed 6451670 words, keeping 477483 word types\n",
      "2019-11-20 09:43:18,112 : INFO : PROGRESS: at sentence #170000, processed 6988829 words, keeping 509744 word types\n",
      "2019-11-20 09:43:18,194 : INFO : PROGRESS: at sentence #180000, processed 7371115 words, keeping 529460 word types\n",
      "2019-11-20 09:43:18,278 : INFO : PROGRESS: at sentence #190000, processed 7743602 words, keeping 550480 word types\n",
      "2019-11-20 09:43:18,357 : INFO : PROGRESS: at sentence #200000, processed 8093458 words, keeping 567964 word types\n",
      "2019-11-20 09:43:18,449 : INFO : PROGRESS: at sentence #210000, processed 8496646 words, keeping 589560 word types\n",
      "2019-11-20 09:43:18,547 : INFO : PROGRESS: at sentence #220000, processed 8931254 words, keeping 613473 word types\n",
      "2019-11-20 09:43:18,633 : INFO : PROGRESS: at sentence #230000, processed 9277460 words, keeping 632909 word types\n",
      "2019-11-20 09:43:18,734 : INFO : PROGRESS: at sentence #240000, processed 9688698 words, keeping 657122 word types\n",
      "2019-11-20 09:43:18,875 : INFO : PROGRESS: at sentence #250000, processed 10277849 words, keeping 693227 word types\n",
      "2019-11-20 09:43:18,949 : INFO : collected 700972 word types from a corpus of 10460495 raw words and 255523 sentences\n",
      "2019-11-20 09:43:18,950 : INFO : Updating model with new vocabulary\n",
      "2019-11-20 09:43:19,327 : INFO : New added 64366 unique words (8% of original 765338) and increased the count of 64366 pre-existing words (8% of original 765338)\n",
      "2019-11-20 09:43:19,748 : INFO : deleting the raw counts dictionary of 700972 items\n",
      "2019-11-20 09:43:19,762 : INFO : sample=0.001 downsamples 80 most-common words\n",
      "2019-11-20 09:43:19,763 : INFO : downsampling leaves estimated 14177546 word corpus (146.5% of prior 9678286)\n",
      "2019-11-20 09:43:20,924 : INFO : estimated required memory for 128732 words and 300 dimensions: 373322800 bytes\n",
      "2019-11-20 09:43:20,925 : INFO : updating layer weights\n",
      "2019-11-20 09:43:24,439 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-20 09:43:24,440 : INFO : training model with 8 workers on 453085 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=20 window=5\n",
      "2019-11-20 09:43:25,708 : INFO : EPOCH 1 - PROGRESS: at 1.12% examples, 95318 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:26,725 : INFO : EPOCH 1 - PROGRESS: at 2.43% examples, 105462 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-20 09:43:27,816 : INFO : EPOCH 1 - PROGRESS: at 4.97% examples, 118749 words/s, in_qsize 16, out_qsize 2\n",
      "2019-11-20 09:43:28,903 : INFO : EPOCH 1 - PROGRESS: at 7.64% examples, 125792 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:30,058 : INFO : EPOCH 1 - PROGRESS: at 9.97% examples, 128540 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:31,166 : INFO : EPOCH 1 - PROGRESS: at 12.34% examples, 131209 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:32,378 : INFO : EPOCH 1 - PROGRESS: at 15.21% examples, 131356 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:33,450 : INFO : EPOCH 1 - PROGRESS: at 17.95% examples, 133261 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:34,454 : INFO : EPOCH 1 - PROGRESS: at 19.71% examples, 131670 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:35,569 : INFO : EPOCH 1 - PROGRESS: at 22.37% examples, 132857 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:36,601 : INFO : EPOCH 1 - PROGRESS: at 24.85% examples, 134254 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:37,606 : INFO : EPOCH 1 - PROGRESS: at 26.55% examples, 132955 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:38,671 : INFO : EPOCH 1 - PROGRESS: at 29.31% examples, 132902 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:43:39,805 : INFO : EPOCH 1 - PROGRESS: at 30.96% examples, 131621 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:43:40,805 : INFO : EPOCH 1 - PROGRESS: at 32.58% examples, 131826 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:41,893 : INFO : EPOCH 1 - PROGRESS: at 34.30% examples, 131359 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:42,913 : INFO : EPOCH 1 - PROGRESS: at 36.21% examples, 130842 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:43:43,985 : INFO : EPOCH 1 - PROGRESS: at 38.21% examples, 131475 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:45,030 : INFO : EPOCH 1 - PROGRESS: at 40.20% examples, 130384 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:46,076 : INFO : EPOCH 1 - PROGRESS: at 42.10% examples, 130562 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:47,119 : INFO : EPOCH 1 - PROGRESS: at 42.86% examples, 129404 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:48,189 : INFO : EPOCH 1 - PROGRESS: at 44.13% examples, 128411 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:43:49,215 : INFO : EPOCH 1 - PROGRESS: at 46.44% examples, 128846 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:50,299 : INFO : EPOCH 1 - PROGRESS: at 48.27% examples, 128621 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:51,378 : INFO : EPOCH 1 - PROGRESS: at 49.47% examples, 128927 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:52,491 : INFO : EPOCH 1 - PROGRESS: at 51.03% examples, 128505 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:43:53,538 : INFO : EPOCH 1 - PROGRESS: at 53.20% examples, 129302 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:54,563 : INFO : EPOCH 1 - PROGRESS: at 54.93% examples, 129113 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:55,747 : INFO : EPOCH 1 - PROGRESS: at 57.30% examples, 128985 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:56,765 : INFO : EPOCH 1 - PROGRESS: at 59.15% examples, 129634 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:43:57,803 : INFO : EPOCH 1 - PROGRESS: at 60.79% examples, 129388 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:58,847 : INFO : EPOCH 1 - PROGRESS: at 62.98% examples, 130048 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:43:59,925 : INFO : EPOCH 1 - PROGRESS: at 64.12% examples, 129341 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:44:00,930 : INFO : EPOCH 1 - PROGRESS: at 65.58% examples, 129700 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:02,076 : INFO : EPOCH 1 - PROGRESS: at 67.05% examples, 129189 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:03,083 : INFO : EPOCH 1 - PROGRESS: at 69.28% examples, 129707 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:04,127 : INFO : EPOCH 1 - PROGRESS: at 71.35% examples, 129374 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:44:05,151 : INFO : EPOCH 1 - PROGRESS: at 73.37% examples, 129818 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:44:06,206 : INFO : EPOCH 1 - PROGRESS: at 75.64% examples, 129926 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:07,219 : INFO : EPOCH 1 - PROGRESS: at 78.16% examples, 130165 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:08,245 : INFO : EPOCH 1 - PROGRESS: at 80.10% examples, 130170 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:09,303 : INFO : EPOCH 1 - PROGRESS: at 82.13% examples, 130311 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:44:10,332 : INFO : EPOCH 1 - PROGRESS: at 83.58% examples, 130371 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:11,342 : INFO : EPOCH 1 - PROGRESS: at 85.74% examples, 130626 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:12,356 : INFO : EPOCH 1 - PROGRESS: at 88.02% examples, 130942 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:13,369 : INFO : EPOCH 1 - PROGRESS: at 90.41% examples, 130962 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:14,438 : INFO : EPOCH 1 - PROGRESS: at 92.51% examples, 130992 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:15,491 : INFO : EPOCH 1 - PROGRESS: at 94.17% examples, 131064 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:16,494 : INFO : EPOCH 1 - PROGRESS: at 95.45% examples, 131156 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:44:17,537 : INFO : EPOCH 1 - PROGRESS: at 96.65% examples, 130886 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:18,558 : INFO : EPOCH 1 - PROGRESS: at 97.61% examples, 130465 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:19,244 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:44:19,267 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:44:19,327 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:44:19,422 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:44:19,519 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:44:19,538 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:44:19,610 : INFO : EPOCH 1 - PROGRESS: at 99.93% examples, 130334 words/s, in_qsize 1, out_qsize 1\n",
      "2019-11-20 09:44:19,610 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:44:19,626 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:44:19,626 : INFO : EPOCH - 1 : training on 10460495 raw words (7196927 effective words) took 55.2s, 130426 effective words/s\n",
      "2019-11-20 09:44:20,635 : INFO : EPOCH 2 - PROGRESS: at 1.01% examples, 105362 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:21,773 : INFO : EPOCH 2 - PROGRESS: at 2.30% examples, 109190 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:22,910 : INFO : EPOCH 2 - PROGRESS: at 4.95% examples, 122016 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:44:23,916 : INFO : EPOCH 2 - PROGRESS: at 7.10% examples, 124705 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:25,070 : INFO : EPOCH 2 - PROGRESS: at 9.30% examples, 122776 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:44:26,097 : INFO : EPOCH 2 - PROGRESS: at 11.29% examples, 127158 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:27,300 : INFO : EPOCH 2 - PROGRESS: at 13.35% examples, 122035 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:28,342 : INFO : EPOCH 2 - PROGRESS: at 16.00% examples, 124316 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:29,378 : INFO : EPOCH 2 - PROGRESS: at 17.95% examples, 123202 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:30,425 : INFO : EPOCH 2 - PROGRESS: at 19.61% examples, 121569 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:31,523 : INFO : EPOCH 2 - PROGRESS: at 21.07% examples, 118858 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:44:32,534 : INFO : EPOCH 2 - PROGRESS: at 23.11% examples, 117715 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:44:33,673 : INFO : EPOCH 2 - PROGRESS: at 24.78% examples, 115792 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:44:34,697 : INFO : EPOCH 2 - PROGRESS: at 26.04% examples, 114390 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:35,701 : INFO : EPOCH 2 - PROGRESS: at 28.32% examples, 113477 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:36,748 : INFO : EPOCH 2 - PROGRESS: at 29.57% examples, 111735 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:37,835 : INFO : EPOCH 2 - PROGRESS: at 30.75% examples, 109999 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:38,898 : INFO : EPOCH 2 - PROGRESS: at 31.56% examples, 107813 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:39,947 : INFO : EPOCH 2 - PROGRESS: at 32.36% examples, 105556 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:40,956 : INFO : EPOCH 2 - PROGRESS: at 33.14% examples, 103418 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:41,973 : INFO : EPOCH 2 - PROGRESS: at 33.70% examples, 100831 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:43,096 : INFO : EPOCH 2 - PROGRESS: at 35.32% examples, 100708 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:44,279 : INFO : EPOCH 2 - PROGRESS: at 36.64% examples, 100140 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:45,325 : INFO : EPOCH 2 - PROGRESS: at 38.31% examples, 100303 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:46,358 : INFO : EPOCH 2 - PROGRESS: at 40.31% examples, 100722 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:47,402 : INFO : EPOCH 2 - PROGRESS: at 41.91% examples, 100792 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:48,430 : INFO : EPOCH 2 - PROGRESS: at 42.62% examples, 100753 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:49,502 : INFO : EPOCH 2 - PROGRESS: at 43.12% examples, 99660 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:44:50,535 : INFO : EPOCH 2 - PROGRESS: at 44.27% examples, 98918 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:51,871 : INFO : EPOCH 2 - PROGRESS: at 45.23% examples, 96685 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:44:52,923 : INFO : EPOCH 2 - PROGRESS: at 47.12% examples, 96935 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:53,996 : INFO : EPOCH 2 - PROGRESS: at 48.44% examples, 97242 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:44:55,042 : INFO : EPOCH 2 - PROGRESS: at 49.36% examples, 97717 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:44:56,057 : INFO : EPOCH 2 - PROGRESS: at 50.73% examples, 98424 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:57,112 : INFO : EPOCH 2 - PROGRESS: at 52.19% examples, 98616 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:44:58,150 : INFO : EPOCH 2 - PROGRESS: at 53.90% examples, 99111 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:44:59,151 : INFO : EPOCH 2 - PROGRESS: at 55.80% examples, 99808 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:00,191 : INFO : EPOCH 2 - PROGRESS: at 57.59% examples, 100117 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:01,240 : INFO : EPOCH 2 - PROGRESS: at 58.91% examples, 100230 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:02,252 : INFO : EPOCH 2 - PROGRESS: at 60.23% examples, 100500 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:03,262 : INFO : EPOCH 2 - PROGRESS: at 61.93% examples, 100876 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:04,432 : INFO : EPOCH 2 - PROGRESS: at 63.61% examples, 101268 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:05,545 : INFO : EPOCH 2 - PROGRESS: at 64.61% examples, 101169 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:06,578 : INFO : EPOCH 2 - PROGRESS: at 65.86% examples, 101420 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:07,604 : INFO : EPOCH 2 - PROGRESS: at 67.60% examples, 102081 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:08,660 : INFO : EPOCH 2 - PROGRESS: at 69.48% examples, 102533 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:09,690 : INFO : EPOCH 2 - PROGRESS: at 71.43% examples, 102728 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:10,717 : INFO : EPOCH 2 - PROGRESS: at 73.21% examples, 103196 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:11,725 : INFO : EPOCH 2 - PROGRESS: at 75.17% examples, 103671 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:12,781 : INFO : EPOCH 2 - PROGRESS: at 77.47% examples, 103901 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:45:13,790 : INFO : EPOCH 2 - PROGRESS: at 79.25% examples, 104191 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:14,799 : INFO : EPOCH 2 - PROGRESS: at 81.24% examples, 104730 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:15,800 : INFO : EPOCH 2 - PROGRESS: at 82.38% examples, 104575 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:16,814 : INFO : EPOCH 2 - PROGRESS: at 83.41% examples, 104395 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:17,845 : INFO : EPOCH 2 - PROGRESS: at 85.59% examples, 105018 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:18,997 : INFO : EPOCH 2 - PROGRESS: at 87.12% examples, 104907 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:20,233 : INFO : EPOCH 2 - PROGRESS: at 90.07% examples, 105410 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:21,310 : INFO : EPOCH 2 - PROGRESS: at 92.45% examples, 106076 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:22,370 : INFO : EPOCH 2 - PROGRESS: at 93.77% examples, 106004 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:23,477 : INFO : EPOCH 2 - PROGRESS: at 94.92% examples, 105973 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:24,532 : INFO : EPOCH 2 - PROGRESS: at 95.92% examples, 105935 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:45:25,586 : INFO : EPOCH 2 - PROGRESS: at 96.99% examples, 106209 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:26,702 : INFO : EPOCH 2 - PROGRESS: at 98.97% examples, 106481 words/s, in_qsize 9, out_qsize 0\n",
      "2019-11-20 09:45:26,791 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:45:26,815 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:45:26,836 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:45:26,887 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:45:26,900 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:45:26,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:45:27,005 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:45:27,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:45:27,106 : INFO : EPOCH - 2 : training on 10460495 raw words (7197310 effective words) took 67.5s, 106670 effective words/s\n",
      "2019-11-20 09:45:28,124 : INFO : EPOCH 3 - PROGRESS: at 0.92% examples, 90062 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:29,137 : INFO : EPOCH 3 - PROGRESS: at 2.04% examples, 104829 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:45:30,343 : INFO : EPOCH 3 - PROGRESS: at 3.91% examples, 106757 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:31,593 : INFO : EPOCH 3 - PROGRESS: at 6.75% examples, 113099 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:32,602 : INFO : EPOCH 3 - PROGRESS: at 8.90% examples, 116699 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:33,694 : INFO : EPOCH 3 - PROGRESS: at 10.68% examples, 117735 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:34,713 : INFO : EPOCH 3 - PROGRESS: at 13.25% examples, 122165 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:35,767 : INFO : EPOCH 3 - PROGRESS: at 15.21% examples, 120379 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:36,789 : INFO : EPOCH 3 - PROGRESS: at 17.95% examples, 123983 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:37,821 : INFO : EPOCH 3 - PROGRESS: at 19.71% examples, 123035 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:45:38,863 : INFO : EPOCH 3 - PROGRESS: at 22.24% examples, 125187 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:39,943 : INFO : EPOCH 3 - PROGRESS: at 24.21% examples, 123473 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:45:41,025 : INFO : EPOCH 3 - PROGRESS: at 25.72% examples, 121628 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:42,096 : INFO : EPOCH 3 - PROGRESS: at 28.72% examples, 123422 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:43,266 : INFO : EPOCH 3 - PROGRESS: at 30.41% examples, 121786 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:44,347 : INFO : EPOCH 3 - PROGRESS: at 31.65% examples, 120823 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:45,403 : INFO : EPOCH 3 - PROGRESS: at 33.14% examples, 120490 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:46,482 : INFO : EPOCH 3 - PROGRESS: at 35.19% examples, 121573 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:47,566 : INFO : EPOCH 3 - PROGRESS: at 37.31% examples, 122954 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:48,569 : INFO : EPOCH 3 - PROGRESS: at 39.42% examples, 123203 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:49,586 : INFO : EPOCH 3 - PROGRESS: at 41.78% examples, 123552 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:50,595 : INFO : EPOCH 3 - PROGRESS: at 42.36% examples, 122000 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:51,618 : INFO : EPOCH 3 - PROGRESS: at 43.35% examples, 122495 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:52,690 : INFO : EPOCH 3 - PROGRESS: at 45.83% examples, 123392 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:53,748 : INFO : EPOCH 3 - PROGRESS: at 48.08% examples, 124014 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:54,778 : INFO : EPOCH 3 - PROGRESS: at 49.21% examples, 124243 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:55,847 : INFO : EPOCH 3 - PROGRESS: at 50.63% examples, 124449 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:56,872 : INFO : EPOCH 3 - PROGRESS: at 52.12% examples, 123884 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:57,872 : INFO : EPOCH 3 - PROGRESS: at 53.98% examples, 124238 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:58,881 : INFO : EPOCH 3 - PROGRESS: at 55.96% examples, 124290 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:45:59,990 : INFO : EPOCH 3 - PROGRESS: at 57.66% examples, 123642 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:46:01,006 : INFO : EPOCH 3 - PROGRESS: at 59.00% examples, 123170 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:46:02,021 : INFO : EPOCH 3 - PROGRESS: at 60.69% examples, 123427 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:03,053 : INFO : EPOCH 3 - PROGRESS: at 62.58% examples, 123727 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:04,152 : INFO : EPOCH 3 - PROGRESS: at 63.94% examples, 123325 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:05,171 : INFO : EPOCH 3 - PROGRESS: at 64.97% examples, 123063 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:06,227 : INFO : EPOCH 3 - PROGRESS: at 66.56% examples, 123223 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:07,309 : INFO : EPOCH 3 - PROGRESS: at 68.49% examples, 123284 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:08,341 : INFO : EPOCH 3 - PROGRESS: at 70.30% examples, 123022 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:09,367 : INFO : EPOCH 3 - PROGRESS: at 71.62% examples, 121965 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:10,381 : INFO : EPOCH 3 - PROGRESS: at 73.01% examples, 121465 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:11,464 : INFO : EPOCH 3 - PROGRESS: at 74.77% examples, 121106 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:12,486 : INFO : EPOCH 3 - PROGRESS: at 75.80% examples, 119861 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:13,520 : INFO : EPOCH 3 - PROGRESS: at 76.83% examples, 118356 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:14,526 : INFO : EPOCH 3 - PROGRESS: at 78.16% examples, 117406 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:15,548 : INFO : EPOCH 3 - PROGRESS: at 79.44% examples, 116730 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:16,582 : INFO : EPOCH 3 - PROGRESS: at 81.24% examples, 116746 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:17,620 : INFO : EPOCH 3 - PROGRESS: at 82.86% examples, 117068 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:18,696 : INFO : EPOCH 3 - PROGRESS: at 84.49% examples, 117011 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:19,702 : INFO : EPOCH 3 - PROGRESS: at 86.44% examples, 117482 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:20,769 : INFO : EPOCH 3 - PROGRESS: at 88.73% examples, 117522 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:21,778 : INFO : EPOCH 3 - PROGRESS: at 91.23% examples, 118163 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:46:22,781 : INFO : EPOCH 3 - PROGRESS: at 92.89% examples, 118214 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:23,782 : INFO : EPOCH 3 - PROGRESS: at 94.17% examples, 118032 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:24,797 : INFO : EPOCH 3 - PROGRESS: at 95.33% examples, 118080 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:25,878 : INFO : EPOCH 3 - PROGRESS: at 96.68% examples, 118343 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:26,889 : INFO : EPOCH 3 - PROGRESS: at 98.41% examples, 118864 words/s, in_qsize 14, out_qsize 0\n",
      "2019-11-20 09:46:27,257 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:46:27,260 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:46:27,313 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:46:27,351 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:46:27,445 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:46:27,446 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:46:27,583 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:46:27,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:46:27,585 : INFO : EPOCH - 3 : training on 10460495 raw words (7195237 effective words) took 60.5s, 118984 effective words/s\n",
      "2019-11-20 09:46:28,626 : INFO : EPOCH 4 - PROGRESS: at 1.06% examples, 109433 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:29,824 : INFO : EPOCH 4 - PROGRESS: at 2.30% examples, 104600 words/s, in_qsize 15, out_qsize 1\n",
      "2019-11-20 09:46:30,989 : INFO : EPOCH 4 - PROGRESS: at 4.97% examples, 117784 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:32,124 : INFO : EPOCH 4 - PROGRESS: at 7.59% examples, 123662 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:33,204 : INFO : EPOCH 4 - PROGRESS: at 9.86% examples, 127381 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:34,207 : INFO : EPOCH 4 - PROGRESS: at 11.13% examples, 122198 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:35,230 : INFO : EPOCH 4 - PROGRESS: at 13.25% examples, 121633 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:36,255 : INFO : EPOCH 4 - PROGRESS: at 15.21% examples, 120309 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:46:37,358 : INFO : EPOCH 4 - PROGRESS: at 17.95% examples, 122907 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:38,468 : INFO : EPOCH 4 - PROGRESS: at 19.61% examples, 120580 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:39,486 : INFO : EPOCH 4 - PROGRESS: at 21.38% examples, 119847 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:40,487 : INFO : EPOCH 4 - PROGRESS: at 24.03% examples, 121816 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:41,549 : INFO : EPOCH 4 - PROGRESS: at 25.63% examples, 120761 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:42,549 : INFO : EPOCH 4 - PROGRESS: at 27.86% examples, 120525 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:43,580 : INFO : EPOCH 4 - PROGRESS: at 29.67% examples, 119994 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:44,589 : INFO : EPOCH 4 - PROGRESS: at 30.95% examples, 118934 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:45,802 : INFO : EPOCH 4 - PROGRESS: at 32.36% examples, 117671 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:46,825 : INFO : EPOCH 4 - PROGRESS: at 34.30% examples, 119142 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:47,900 : INFO : EPOCH 4 - PROGRESS: at 36.21% examples, 118952 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:46:48,964 : INFO : EPOCH 4 - PROGRESS: at 38.31% examples, 120511 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:50,123 : INFO : EPOCH 4 - PROGRESS: at 41.14% examples, 121469 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:51,137 : INFO : EPOCH 4 - PROGRESS: at 42.32% examples, 121389 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:52,189 : INFO : EPOCH 4 - PROGRESS: at 43.25% examples, 121785 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:53,250 : INFO : EPOCH 4 - PROGRESS: at 45.23% examples, 121414 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:54,286 : INFO : EPOCH 4 - PROGRESS: at 47.26% examples, 121083 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:46:55,340 : INFO : EPOCH 4 - PROGRESS: at 48.74% examples, 121634 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:56,418 : INFO : EPOCH 4 - PROGRESS: at 49.59% examples, 120927 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:57,440 : INFO : EPOCH 4 - PROGRESS: at 50.91% examples, 120511 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:58,478 : INFO : EPOCH 4 - PROGRESS: at 52.79% examples, 120909 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:46:59,489 : INFO : EPOCH 4 - PROGRESS: at 54.82% examples, 121689 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:00,490 : INFO : EPOCH 4 - PROGRESS: at 56.91% examples, 121886 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:01,496 : INFO : EPOCH 4 - PROGRESS: at 58.73% examples, 122523 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:02,636 : INFO : EPOCH 4 - PROGRESS: at 60.37% examples, 122359 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:03,683 : INFO : EPOCH 4 - PROGRESS: at 61.93% examples, 121883 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:04,721 : INFO : EPOCH 4 - PROGRESS: at 63.61% examples, 122120 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:05,737 : INFO : EPOCH 4 - PROGRESS: at 64.83% examples, 122420 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:06,773 : INFO : EPOCH 4 - PROGRESS: at 66.47% examples, 122844 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:07,790 : INFO : EPOCH 4 - PROGRESS: at 68.39% examples, 123116 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:08,833 : INFO : EPOCH 4 - PROGRESS: at 70.69% examples, 123478 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:09,879 : INFO : EPOCH 4 - PROGRESS: at 72.56% examples, 123492 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:47:10,886 : INFO : EPOCH 4 - PROGRESS: at 74.68% examples, 123910 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:11,910 : INFO : EPOCH 4 - PROGRESS: at 77.19% examples, 124246 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:12,985 : INFO : EPOCH 4 - PROGRESS: at 79.24% examples, 124254 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:14,003 : INFO : EPOCH 4 - PROGRESS: at 81.44% examples, 124737 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:15,027 : INFO : EPOCH 4 - PROGRESS: at 82.98% examples, 124936 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:16,037 : INFO : EPOCH 4 - PROGRESS: at 84.99% examples, 125150 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:17,059 : INFO : EPOCH 4 - PROGRESS: at 87.04% examples, 125713 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:18,149 : INFO : EPOCH 4 - PROGRESS: at 89.49% examples, 125646 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:19,246 : INFO : EPOCH 4 - PROGRESS: at 91.81% examples, 125719 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:47:20,362 : INFO : EPOCH 4 - PROGRESS: at 93.77% examples, 125999 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:21,372 : INFO : EPOCH 4 - PROGRESS: at 95.17% examples, 126282 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:22,444 : INFO : EPOCH 4 - PROGRESS: at 96.43% examples, 126175 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:23,455 : INFO : EPOCH 4 - PROGRESS: at 97.61% examples, 126361 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:24,113 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:47:24,118 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:47:24,146 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:47:24,218 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:47:24,232 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:47:24,252 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:47:24,302 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:47:24,460 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 126539 words/s, in_qsize 0, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:47:24,461 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:47:24,461 : INFO : EPOCH - 4 : training on 10460495 raw words (7196090 effective words) took 56.9s, 126536 effective words/s\n",
      "2019-11-20 09:47:25,715 : INFO : EPOCH 5 - PROGRESS: at 1.12% examples, 96319 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:47:26,916 : INFO : EPOCH 5 - PROGRESS: at 3.12% examples, 117997 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:28,087 : INFO : EPOCH 5 - PROGRESS: at 6.07% examples, 125246 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:29,237 : INFO : EPOCH 5 - PROGRESS: at 8.56% examples, 128821 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:30,297 : INFO : EPOCH 5 - PROGRESS: at 10.61% examples, 131862 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:31,321 : INFO : EPOCH 5 - PROGRESS: at 12.65% examples, 131681 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:32,390 : INFO : EPOCH 5 - PROGRESS: at 15.21% examples, 131580 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:47:33,458 : INFO : EPOCH 5 - PROGRESS: at 17.95% examples, 133547 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:34,479 : INFO : EPOCH 5 - PROGRESS: at 19.99% examples, 133706 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:35,608 : INFO : EPOCH 5 - PROGRESS: at 22.37% examples, 132711 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:36,738 : INFO : EPOCH 5 - PROGRESS: at 24.85% examples, 133033 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:37,813 : INFO : EPOCH 5 - PROGRESS: at 26.55% examples, 131176 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:38,836 : INFO : EPOCH 5 - PROGRESS: at 29.39% examples, 132099 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:39,876 : INFO : EPOCH 5 - PROGRESS: at 31.03% examples, 131701 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:40,898 : INFO : EPOCH 5 - PROGRESS: at 32.58% examples, 131299 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:41,938 : INFO : EPOCH 5 - PROGRESS: at 34.40% examples, 131632 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:42,965 : INFO : EPOCH 5 - PROGRESS: at 36.45% examples, 132199 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:47:44,070 : INFO : EPOCH 5 - PROGRESS: at 38.31% examples, 131438 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:45,078 : INFO : EPOCH 5 - PROGRESS: at 40.31% examples, 130576 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:46,107 : INFO : EPOCH 5 - PROGRESS: at 42.10% examples, 130535 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:47,189 : INFO : EPOCH 5 - PROGRESS: at 42.86% examples, 129145 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:48,235 : INFO : EPOCH 5 - PROGRESS: at 44.13% examples, 128292 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:49,271 : INFO : EPOCH 5 - PROGRESS: at 46.39% examples, 128402 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:50,276 : INFO : EPOCH 5 - PROGRESS: at 47.99% examples, 127751 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:47:51,287 : INFO : EPOCH 5 - PROGRESS: at 48.96% examples, 126909 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:52,300 : INFO : EPOCH 5 - PROGRESS: at 49.96% examples, 126792 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:53,304 : INFO : EPOCH 5 - PROGRESS: at 51.41% examples, 125954 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:54,320 : INFO : EPOCH 5 - PROGRESS: at 53.31% examples, 126256 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:55,379 : INFO : EPOCH 5 - PROGRESS: at 55.10% examples, 126262 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:47:56,545 : INFO : EPOCH 5 - PROGRESS: at 57.30% examples, 125890 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:57,618 : INFO : EPOCH 5 - PROGRESS: at 58.73% examples, 125347 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:58,648 : INFO : EPOCH 5 - PROGRESS: at 60.33% examples, 125479 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:47:59,668 : INFO : EPOCH 5 - PROGRESS: at 62.39% examples, 125977 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:00,715 : INFO : EPOCH 5 - PROGRESS: at 63.90% examples, 126058 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:48:01,719 : INFO : EPOCH 5 - PROGRESS: at 65.08% examples, 126133 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:02,762 : INFO : EPOCH 5 - PROGRESS: at 66.83% examples, 126429 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:03,762 : INFO : EPOCH 5 - PROGRESS: at 68.85% examples, 126850 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:04,851 : INFO : EPOCH 5 - PROGRESS: at 71.35% examples, 127135 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:05,914 : INFO : EPOCH 5 - PROGRESS: at 72.94% examples, 126675 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:06,935 : INFO : EPOCH 5 - PROGRESS: at 74.86% examples, 126665 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:48:07,944 : INFO : EPOCH 5 - PROGRESS: at 77.47% examples, 126991 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:08,952 : INFO : EPOCH 5 - PROGRESS: at 79.34% examples, 126979 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:09,955 : INFO : EPOCH 5 - PROGRESS: at 81.12% examples, 126851 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:10,966 : INFO : EPOCH 5 - PROGRESS: at 82.44% examples, 126460 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:12,111 : INFO : EPOCH 5 - PROGRESS: at 84.07% examples, 126135 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:13,166 : INFO : EPOCH 5 - PROGRESS: at 85.79% examples, 125788 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:14,180 : INFO : EPOCH 5 - PROGRESS: at 87.54% examples, 125797 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:48:15,237 : INFO : EPOCH 5 - PROGRESS: at 90.07% examples, 125809 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:48:16,316 : INFO : EPOCH 5 - PROGRESS: at 92.31% examples, 125909 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:48:17,531 : INFO : EPOCH 5 - PROGRESS: at 93.84% examples, 125445 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:18,622 : INFO : EPOCH 5 - PROGRESS: at 95.12% examples, 125302 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:19,671 : INFO : EPOCH 5 - PROGRESS: at 96.15% examples, 124897 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:20,716 : INFO : EPOCH 5 - PROGRESS: at 96.99% examples, 124525 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:48:21,722 : INFO : EPOCH 5 - PROGRESS: at 98.97% examples, 124723 words/s, in_qsize 9, out_qsize 0\n",
      "2019-11-20 09:48:21,814 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:48:21,822 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:48:21,829 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:48:21,958 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:48:21,962 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:48:21,998 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:48:22,034 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:48:22,099 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:48:22,100 : INFO : EPOCH - 5 : training on 10460495 raw words (7196078 effective words) took 57.6s, 124873 effective words/s\n",
      "2019-11-20 09:48:22,101 : INFO : training on a 52302475 raw words (35981642 effective words) took 297.7s, 120882 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35981642, 52302475)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_ocr.build_vocab(flattened_list_sentences, update=True)\n",
    "embedding_model_ocr.train(flattened_list_sentences, \n",
    "                          total_examples=embedding_model_ocr.corpus_count,\n",
    "                          epochs=w2v_args.epochs,  \n",
    "                          compute_loss=w2v_args.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:48:22,108 : INFO : saving Word2Vec object under ./w2v_005_EM_ocr_qual_1_2.model, separately None\n",
      "2019-11-20 09:48:22,109 : INFO : storing np array 'vectors' to ./w2v_005_EM_ocr_qual_1_2.model.wv.vectors.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] Save the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:48:23,334 : INFO : not storing attribute vectors_norm\n",
      "2019-11-20 09:48:23,335 : INFO : storing np array 'syn1neg' to ./w2v_005_EM_ocr_qual_1_2.model.trainables.syn1neg.npy\n",
      "2019-11-20 09:48:24,537 : INFO : not storing attribute cum_table\n",
      "2019-11-20 09:48:25,633 : INFO : saved ./w2v_005_EM_ocr_qual_1_2.model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n[INFO] Save the model\")\n",
    "embedding_model_ocr.save(\"./w2v_005_EM_ocr_qual_1_2.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#sentences: 25414\n"
     ]
    }
   ],
   "source": [
    "list_sentences = db_sentence[\"corrected_sentencizer_cleaned\"].to_list()\n",
    "print('#sentences: {}'.format(len(list_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_list_sentences = [val for sublist in list_sentences for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'river',\n",
       " 'crossing',\n",
       " 'to',\n",
       " 'end',\n",
       " 'of',\n",
       " 'trial',\n",
       " 'splash',\n",
       " ':',\n",
       " 'peler',\n",
       " 'antill',\n",
       " 'ploughed',\n",
       " 'deep',\n",
       " 'into',\n",
       " 'paddy',\n",
       " \"'s\",\n",
       " 'river',\n",
       " 'in',\n",
       " 'his',\n",
       " 'chrysler',\n",
       " 'plymouth',\n",
       " 'during',\n",
       " 'the',\n",
       " 'elimination',\n",
       " 'section',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_list_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_corrected = copy.deepcopy(embedding_model_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_corrected.workers = 8\n",
    "embedding_model_corrected.vocabulary.min_count = 5\n",
    "embedding_model_corrected.alpha = min_alpha_yet_reached*10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:48:37,484 : INFO : collecting all words and their counts\n",
      "2019-11-20 09:48:37,485 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-20 09:48:37,541 : INFO : PROGRESS: at sentence #10000, processed 236636 words, keeping 21073 word types\n",
      "2019-11-20 09:48:37,598 : INFO : PROGRESS: at sentence #20000, processed 482228 words, keeping 31622 word types\n",
      "2019-11-20 09:48:37,665 : INFO : PROGRESS: at sentence #30000, processed 740659 words, keeping 39961 word types\n",
      "2019-11-20 09:48:37,721 : INFO : PROGRESS: at sentence #40000, processed 988781 words, keeping 46057 word types\n",
      "2019-11-20 09:48:37,770 : INFO : PROGRESS: at sentence #50000, processed 1239629 words, keeping 51414 word types\n",
      "2019-11-20 09:48:37,827 : INFO : PROGRESS: at sentence #60000, processed 1498330 words, keeping 56156 word types\n",
      "2019-11-20 09:48:37,873 : INFO : PROGRESS: at sentence #70000, processed 1736066 words, keeping 61054 word types\n",
      "2019-11-20 09:48:37,947 : INFO : PROGRESS: at sentence #80000, processed 1988337 words, keeping 65526 word types\n",
      "2019-11-20 09:48:38,019 : INFO : PROGRESS: at sentence #90000, processed 2258336 words, keeping 69929 word types\n",
      "2019-11-20 09:48:38,076 : INFO : PROGRESS: at sentence #100000, processed 2501020 words, keeping 73653 word types\n",
      "2019-11-20 09:48:38,125 : INFO : PROGRESS: at sentence #110000, processed 2737705 words, keeping 77225 word types\n",
      "2019-11-20 09:48:38,178 : INFO : PROGRESS: at sentence #120000, processed 2982146 words, keeping 81185 word types\n",
      "2019-11-20 09:48:38,238 : INFO : PROGRESS: at sentence #130000, processed 3242029 words, keeping 84868 word types\n",
      "2019-11-20 09:48:38,294 : INFO : PROGRESS: at sentence #140000, processed 3480476 words, keeping 88484 word types\n",
      "2019-11-20 09:48:38,350 : INFO : PROGRESS: at sentence #150000, processed 3710712 words, keeping 91813 word types\n",
      "2019-11-20 09:48:38,406 : INFO : PROGRESS: at sentence #160000, processed 3962957 words, keeping 95200 word types\n",
      "2019-11-20 09:48:38,457 : INFO : PROGRESS: at sentence #170000, processed 4216432 words, keeping 98086 word types\n",
      "2019-11-20 09:48:38,518 : INFO : PROGRESS: at sentence #180000, processed 4475661 words, keeping 101247 word types\n",
      "2019-11-20 09:48:38,560 : INFO : PROGRESS: at sentence #190000, processed 4713675 words, keeping 104353 word types\n",
      "2019-11-20 09:48:38,603 : INFO : PROGRESS: at sentence #200000, processed 4934287 words, keeping 107349 word types\n",
      "2019-11-20 09:48:38,645 : INFO : PROGRESS: at sentence #210000, processed 5159531 words, keeping 109854 word types\n",
      "2019-11-20 09:48:38,708 : INFO : PROGRESS: at sentence #220000, processed 5400023 words, keeping 112796 word types\n",
      "2019-11-20 09:48:38,780 : INFO : PROGRESS: at sentence #230000, processed 5661814 words, keeping 115034 word types\n",
      "2019-11-20 09:48:38,853 : INFO : PROGRESS: at sentence #240000, processed 5894822 words, keeping 117716 word types\n",
      "2019-11-20 09:48:38,909 : INFO : PROGRESS: at sentence #250000, processed 6124936 words, keeping 120526 word types\n",
      "2019-11-20 09:48:38,989 : INFO : PROGRESS: at sentence #260000, processed 6367150 words, keeping 123237 word types\n",
      "2019-11-20 09:48:39,061 : INFO : PROGRESS: at sentence #270000, processed 6612037 words, keeping 125666 word types\n",
      "2019-11-20 09:48:39,133 : INFO : PROGRESS: at sentence #280000, processed 6845966 words, keeping 127687 word types\n",
      "2019-11-20 09:48:39,204 : INFO : PROGRESS: at sentence #290000, processed 7110897 words, keeping 130110 word types\n",
      "2019-11-20 09:48:39,264 : INFO : PROGRESS: at sentence #300000, processed 7347960 words, keeping 132544 word types\n",
      "2019-11-20 09:48:39,325 : INFO : PROGRESS: at sentence #310000, processed 7579002 words, keeping 134479 word types\n",
      "2019-11-20 09:48:39,390 : INFO : PROGRESS: at sentence #320000, processed 7834629 words, keeping 137049 word types\n",
      "2019-11-20 09:48:39,444 : INFO : PROGRESS: at sentence #330000, processed 8081694 words, keeping 139273 word types\n",
      "2019-11-20 09:48:39,499 : INFO : PROGRESS: at sentence #340000, processed 8329078 words, keeping 141988 word types\n",
      "2019-11-20 09:48:39,561 : INFO : PROGRESS: at sentence #350000, processed 8560835 words, keeping 143826 word types\n",
      "2019-11-20 09:48:39,626 : INFO : PROGRESS: at sentence #360000, processed 8799790 words, keeping 145985 word types\n",
      "2019-11-20 09:48:39,690 : INFO : PROGRESS: at sentence #370000, processed 9039675 words, keeping 148288 word types\n",
      "2019-11-20 09:48:39,750 : INFO : PROGRESS: at sentence #380000, processed 9316001 words, keeping 151018 word types\n",
      "2019-11-20 09:48:39,817 : INFO : PROGRESS: at sentence #390000, processed 9558846 words, keeping 152894 word types\n",
      "2019-11-20 09:48:39,883 : INFO : PROGRESS: at sentence #400000, processed 9816015 words, keeping 154088 word types\n",
      "2019-11-20 09:48:39,948 : INFO : PROGRESS: at sentence #410000, processed 10058648 words, keeping 155418 word types\n",
      "2019-11-20 09:48:40,017 : INFO : PROGRESS: at sentence #420000, processed 10314424 words, keeping 158235 word types\n",
      "2019-11-20 09:48:40,018 : INFO : collected 158266 word types from a corpus of 10316357 raw words and 420082 sentences\n",
      "2019-11-20 09:48:40,019 : INFO : Updating model with new vocabulary\n",
      "2019-11-20 09:48:40,203 : INFO : New added 40975 unique words (20% of original 199241) and increased the count of 40975 pre-existing words (20% of original 199241)\n",
      "2019-11-20 09:48:40,551 : INFO : deleting the raw counts dictionary of 158266 items\n",
      "2019-11-20 09:48:40,571 : INFO : sample=0.001 downsamples 68 most-common words\n",
      "2019-11-20 09:48:40,574 : INFO : downsampling leaves estimated 13940641 word corpus (137.4% of prior 10147491)\n",
      "2019-11-20 09:48:41,875 : INFO : estimated required memory for 81950 words and 300 dimensions: 237655000 bytes\n",
      "2019-11-20 09:48:41,876 : INFO : updating layer weights\n",
      "2019-11-20 09:48:43,484 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-11-20 09:48:43,485 : INFO : training model with 8 workers on 439314 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=20 window=5\n",
      "2019-11-20 09:48:44,787 : INFO : EPOCH 1 - PROGRESS: at 1.66% examples, 93429 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:48:46,088 : INFO : EPOCH 1 - PROGRESS: at 4.08% examples, 111386 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:47,313 : INFO : EPOCH 1 - PROGRESS: at 6.39% examples, 117872 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:48,521 : INFO : EPOCH 1 - PROGRESS: at 8.54% examples, 121970 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:49,543 : INFO : EPOCH 1 - PROGRESS: at 10.15% examples, 120359 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:50,580 : INFO : EPOCH 1 - PROGRESS: at 11.58% examples, 117121 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:51,587 : INFO : EPOCH 1 - PROGRESS: at 13.10% examples, 115923 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:48:52,789 : INFO : EPOCH 1 - PROGRESS: at 15.33% examples, 118322 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:53,995 : INFO : EPOCH 1 - PROGRESS: at 17.70% examples, 120182 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:55,094 : INFO : EPOCH 1 - PROGRESS: at 19.92% examples, 122746 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:56,211 : INFO : EPOCH 1 - PROGRESS: at 22.04% examples, 124743 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:57,454 : INFO : EPOCH 1 - PROGRESS: at 24.45% examples, 125292 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:58,547 : INFO : EPOCH 1 - PROGRESS: at 26.82% examples, 126970 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:48:59,703 : INFO : EPOCH 1 - PROGRESS: at 29.10% examples, 127943 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:00,893 : INFO : EPOCH 1 - PROGRESS: at 31.31% examples, 128601 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:01,902 : INFO : EPOCH 1 - PROGRESS: at 33.65% examples, 130069 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:02,923 : INFO : EPOCH 1 - PROGRESS: at 35.41% examples, 129176 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:04,042 : INFO : EPOCH 1 - PROGRESS: at 37.70% examples, 130024 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:49:05,135 : INFO : EPOCH 1 - PROGRESS: at 39.94% examples, 130983 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:06,176 : INFO : EPOCH 1 - PROGRESS: at 42.17% examples, 132157 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:49:07,181 : INFO : EPOCH 1 - PROGRESS: at 44.13% examples, 132585 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:08,210 : INFO : EPOCH 1 - PROGRESS: at 46.38% examples, 132853 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:09,296 : INFO : EPOCH 1 - PROGRESS: at 48.52% examples, 132684 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:10,328 : INFO : EPOCH 1 - PROGRESS: at 50.69% examples, 132985 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:49:11,420 : INFO : EPOCH 1 - PROGRESS: at 52.62% examples, 132689 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:12,467 : INFO : EPOCH 1 - PROGRESS: at 54.71% examples, 133313 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:13,545 : INFO : EPOCH 1 - PROGRESS: at 56.84% examples, 133277 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:49:14,598 : INFO : EPOCH 1 - PROGRESS: at 58.86% examples, 133184 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:15,617 : INFO : EPOCH 1 - PROGRESS: at 60.90% examples, 133264 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:16,737 : INFO : EPOCH 1 - PROGRESS: at 62.78% examples, 132888 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:17,745 : INFO : EPOCH 1 - PROGRESS: at 64.58% examples, 132535 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:18,779 : INFO : EPOCH 1 - PROGRESS: at 66.73% examples, 132689 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:19,802 : INFO : EPOCH 1 - PROGRESS: at 68.23% examples, 132141 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:20,809 : INFO : EPOCH 1 - PROGRESS: at 70.32% examples, 132606 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:21,858 : INFO : EPOCH 1 - PROGRESS: at 72.06% examples, 132016 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:22,878 : INFO : EPOCH 1 - PROGRESS: at 74.44% examples, 132576 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:23,957 : INFO : EPOCH 1 - PROGRESS: at 75.96% examples, 131908 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:25,098 : INFO : EPOCH 1 - PROGRESS: at 78.30% examples, 132201 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:26,181 : INFO : EPOCH 1 - PROGRESS: at 80.52% examples, 132668 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:27,244 : INFO : EPOCH 1 - PROGRESS: at 83.02% examples, 133158 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:49:28,316 : INFO : EPOCH 1 - PROGRESS: at 85.39% examples, 133628 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:29,451 : INFO : EPOCH 1 - PROGRESS: at 87.77% examples, 133869 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:30,465 : INFO : EPOCH 1 - PROGRESS: at 89.65% examples, 134171 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:31,472 : INFO : EPOCH 1 - PROGRESS: at 91.43% examples, 134027 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:32,515 : INFO : EPOCH 1 - PROGRESS: at 93.65% examples, 134362 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:33,564 : INFO : EPOCH 1 - PROGRESS: at 95.70% examples, 134549 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:34,566 : INFO : EPOCH 1 - PROGRESS: at 97.77% examples, 134710 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:35,434 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:49:35,437 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:49:35,483 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:49:35,507 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:49:35,607 : INFO : EPOCH 1 - PROGRESS: at 99.77% examples, 134807 words/s, in_qsize 3, out_qsize 1\n",
      "2019-11-20 09:49:35,609 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:49:35,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:49:35,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:49:35,758 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:49:35,759 : INFO : EPOCH - 1 : training on 10316357 raw words (7045816 effective words) took 52.3s, 134823 effective words/s\n",
      "2019-11-20 09:49:36,819 : INFO : EPOCH 2 - PROGRESS: at 1.56% examples, 107972 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:38,010 : INFO : EPOCH 2 - PROGRESS: at 3.32% examples, 103933 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:39,157 : INFO : EPOCH 2 - PROGRESS: at 5.61% examples, 116878 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:40,176 : INFO : EPOCH 2 - PROGRESS: at 7.59% examples, 123671 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:41,185 : INFO : EPOCH 2 - PROGRESS: at 9.22% examples, 121800 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:42,262 : INFO : EPOCH 2 - PROGRESS: at 10.93% examples, 120393 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:43,279 : INFO : EPOCH 2 - PROGRESS: at 12.77% examples, 121236 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:44,319 : INFO : EPOCH 2 - PROGRESS: at 14.58% examples, 122367 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:49:45,341 : INFO : EPOCH 2 - PROGRESS: at 16.56% examples, 123370 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:49:46,391 : INFO : EPOCH 2 - PROGRESS: at 18.30% examples, 122533 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:47,410 : INFO : EPOCH 2 - PROGRESS: at 19.83% examples, 121713 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:48,509 : INFO : EPOCH 2 - PROGRESS: at 21.19% examples, 119683 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:49:49,520 : INFO : EPOCH 2 - PROGRESS: at 22.96% examples, 120318 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:50,574 : INFO : EPOCH 2 - PROGRESS: at 24.55% examples, 118579 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:51,594 : INFO : EPOCH 2 - PROGRESS: at 26.73% examples, 120337 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:49:52,597 : INFO : EPOCH 2 - PROGRESS: at 28.76% examples, 121622 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:53,607 : INFO : EPOCH 2 - PROGRESS: at 30.55% examples, 122342 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:54,698 : INFO : EPOCH 2 - PROGRESS: at 32.85% examples, 123592 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:55,706 : INFO : EPOCH 2 - PROGRESS: at 34.72% examples, 123506 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:56,760 : INFO : EPOCH 2 - PROGRESS: at 36.43% examples, 122780 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:49:57,767 : INFO : EPOCH 2 - PROGRESS: at 38.55% examples, 124214 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:58,825 : INFO : EPOCH 2 - PROGRESS: at 40.50% examples, 124716 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:49:59,936 : INFO : EPOCH 2 - PROGRESS: at 42.35% examples, 124599 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:01,018 : INFO : EPOCH 2 - PROGRESS: at 43.83% examples, 123567 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:02,026 : INFO : EPOCH 2 - PROGRESS: at 45.44% examples, 122712 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:03,027 : INFO : EPOCH 2 - PROGRESS: at 46.68% examples, 121226 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:04,058 : INFO : EPOCH 2 - PROGRESS: at 48.97% examples, 122015 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:05,079 : INFO : EPOCH 2 - PROGRESS: at 50.91% examples, 122232 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:06,120 : INFO : EPOCH 2 - PROGRESS: at 53.05% examples, 123213 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:07,139 : INFO : EPOCH 2 - PROGRESS: at 55.31% examples, 124399 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:08,285 : INFO : EPOCH 2 - PROGRESS: at 57.73% examples, 125061 words/s, in_qsize 13, out_qsize 2\n",
      "2019-11-20 09:50:09,324 : INFO : EPOCH 2 - PROGRESS: at 60.13% examples, 125924 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:10,337 : INFO : EPOCH 2 - PROGRESS: at 61.68% examples, 125401 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:11,339 : INFO : EPOCH 2 - PROGRESS: at 63.25% examples, 125133 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:12,357 : INFO : EPOCH 2 - PROGRESS: at 65.34% examples, 125533 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:50:13,540 : INFO : EPOCH 2 - PROGRESS: at 67.28% examples, 125009 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:14,545 : INFO : EPOCH 2 - PROGRESS: at 68.77% examples, 124772 words/s, in_qsize 16, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:50:15,579 : INFO : EPOCH 2 - PROGRESS: at 70.73% examples, 124977 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:16,702 : INFO : EPOCH 2 - PROGRESS: at 72.40% examples, 124223 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:17,732 : INFO : EPOCH 2 - PROGRESS: at 73.91% examples, 123607 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:18,816 : INFO : EPOCH 2 - PROGRESS: at 75.49% examples, 123194 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:19,881 : INFO : EPOCH 2 - PROGRESS: at 77.30% examples, 123006 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:20,882 : INFO : EPOCH 2 - PROGRESS: at 78.82% examples, 122814 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:21,890 : INFO : EPOCH 2 - PROGRESS: at 80.63% examples, 122944 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:22,952 : INFO : EPOCH 2 - PROGRESS: at 82.10% examples, 122197 words/s, in_qsize 15, out_qsize 1\n",
      "2019-11-20 09:50:24,015 : INFO : EPOCH 2 - PROGRESS: at 83.52% examples, 121454 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:25,020 : INFO : EPOCH 2 - PROGRESS: at 85.10% examples, 121198 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:26,030 : INFO : EPOCH 2 - PROGRESS: at 86.46% examples, 120657 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:27,124 : INFO : EPOCH 2 - PROGRESS: at 88.73% examples, 121262 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:28,179 : INFO : EPOCH 2 - PROGRESS: at 90.87% examples, 121922 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:29,215 : INFO : EPOCH 2 - PROGRESS: at 93.10% examples, 122473 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:30,248 : INFO : EPOCH 2 - PROGRESS: at 94.60% examples, 122160 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:31,250 : INFO : EPOCH 2 - PROGRESS: at 96.80% examples, 122783 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:32,255 : INFO : EPOCH 2 - PROGRESS: at 99.05% examples, 123390 words/s, in_qsize 12, out_qsize 0\n",
      "2019-11-20 09:50:32,559 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:50:32,579 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:50:32,588 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:50:32,622 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:50:32,668 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:50:32,686 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:50:32,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:50:32,826 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:50:32,827 : INFO : EPOCH - 2 : training on 10316357 raw words (7046756 effective words) took 57.1s, 123501 effective words/s\n",
      "2019-11-20 09:50:34,065 : INFO : EPOCH 3 - PROGRESS: at 1.66% examples, 98252 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:35,073 : INFO : EPOCH 3 - PROGRESS: at 4.00% examples, 125985 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:36,271 : INFO : EPOCH 3 - PROGRESS: at 5.61% examples, 115328 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:37,304 : INFO : EPOCH 3 - PROGRESS: at 7.23% examples, 115914 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:38,451 : INFO : EPOCH 3 - PROGRESS: at 8.54% examples, 109123 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:39,642 : INFO : EPOCH 3 - PROGRESS: at 10.84% examples, 113912 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:40,644 : INFO : EPOCH 3 - PROGRESS: at 12.77% examples, 116693 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:41,649 : INFO : EPOCH 3 - PROGRESS: at 14.67% examples, 119514 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:42,756 : INFO : EPOCH 3 - PROGRESS: at 16.95% examples, 121826 words/s, in_qsize 15, out_qsize 3\n",
      "2019-11-20 09:50:44,003 : INFO : EPOCH 3 - PROGRESS: at 19.19% examples, 122656 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:45,009 : INFO : EPOCH 3 - PROGRESS: at 21.10% examples, 124721 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:46,035 : INFO : EPOCH 3 - PROGRESS: at 22.60% examples, 123259 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:47,188 : INFO : EPOCH 3 - PROGRESS: at 24.45% examples, 121832 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:48,230 : INFO : EPOCH 3 - PROGRESS: at 26.44% examples, 122364 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:49,245 : INFO : EPOCH 3 - PROGRESS: at 28.17% examples, 122219 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:50,645 : INFO : EPOCH 3 - PROGRESS: at 29.77% examples, 119478 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:52,079 : INFO : EPOCH 3 - PROGRESS: at 31.30% examples, 116249 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:50:53,103 : INFO : EPOCH 3 - PROGRESS: at 33.44% examples, 117429 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:54,248 : INFO : EPOCH 3 - PROGRESS: at 35.41% examples, 117202 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:50:55,250 : INFO : EPOCH 3 - PROGRESS: at 37.25% examples, 117702 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:56,270 : INFO : EPOCH 3 - PROGRESS: at 38.91% examples, 117756 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:57,374 : INFO : EPOCH 3 - PROGRESS: at 40.69% examples, 117754 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:50:58,477 : INFO : EPOCH 3 - PROGRESS: at 42.61% examples, 118248 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:50:59,519 : INFO : EPOCH 3 - PROGRESS: at 44.44% examples, 118478 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:00,631 : INFO : EPOCH 3 - PROGRESS: at 46.68% examples, 118912 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:01,665 : INFO : EPOCH 3 - PROGRESS: at 48.43% examples, 118528 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:02,675 : INFO : EPOCH 3 - PROGRESS: at 50.59% examples, 119393 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:51:03,719 : INFO : EPOCH 3 - PROGRESS: at 52.26% examples, 119140 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:04,721 : INFO : EPOCH 3 - PROGRESS: at 54.09% examples, 119665 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:05,731 : INFO : EPOCH 3 - PROGRESS: at 56.02% examples, 120117 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:06,797 : INFO : EPOCH 3 - PROGRESS: at 58.23% examples, 120787 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:07,807 : INFO : EPOCH 3 - PROGRESS: at 60.31% examples, 121271 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:08,819 : INFO : EPOCH 3 - PROGRESS: at 62.32% examples, 121852 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:09,846 : INFO : EPOCH 3 - PROGRESS: at 64.20% examples, 121968 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:10,904 : INFO : EPOCH 3 - PROGRESS: at 65.67% examples, 121246 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:11,951 : INFO : EPOCH 3 - PROGRESS: at 67.47% examples, 121119 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:12,993 : INFO : EPOCH 3 - PROGRESS: at 68.86% examples, 120702 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:14,012 : INFO : EPOCH 3 - PROGRESS: at 70.42% examples, 120375 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:15,022 : INFO : EPOCH 3 - PROGRESS: at 72.19% examples, 120256 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:16,050 : INFO : EPOCH 3 - PROGRESS: at 74.32% examples, 120713 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:17,156 : INFO : EPOCH 3 - PROGRESS: at 75.49% examples, 119696 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:18,296 : INFO : EPOCH 3 - PROGRESS: at 76.62% examples, 118491 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:19,319 : INFO : EPOCH 3 - PROGRESS: at 78.20% examples, 118220 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:20,403 : INFO : EPOCH 3 - PROGRESS: at 80.00% examples, 118372 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:21,424 : INFO : EPOCH 3 - PROGRESS: at 81.77% examples, 118272 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:22,437 : INFO : EPOCH 3 - PROGRESS: at 84.00% examples, 118851 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:23,486 : INFO : EPOCH 3 - PROGRESS: at 85.80% examples, 118826 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:24,614 : INFO : EPOCH 3 - PROGRESS: at 88.07% examples, 119249 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:51:25,649 : INFO : EPOCH 3 - PROGRESS: at 89.90% examples, 119765 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:26,659 : INFO : EPOCH 3 - PROGRESS: at 91.94% examples, 120148 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:27,735 : INFO : EPOCH 3 - PROGRESS: at 94.10% examples, 120649 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:28,779 : INFO : EPOCH 3 - PROGRESS: at 96.17% examples, 121080 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:29,935 : INFO : EPOCH 3 - PROGRESS: at 98.55% examples, 121504 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:30,564 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:51:30,575 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:51:30,617 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:51:30,651 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:51:30,696 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:51:30,714 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:51:30,802 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:51:30,927 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:51:30,928 : INFO : EPOCH - 3 : training on 10316357 raw words (7048622 effective words) took 58.1s, 121343 effective words/s\n",
      "2019-11-20 09:51:32,084 : INFO : EPOCH 4 - PROGRESS: at 0.91% examples, 56236 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:33,214 : INFO : EPOCH 4 - PROGRESS: at 2.52% examples, 78044 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:34,343 : INFO : EPOCH 4 - PROGRESS: at 4.81% examples, 100696 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:35,385 : INFO : EPOCH 4 - PROGRESS: at 6.39% examples, 101384 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:36,387 : INFO : EPOCH 4 - PROGRESS: at 8.30% examples, 108919 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:37,430 : INFO : EPOCH 4 - PROGRESS: at 10.07% examples, 111155 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:38,434 : INFO : EPOCH 4 - PROGRESS: at 12.22% examples, 117099 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:39,582 : INFO : EPOCH 4 - PROGRESS: at 13.77% examples, 114922 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:40,605 : INFO : EPOCH 4 - PROGRESS: at 15.54% examples, 115259 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:51:41,646 : INFO : EPOCH 4 - PROGRESS: at 17.61% examples, 117348 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:42,701 : INFO : EPOCH 4 - PROGRESS: at 19.28% examples, 117123 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:43,703 : INFO : EPOCH 4 - PROGRESS: at 20.85% examples, 117449 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:44,715 : INFO : EPOCH 4 - PROGRESS: at 22.69% examples, 118674 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:45,716 : INFO : EPOCH 4 - PROGRESS: at 24.98% examples, 120663 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:46,743 : INFO : EPOCH 4 - PROGRESS: at 27.18% examples, 122703 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:47,798 : INFO : EPOCH 4 - PROGRESS: at 29.41% examples, 124693 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:48,835 : INFO : EPOCH 4 - PROGRESS: at 31.42% examples, 125460 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:51:49,861 : INFO : EPOCH 4 - PROGRESS: at 33.12% examples, 124775 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:50,875 : INFO : EPOCH 4 - PROGRESS: at 35.10% examples, 124933 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:51:51,918 : INFO : EPOCH 4 - PROGRESS: at 37.44% examples, 126470 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:52,944 : INFO : EPOCH 4 - PROGRESS: at 39.36% examples, 127015 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:54,012 : INFO : EPOCH 4 - PROGRESS: at 41.53% examples, 127931 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:55,030 : INFO : EPOCH 4 - PROGRESS: at 43.64% examples, 128994 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:56,063 : INFO : EPOCH 4 - PROGRESS: at 45.88% examples, 129376 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:57,103 : INFO : EPOCH 4 - PROGRESS: at 48.32% examples, 130373 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:58,146 : INFO : EPOCH 4 - PROGRESS: at 50.49% examples, 130722 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:51:59,181 : INFO : EPOCH 4 - PROGRESS: at 52.35% examples, 130543 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:00,229 : INFO : EPOCH 4 - PROGRESS: at 54.17% examples, 130522 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:01,259 : INFO : EPOCH 4 - PROGRESS: at 56.11% examples, 130564 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:02,261 : INFO : EPOCH 4 - PROGRESS: at 57.81% examples, 130103 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:03,286 : INFO : EPOCH 4 - PROGRESS: at 59.82% examples, 130056 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:04,304 : INFO : EPOCH 4 - PROGRESS: at 61.68% examples, 129995 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:05,446 : INFO : EPOCH 4 - PROGRESS: at 63.34% examples, 129260 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:06,567 : INFO : EPOCH 4 - PROGRESS: at 65.14% examples, 128611 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:07,596 : INFO : EPOCH 4 - PROGRESS: at 67.38% examples, 129067 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:08,624 : INFO : EPOCH 4 - PROGRESS: at 69.42% examples, 129733 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:09,741 : INFO : EPOCH 4 - PROGRESS: at 71.86% examples, 130231 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:10,843 : INFO : EPOCH 4 - PROGRESS: at 74.32% examples, 130739 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:11,853 : INFO : EPOCH 4 - PROGRESS: at 76.24% examples, 131013 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:12,886 : INFO : EPOCH 4 - PROGRESS: at 78.47% examples, 131497 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:13,927 : INFO : EPOCH 4 - PROGRESS: at 80.31% examples, 131476 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:14,932 : INFO : EPOCH 4 - PROGRESS: at 82.20% examples, 131257 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:15,956 : INFO : EPOCH 4 - PROGRESS: at 84.00% examples, 130970 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:17,014 : INFO : EPOCH 4 - PROGRESS: at 85.89% examples, 130783 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:18,021 : INFO : EPOCH 4 - PROGRESS: at 88.16% examples, 131298 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:19,025 : INFO : EPOCH 4 - PROGRESS: at 89.90% examples, 131540 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:20,072 : INFO : EPOCH 4 - PROGRESS: at 91.94% examples, 131621 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:52:21,219 : INFO : EPOCH 4 - PROGRESS: at 94.21% examples, 131877 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:22,311 : INFO : EPOCH 4 - PROGRESS: at 96.49% examples, 132254 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:23,314 : INFO : EPOCH 4 - PROGRESS: at 98.73% examples, 132735 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:23,674 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:52:23,707 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:52:23,830 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:52:23,854 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:52:23,880 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:52:23,905 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:52:24,016 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:52:24,024 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:52:24,025 : INFO : EPOCH - 4 : training on 10316357 raw words (7048455 effective words) took 53.1s, 132794 effective words/s\n",
      "2019-11-20 09:52:25,132 : INFO : EPOCH 5 - PROGRESS: at 1.66% examples, 109988 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:26,349 : INFO : EPOCH 5 - PROGRESS: at 4.10% examples, 124611 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:52:27,435 : INFO : EPOCH 5 - PROGRESS: at 6.39% examples, 132306 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:28,472 : INFO : EPOCH 5 - PROGRESS: at 8.46% examples, 136512 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:29,552 : INFO : EPOCH 5 - PROGRESS: at 10.07% examples, 130611 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:30,570 : INFO : EPOCH 5 - PROGRESS: at 12.14% examples, 133146 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:31,570 : INFO : EPOCH 5 - PROGRESS: at 14.07% examples, 134368 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:32,589 : INFO : EPOCH 5 - PROGRESS: at 16.21% examples, 135633 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:33,598 : INFO : EPOCH 5 - PROGRESS: at 18.39% examples, 136811 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:34,630 : INFO : EPOCH 5 - PROGRESS: at 20.16% examples, 136274 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:35,701 : INFO : EPOCH 5 - PROGRESS: at 21.77% examples, 134148 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:36,711 : INFO : EPOCH 5 - PROGRESS: at 23.83% examples, 134741 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:37,775 : INFO : EPOCH 5 - PROGRESS: at 25.84% examples, 134149 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:38,873 : INFO : EPOCH 5 - PROGRESS: at 27.86% examples, 133825 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:39,942 : INFO : EPOCH 5 - PROGRESS: at 30.05% examples, 135068 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:41,042 : INFO : EPOCH 5 - PROGRESS: at 32.43% examples, 135998 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:42,154 : INFO : EPOCH 5 - PROGRESS: at 34.91% examples, 136628 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-20 09:52:43,157 : INFO : EPOCH 5 - PROGRESS: at 37.16% examples, 137612 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:44,200 : INFO : EPOCH 5 - PROGRESS: at 38.99% examples, 137188 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:45,216 : INFO : EPOCH 5 - PROGRESS: at 41.07% examples, 137697 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:46,262 : INFO : EPOCH 5 - PROGRESS: at 42.97% examples, 137622 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:47,263 : INFO : EPOCH 5 - PROGRESS: at 45.07% examples, 137842 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:48,339 : INFO : EPOCH 5 - PROGRESS: at 47.44% examples, 137995 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:49,370 : INFO : EPOCH 5 - PROGRESS: at 49.69% examples, 138169 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:50,427 : INFO : EPOCH 5 - PROGRESS: at 51.62% examples, 137585 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:51,438 : INFO : EPOCH 5 - PROGRESS: at 53.72% examples, 138224 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:52:52,505 : INFO : EPOCH 5 - PROGRESS: at 55.59% examples, 137806 words/s, in_qsize 16, out_qsize 1\n",
      "2019-11-20 09:52:53,607 : INFO : EPOCH 5 - PROGRESS: at 57.72% examples, 137517 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:54,654 : INFO : EPOCH 5 - PROGRESS: at 60.13% examples, 138010 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:55,673 : INFO : EPOCH 5 - PROGRESS: at 62.17% examples, 138104 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:56,785 : INFO : EPOCH 5 - PROGRESS: at 64.09% examples, 137569 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:57,806 : INFO : EPOCH 5 - PROGRESS: at 66.22% examples, 137610 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:58,862 : INFO : EPOCH 5 - PROGRESS: at 68.32% examples, 137922 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:52:59,897 : INFO : EPOCH 5 - PROGRESS: at 70.52% examples, 138336 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:53:01,017 : INFO : EPOCH 5 - PROGRESS: at 72.82% examples, 138205 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:02,148 : INFO : EPOCH 5 - PROGRESS: at 74.73% examples, 137495 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:53:03,165 : INFO : EPOCH 5 - PROGRESS: at 76.05% examples, 136543 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:53:04,179 : INFO : EPOCH 5 - PROGRESS: at 77.38% examples, 135291 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:05,182 : INFO : EPOCH 5 - PROGRESS: at 78.90% examples, 134773 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:06,209 : INFO : EPOCH 5 - PROGRESS: at 80.85% examples, 134731 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:07,220 : INFO : EPOCH 5 - PROGRESS: at 83.21% examples, 135193 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:08,224 : INFO : EPOCH 5 - PROGRESS: at 85.10% examples, 135058 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:53:09,225 : INFO : EPOCH 5 - PROGRESS: at 87.35% examples, 135523 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:10,234 : INFO : EPOCH 5 - PROGRESS: at 89.04% examples, 135366 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:11,239 : INFO : EPOCH 5 - PROGRESS: at 90.57% examples, 134933 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:12,253 : INFO : EPOCH 5 - PROGRESS: at 92.52% examples, 134890 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:13,277 : INFO : EPOCH 5 - PROGRESS: at 94.60% examples, 135140 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-20 09:53:14,318 : INFO : EPOCH 5 - PROGRESS: at 96.71% examples, 135322 words/s, in_qsize 16, out_qsize 0\n",
      "2019-11-20 09:53:15,335 : INFO : EPOCH 5 - PROGRESS: at 98.94% examples, 135711 words/s, in_qsize 13, out_qsize 0\n",
      "2019-11-20 09:53:15,555 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-20 09:53:15,604 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-20 09:53:15,638 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-20 09:53:15,711 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-20 09:53:15,775 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-20 09:53:15,799 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-20 09:53:15,848 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-20 09:53:15,869 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-20 09:53:15,870 : INFO : EPOCH - 5 : training on 10316357 raw words (7045544 effective words) took 51.8s, 135930 effective words/s\n",
      "2019-11-20 09:53:15,870 : INFO : training on a 51581785 raw words (35235193 effective words) took 272.4s, 129358 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35235193, 51581785)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_corrected.build_vocab(flattened_list_sentences, update=True)\n",
    "embedding_model_corrected.train(flattened_list_sentences, \n",
    "                                total_examples=embedding_model_corrected.corpus_count,\n",
    "                                epochs=w2v_args.epochs,  \n",
    "                                compute_loss=w2v_args.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:53:15,879 : INFO : saving Word2Vec object under ./w2v_005_EM_corr_qual_1_2.model, separately None\n",
      "2019-11-20 09:53:15,880 : INFO : storing np array 'vectors' to ./w2v_005_EM_corr_qual_1_2.model.wv.vectors.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] Save the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-20 09:53:17,260 : INFO : not storing attribute vectors_norm\n",
      "2019-11-20 09:53:17,261 : INFO : storing np array 'syn1neg' to ./w2v_005_EM_corr_qual_1_2.model.trainables.syn1neg.npy\n",
      "2019-11-20 09:53:18,450 : INFO : not storing attribute cum_table\n",
      "2019-11-20 09:53:19,527 : INFO : saved ./w2v_005_EM_corr_qual_1_2.model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n[INFO] Save the model\")\n",
    "embedding_model_corrected.save(\"./w2v_005_EM_corr_qual_1_2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
