{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment of the impact of OCR on linguistic tasks\n",
    "\n",
    "In particular, in this notebook we're looking at:\n",
    "- Part-of-speech tagging accuracy (fine- and coarse-grained)\n",
    "- Named entity recognition accuracy (matching type, matching type and IOB-tag)\n",
    "    - Persons: f-score (by quality band)\n",
    "    - Geopolitical entities: f-score (by quality band)\n",
    "    - Dates: f-score (by quality band)\n",
    "- Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sample and apply Spacy\n",
    "\n",
    "Obtain a sample of the data for which we have the same number of articles from each quality band. Apply Spacy to the OCR and the human-corrected texts, and save the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacyfile = pathlib.Path(\"trove_subsample_aligned_spacy.pkl\")\n",
    "trovedf = pd.DataFrame()\n",
    "sampledf_band1 = pd.DataFrame()\n",
    "sampledf_band2 = pd.DataFrame()\n",
    "sampledf_band3 = pd.DataFrame()\n",
    "sampledf_band4 = pd.DataFrame()\n",
    "sampledf = pd.DataFrame()\n",
    "if not spacyfile.exists():\n",
    "    trovedf = pd.read_pickle(\"trove_subsample_aligned.pkl\")\n",
    "    sampledf_band1 = trovedf[trovedf['quality_band'] == 1].sample(950)\n",
    "    sampledf_band2 = trovedf[trovedf['quality_band'] == 2].sample(950)\n",
    "    sampledf_band3 = trovedf[trovedf['quality_band'] == 3].sample(950)\n",
    "    sampledf_band4 = trovedf[trovedf['quality_band'] == 4].sample(950)\n",
    "    sampledf = pd.concat([sampledf_band1, sampledf_band2, sampledf_band3, sampledf_band4])\n",
    "    sampledf['spacyOcr'] = sampledf['ocrText'].apply(nlp)\n",
    "    sampledf['spacyHum'] = sampledf['corrected'].apply(nlp)\n",
    "    sampledf.to_pickle('trove_subsample_aligned_spacy.pkl')\n",
    "else:\n",
    "    sampledf = pd.read_pickle('trove_subsample_aligned_spacy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filePath</th>\n",
       "      <th>articleId</th>\n",
       "      <th>articleType</th>\n",
       "      <th>year</th>\n",
       "      <th>ocrText</th>\n",
       "      <th>humanText</th>\n",
       "      <th>corrected</th>\n",
       "      <th>str_similarity</th>\n",
       "      <th>str_length</th>\n",
       "      <th>quality_band</th>\n",
       "      <th>use_corrected</th>\n",
       "      <th>alignment</th>\n",
       "      <th>processed</th>\n",
       "      <th>spacyOcr</th>\n",
       "      <th>spacyHum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11720</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>12968079</td>\n",
       "      <td>Article</td>\n",
       "      <td>1855</td>\n",
       "      <td>RUSSIA. A letter from St. Petersburg of tho 29...</td>\n",
       "      <td>RUSSIA. A letter from St. Petersburg of the 29...</td>\n",
       "      <td>RUSSIA. A letter from St. Petersburg of the 29...</td>\n",
       "      <td>0.920876</td>\n",
       "      <td>3397</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 7, 0, 7), (8, 9, 8, 9), (10, 16, 10, 16),...</td>\n",
       "      <td>yes</td>\n",
       "      <td>(RUSSIA, ., A, letter, from, St., Petersburg, ...</td>\n",
       "      <td>(RUSSIA, ., A, letter, from, St., Petersburg, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26626</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>14223127</td>\n",
       "      <td>Article</td>\n",
       "      <td>1900</td>\n",
       "      <td>AUSTRALIAN UNION BENEFIT SOCIETY. The annual g...</td>\n",
       "      <td>AUSTRALIAN UNION BENEFIT SOCIETY. The annual g...</td>\n",
       "      <td>AUSTRALIAN UNION BENEFIT SOCIETY. The annual g...</td>\n",
       "      <td>0.901502</td>\n",
       "      <td>2851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 10, 0, 10), (11, 16, 11, 16), (17, 24, 17...</td>\n",
       "      <td>yes</td>\n",
       "      <td>(AUSTRALIAN, UNION, BENEFIT, SOCIETY, ., The, ...</td>\n",
       "      <td>(AUSTRALIAN, UNION, BENEFIT, SOCIETY, ., The, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14814</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>27972895</td>\n",
       "      <td>Article</td>\n",
       "      <td>1939</td>\n",
       "      <td>HOME NEWS; G.f'.O. Contract. Tim unexpected si...</td>\n",
       "      <td>HOME NEWS; G.P.O. Contract. The unexpected sig...</td>\n",
       "      <td>HOME NEWS; G.P.O. Contract. The unexpected sig...</td>\n",
       "      <td>0.916455</td>\n",
       "      <td>4716</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 4, 0, 4), (5, 10, 5, 10), (11, 18, 11, 17...</td>\n",
       "      <td>yes</td>\n",
       "      <td>(HOME, NEWS, ;, G.f'.O., Contract, ., Tim, une...</td>\n",
       "      <td>(HOME, NEWS, ;, G.P.O., Contract, ., The, unex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18765</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>14817371</td>\n",
       "      <td>Article</td>\n",
       "      <td>1906</td>\n",
       "      <td>DIVORCE COURT. (Before Mr. Justice Simpson.) T...</td>\n",
       "      <td>DIVORCE COURT. (Before Mr. Justice Simpson.) T...</td>\n",
       "      <td>DIVORCE COURT. (Before Mr. Justice Simpson.) T...</td>\n",
       "      <td>0.921374</td>\n",
       "      <td>3021</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 7, 0, 7), (8, 14, 8, 14), (15, 22, 15, 22...</td>\n",
       "      <td>yes</td>\n",
       "      <td>(DIVORCE, COURT, ., (, Before, Mr., Justice, S...</td>\n",
       "      <td>(DIVORCE, COURT, ., (, Before, Mr., Justice, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20995</td>\n",
       "      <td>./trove_overproof/datasets/dataset1/rawTextAnd...</td>\n",
       "      <td>13324224</td>\n",
       "      <td>Article</td>\n",
       "      <td>1873</td>\n",
       "      <td>PORT MACRAT. DKFABTUBI. Octobers.-Hauaah Newto...</td>\n",
       "      <td>PORT MACKAY. DEPARTURE. Octobers 3.—Hannah New...</td>\n",
       "      <td>PORT MACKAY. DEPARTURE. Octobers 3.—Hannah New...</td>\n",
       "      <td>0.911892</td>\n",
       "      <td>1615</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 4, 0, 4), (5, 12, 5, 12), (41, 48, 43, 50...</td>\n",
       "      <td>yes</td>\n",
       "      <td>(PORT, MACRAT, ., DKFABTUBI, ., Octobers.-Haua...</td>\n",
       "      <td>(PORT, MACKAY, ., DEPARTURE, ., Octobers, 3.—H...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filePath articleId  \\\n",
       "11720  ./trove_overproof/datasets/dataset1/rawTextAnd...  12968079   \n",
       "26626  ./trove_overproof/datasets/dataset1/rawTextAnd...  14223127   \n",
       "14814  ./trove_overproof/datasets/dataset1/rawTextAnd...  27972895   \n",
       "18765  ./trove_overproof/datasets/dataset1/rawTextAnd...  14817371   \n",
       "20995  ./trove_overproof/datasets/dataset1/rawTextAnd...  13324224   \n",
       "\n",
       "      articleType  year                                            ocrText  \\\n",
       "11720     Article  1855  RUSSIA. A letter from St. Petersburg of tho 29...   \n",
       "26626     Article  1900  AUSTRALIAN UNION BENEFIT SOCIETY. The annual g...   \n",
       "14814     Article  1939  HOME NEWS; G.f'.O. Contract. Tim unexpected si...   \n",
       "18765     Article  1906  DIVORCE COURT. (Before Mr. Justice Simpson.) T...   \n",
       "20995     Article  1873  PORT MACRAT. DKFABTUBI. Octobers.-Hauaah Newto...   \n",
       "\n",
       "                                               humanText  \\\n",
       "11720  RUSSIA. A letter from St. Petersburg of the 29...   \n",
       "26626  AUSTRALIAN UNION BENEFIT SOCIETY. The annual g...   \n",
       "14814  HOME NEWS; G.P.O. Contract. The unexpected sig...   \n",
       "18765  DIVORCE COURT. (Before Mr. Justice Simpson.) T...   \n",
       "20995  PORT MACKAY. DEPARTURE. Octobers 3.—Hannah New...   \n",
       "\n",
       "                                               corrected  str_similarity  \\\n",
       "11720  RUSSIA. A letter from St. Petersburg of the 29...        0.920876   \n",
       "26626  AUSTRALIAN UNION BENEFIT SOCIETY. The annual g...        0.901502   \n",
       "14814  HOME NEWS; G.P.O. Contract. The unexpected sig...        0.916455   \n",
       "18765  DIVORCE COURT. (Before Mr. Justice Simpson.) T...        0.921374   \n",
       "20995  PORT MACKAY. DEPARTURE. Octobers 3.—Hannah New...        0.911892   \n",
       "\n",
       "       str_length  quality_band  use_corrected  \\\n",
       "11720        3397             1              0   \n",
       "26626        2851             1              0   \n",
       "14814        4716             1              0   \n",
       "18765        3021             1              0   \n",
       "20995        1615             1              0   \n",
       "\n",
       "                                               alignment processed  \\\n",
       "11720  [(0, 7, 0, 7), (8, 9, 8, 9), (10, 16, 10, 16),...       yes   \n",
       "26626  [(0, 10, 0, 10), (11, 16, 11, 16), (17, 24, 17...       yes   \n",
       "14814  [(0, 4, 0, 4), (5, 10, 5, 10), (11, 18, 11, 17...       yes   \n",
       "18765  [(0, 7, 0, 7), (8, 14, 8, 14), (15, 22, 15, 22...       yes   \n",
       "20995  [(0, 4, 0, 4), (5, 12, 5, 12), (41, 48, 43, 50...       yes   \n",
       "\n",
       "                                                spacyOcr  \\\n",
       "11720  (RUSSIA, ., A, letter, from, St., Petersburg, ...   \n",
       "26626  (AUSTRALIAN, UNION, BENEFIT, SOCIETY, ., The, ...   \n",
       "14814  (HOME, NEWS, ;, G.f'.O., Contract, ., Tim, une...   \n",
       "18765  (DIVORCE, COURT, ., (, Before, Mr., Justice, S...   \n",
       "20995  (PORT, MACRAT, ., DKFABTUBI, ., Octobers.-Haua...   \n",
       "\n",
       "                                                spacyHum  \n",
       "11720  (RUSSIA, ., A, letter, from, St., Petersburg, ...  \n",
       "26626  (AUSTRALIAN, UNION, BENEFIT, SOCIETY, ., The, ...  \n",
       "14814  (HOME, NEWS, ;, G.P.O., Contract, ., The, unex...  \n",
       "18765  (DIVORCE, COURT, ., (, Before, Mr., Justice, S...  \n",
       "20995  (PORT, MACKAY, ., DEPARTURE, ., Octobers, 3.—H...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSplitting(spacyOcr, spacyHum, alignment):\n",
    "    \"\"\"\n",
    "    We consider a sentence as being correctly split if\n",
    "    both left and right boundaries enclose the exact same\n",
    "    aligned tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # dAligments is a dictionary with the following structure:\n",
    "    # dAlignments = {spacyHum_startchar:spacyOcr_startchar}\n",
    "    dAlignments = dict()\n",
    "    for a in alignment:\n",
    "        dAlignments[a[2]] = a[0]\n",
    "    \n",
    "    # humanSentences is a list of lists:\n",
    "    # - the outer list is the list of sentences from the spacyHum column.\n",
    "    # - each inner list is the list of words in each sentence, represented as the\n",
    "    # word's start character.\n",
    "    #\n",
    "    # expectedOCRSentences is a list of lists:\n",
    "    # - the outer list is the list of sentences from the human-corrected column as\n",
    "    # well (i.e. the list of sentences we would expect if the OCR was identical to\n",
    "    # the human-corrected text).\n",
    "    # - each inner list is the list of words in each expected sentence, represented\n",
    "    # as the start character of the OCR token that is aligned to the expected human-\n",
    "    # corrected token. We therefore should expect humanSentences and expectedOCRSentences\n",
    "    # to have the exact same shape.\n",
    "    humanSentences = []\n",
    "    expectedOCRSentences = []\n",
    "    for sent in spacyHum.sents:\n",
    "        newHumSentence = []\n",
    "        expectedOCRSentence = []\n",
    "        for a in dAlignments:\n",
    "            if a in range(sent.start_char, sent.end_char):\n",
    "                newHumSentence.append(a)\n",
    "                expectedOCRSentence.append(dAlignments[a])\n",
    "        humanSentences.append(newHumSentence)\n",
    "        expectedOCRSentences.append(expectedOCRSentence)\n",
    "    \n",
    "    # ocrSentences is a list of lists:\n",
    "    # - the outer list is the list of sentences from the spacyOcr column.\n",
    "    # - each inner list is the list of words in each sentence in spacyOcr, represented\n",
    "    # as the word's start character.\n",
    "    # If the OCR text was identical to its human-correction, ocrSentences and\n",
    "    # expectedOCRSentences will be identical.\n",
    "    ocrSentences = []\n",
    "    for sent in spacyOcr.sents:\n",
    "        newOCRSentence = []\n",
    "        for a in alignment:\n",
    "            if a[0] in range(sent.start_char, sent.end_char):\n",
    "                newOCRSentence.append(a[0])\n",
    "        ocrSentences.append(newOCRSentence)\n",
    "    \n",
    "    # Calculate precision:\n",
    "    # - true positive: each list of tokens that is exactly the same between the\n",
    "    # expected OCR sentence and the true OCR sentence.\n",
    "    # - false positive: each list of tokens that is found in the true OCR sentence\n",
    "    # but not in the expected OCR sentence.\n",
    "    tp_precision = 0\n",
    "    fp_precision = 0\n",
    "    for sent in ocrSentences:\n",
    "        if sent:\n",
    "            if sent in expectedOCRSentences:\n",
    "                tp_precision += 1\n",
    "            else:\n",
    "                fp_precision += 1\n",
    "    \n",
    "    precision = 0\n",
    "    if tp_precision + fp_precision > 0:\n",
    "        precision = tp_precision / float(tp_precision + fp_precision)\n",
    "    \n",
    "    allsentences = 0\n",
    "    \n",
    "    # Calculate recall:\n",
    "    # - true positive: each list of tokens that is exactly the same between the\n",
    "    # expected OCR sentence and the true OCR sentence.\n",
    "    # - false positive: each list of tokens that is found in the expected OCR sentence\n",
    "    # but not in the true OCR sentence.\n",
    "    tp_recall = 0\n",
    "    fn_recall = 0\n",
    "    for sent in expectedOCRSentences:\n",
    "        if sent:\n",
    "            allsentences += 1\n",
    "            if sent in ocrSentences:\n",
    "                tp_recall += 1\n",
    "            else:\n",
    "                fn_recall += 1\n",
    "    \n",
    "    recall = 0\n",
    "    if tp_recall + fn_recall > 0:\n",
    "        recall = tp_recall / float(tp_recall + fn_recall)\n",
    "    \n",
    "    # Calculate fscore:\n",
    "    fscore = None\n",
    "    if precision + recall > 0:\n",
    "        fscore = (2.0 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Calculate absolute accuracy:\n",
    "    # Accuracy is calculated as, from all the expected OCR sentences, those\n",
    "    # that are true OCR sentences.\n",
    "    sentsegacc = None\n",
    "    if allsentences > 0:\n",
    "        sentsegacc = tp_recall / float(allsentences)\n",
    "    \n",
    "    return fscore, sentsegacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentenceSplitting(...) function:\n",
    "for i, row in sampledf.iloc[:1].iterrows():\n",
    "    alignment = ast.literal_eval(row['alignment'])\n",
    "    spacyOcr = row['spacyOcr']\n",
    "    spacyHum = row['spacyHum']\n",
    "    sentenceSplitting(spacyOcr, spacyHum, alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateNer(ocrText, humText, spacyOcrText, spacyHumText, absoluteIndexLinking, target):\n",
    "    \n",
    "    # dOcrTokenNer is a dictionary with the following structure:\n",
    "    # dOcrTokenNer = {spacyOcrToken_startchar:(spacyOcrToken_entiob, spacyOcrToken_enttype, spacyOcrToken_text)}\n",
    "    dOcrTokenNer = dict()\n",
    "    for token in spacyOcrText:\n",
    "        tokenIndicesNer = token.idx\n",
    "        dOcrTokenNer[tokenIndicesNer] = (token.ent_iob_, token.ent_type_, token.text)\n",
    "    \n",
    "    # dHumTokenNer is a dictionary with the following structure:\n",
    "    # dHumTokenNer = {spacyHumToken_startchar:(spacyHumToken_entiob, spacyHumToken_enttype, spacyHumToken_text)}\n",
    "    # -> where entiob is the IOB tag (i.e. 'inside', 'outside', 'beginning'),\n",
    "    # -> where enttype is the entity type (i.e. 'PER' for person, 'GPE' for geopolitical entity, 'DATE' for date).\n",
    "    dHumTokenNer = dict()\n",
    "    for token in spacyHumText:\n",
    "        tokenIndicesNer = token.idx\n",
    "        dHumTokenNer[tokenIndicesNer] = (token.ent_iob_, token.ent_type_, token.text)\n",
    "    \n",
    "    # Measures for accuracy:\n",
    "    all_matched_words = 0 # overall\n",
    "    correct_type = 0 # correct entity type\n",
    "    correct_iob_type = 0 # correct entity type and iob tag\n",
    "    \n",
    "    # Per-type measures:\n",
    "    person_tp = 0 # person true positives\n",
    "    person_fp = 0 # person false positives\n",
    "    person_fn = 0 # person false negatives\n",
    "    gpe_tp = 0 # geo-political entities true positives\n",
    "    gpe_fp = 0 # geo-political entities false positives\n",
    "    gpe_fn = 0 # geo-political entities false negatives\n",
    "    date_tp = 0 # date entities true positives\n",
    "    date_fp = 0 # date entities false positives\n",
    "    date_fn = 0 # date entities false negatives\n",
    "    \n",
    "    # Loop over the aligned tokens. Each matched_word is a tuple with the following strucure:\n",
    "    # matched_word = (spacyOcrToken_startchar, spacyOcrToken_endchar, spacyHumToken_startchar, spacyHumToken_endchar)\n",
    "    for matched_word in absoluteIndexLinking:\n",
    "        if matched_word[0] in dOcrTokenNer and matched_word[2] in dHumTokenNer:\n",
    "            labelOcr = dOcrTokenNer[matched_word[0]][1].strip() # entity type of ocr token\n",
    "            labelHum = dHumTokenNer[matched_word[2]][1].strip() # entity type of aligned human-corrected token\n",
    "            iobOcr = dOcrTokenNer[matched_word[0]][0].strip() # iob tag of ocr token\n",
    "            iobHum = dHumTokenNer[matched_word[2]][0].strip() # iob tag of aligned human-corrected token\n",
    "            \n",
    "            # Target token:\n",
    "            # -> \"correct\" if identical between OCR and human (i.e. alignment is a perfect string match), \n",
    "            # -> \"incorrect\" if OCR is different, \n",
    "            # -> \"all\" if we don't care\n",
    "            target_condition = False\n",
    "            if target == \"correct\":\n",
    "                if dOcrTokenNer[matched_word[0]][2].lower() == dHumTokenNer[matched_word[2]][2].lower():\n",
    "                    target_condition = True\n",
    "            if target == \"incorrect\":\n",
    "                if dOcrTokenNer[matched_word[0]][2].lower() != dHumTokenNer[matched_word[2]][2].lower():\n",
    "                    target_condition = True\n",
    "            if target == \"all\":\n",
    "                target_condition = True\n",
    "            \n",
    "            # Different measures conditioned on the target:\n",
    "            # -> correct_type = entity type of human token is the same as its corresponding ocr token\n",
    "            # -> correct_iob_type = both entity type and IOB tag are the same\n",
    "            # -> person_tp = entity type is 'person' both in ocr and hum token\n",
    "            # -> gpe_tp = entity type is 'gpe' both in ocr and hum token\n",
    "            # -> date_tp = entity type is 'date' both in ocr and hum token\n",
    "            # -> person_fn = entity type is 'person' in ocr and something else in hum token\n",
    "            # -> gpe_fn = entity type is 'gpe' in ocr and something else in hum token\n",
    "            # -> date_fn = entity type is 'date' in ocr and something else in hum token\n",
    "            # -> person_fp = entity type is 'person' in hum and something else ocr token\n",
    "            # -> gpe_fp = entity type is 'gpe' in hum and something else ocr token\n",
    "            # -> date_fp = entity type is 'date' in hum and something else ocr token\n",
    "            if target_condition == True:\n",
    "                all_matched_words += 1\n",
    "                if labelOcr == labelHum:\n",
    "                    correct_type += 1\n",
    "                    if iobOcr == iobHum:\n",
    "                        correct_iob_type += 1\n",
    "                    if labelOcr == 'PERSON':\n",
    "                        person_tp += 1\n",
    "                    if labelOcr == 'GPE':\n",
    "                        gpe_tp += 1\n",
    "                    if labelOcr == 'DATE':\n",
    "                        date_tp += 1\n",
    "                if labelOcr != labelHum:\n",
    "                    if labelOcr == 'PERSON':\n",
    "                        person_fp += 1\n",
    "                    if labelOcr == 'GPE':\n",
    "                        gpe_fp += 1\n",
    "                    if labelOcr == 'DATE':\n",
    "                        date_fp += 1\n",
    "                    if labelHum == 'PERSON':\n",
    "                        person_fn += 1\n",
    "                    if labelHum == 'GPE':\n",
    "                        gpe_fn += 1\n",
    "                    if labelHum == 'DATE':\n",
    "                        date_fn += 1\n",
    "    \n",
    "    # Accuracy of entity type and IOB tag (not conditioned on the target):\n",
    "    if all_matched_words == 0:\n",
    "        accuracy_type = None\n",
    "        accuracy_iob_type = None\n",
    "    else:\n",
    "        accuracy_type = correct_type/float(all_matched_words)\n",
    "        accuracy_iob_type = correct_iob_type/float(all_matched_words)\n",
    "    \n",
    "    return accuracy_type, accuracy_iob_type, person_tp, person_fp, person_fn, gpe_tp, gpe_fp, gpe_fn, date_tp, date_fp, date_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluateNer(...) function:\n",
    "for i, row in sampledf.iloc[:1].iterrows():\n",
    "    target = \"correct\" # 'correct', 'incorrect', 'all'\n",
    "    ocrText = row['ocrText'].strip(\" \")\n",
    "    humanText = row['corrected'].strip(\" \")\n",
    "    spacyOcr = row['spacyOcr']\n",
    "    spacyHum = row['spacyHum']\n",
    "    alignment = ast.literal_eval(row['alignment'])\n",
    "    evaluateNer(ocrText, humanText, spacyOcr, spacyHum, alignment, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateDependencyParsing(noisyText, cleanText, spacyNoisyText, spacyCleanText, absoluteIndexLinking, dependency_length):\n",
    "    dAlignedOcrHum = dict()\n",
    "    dAlignedHumOcr = dict()\n",
    "    ocrWords = []\n",
    "    humWords = []\n",
    "    \n",
    "    # dAlignedOcrHum is a dictionary with the following structure:\n",
    "    # dAlignedOcrHum = {spacyOcrToken_startchar : spacyHumToken_startchar}\n",
    "    # dAlignedHumOcr = {spacyHumToken_startchar : spacyOcrToken_startchar}\n",
    "    # \n",
    "    # ocrWords is the list of tokens from the spacyOcr that have been succesfully aligned,\n",
    "    # represented as the start character of the token.\n",
    "    # humWords is the list of tokens from the spacyHum that have been succesfully aligned,\n",
    "    # represented as the start character of the token.\n",
    "    for a in absoluteIndexLinking:\n",
    "        ocrWords.append((a[0]))\n",
    "        humWords.append((a[2]))\n",
    "        dAlignedOcrHum[(a[0])] = a[2]\n",
    "        dAlignedHumOcr[(a[2])] = a[0]\n",
    "    \n",
    "    # dNoisyTokenDeps is a dictionary with the following structure:\n",
    "    # dNoisyTokenDeps = {spacyOcrToken_startChar : (spacyOcrToken_headStartChar, spacyOcrToken_dependencyType, spacyOcrToken_children, spacyOcrToken_deplength)}\n",
    "    dNoisyTokenDeps = dict()\n",
    "    for token in spacyNoisyText:\n",
    "        noisy_dependency_span = []\n",
    "        tokenIndex = (token.idx)\n",
    "        tokenHeadIndex = (token.head.idx)\n",
    "        if tokenIndex < tokenHeadIndex:\n",
    "            noisy_dependency_span = noisyText[tokenIndex:tokenHeadIndex].strip().split()\n",
    "        elif tokenIndex > tokenHeadIndex:\n",
    "            noisy_dependency_span = noisyText[tokenHeadIndex:tokenIndex].strip().split()\n",
    "        dNoisyTokenDeps[tokenIndex] = (tokenHeadIndex, token.dep_, [child for child in token.children], len(noisy_dependency_span))\n",
    "    \n",
    "    # dCleanTokenDeps is a dictionary with the following structure:\n",
    "    # dCleanTokenDeps = {spacyHumToken_startChar : (spacyHumToken_headStartChar, spacyHumToken_dependencyType, spacyHumToken_children, spacyHumToken_deplength)}\n",
    "    dCleanTokenDeps = dict()\n",
    "    for token in spacyCleanText:\n",
    "        clean_dependency_span = []\n",
    "        tokenIndex = (token.idx)\n",
    "        tokenHeadIndex = (token.head.idx)\n",
    "        if tokenIndex < tokenHeadIndex:\n",
    "            clean_dependency_span = cleanText[tokenIndex:tokenHeadIndex].strip().split()\n",
    "        elif tokenIndex > tokenHeadIndex:\n",
    "            clean_dependency_span = cleanText[tokenHeadIndex:tokenIndex].strip().split()\n",
    "        dCleanTokenDeps[tokenIndex] = (tokenHeadIndex, token.dep_, [child for child in token.children], len(clean_dependency_span))\n",
    "    \n",
    "    # dNoisyDict is a dictionary with the following structure:\n",
    "    # dNoisyDict = {(spacyOcrToken_startChar, spacyOcrToken_headStartChar) : (dependencyType, numberOfChildren, spacyOcrToken_deplength)}\n",
    "    # -> where dependency type is the dependency type between the token and its head.\n",
    "    # -> where numberOfChildren is the number of children of the token in question\n",
    "    # -> where spacyOcrToken_deplength is the length of the dependency between the token and its head\n",
    "    dNoisyDict = dict()\n",
    "    for ocrWord1 in ocrWords:\n",
    "        for ocrWord2 in ocrWords:\n",
    "            if ocrWord1 in dNoisyTokenDeps:\n",
    "                if ocrWord2 == dNoisyTokenDeps[ocrWord1][0]:\n",
    "                    dNoisyDict[(ocrWord1, ocrWord2)] = (dNoisyTokenDeps[ocrWord1][1], len(dNoisyTokenDeps[ocrWord1][2]), dNoisyTokenDeps[ocrWord1][-1])\n",
    "                    \n",
    "    # dCleanDict is a dictionary with the following structure:\n",
    "    # dCleanDict = {(spacyHumToken_startChar, spacyHumToken_headStartChar) : (dependencyType, numberOfChildren, spacyHumToken_deplength)}\n",
    "    # -> where dependency type is the dependency type between the token and its head.\n",
    "    # -> where numberOfChildren is the number of children of the token in question\n",
    "    # -> where spacyHumToken_deplength is the length of the dependency between the token and its head\n",
    "    dCleanDict = dict()\n",
    "    for humWord1 in humWords:\n",
    "        for humWord2 in humWords:\n",
    "            if humWord1 in dCleanTokenDeps:\n",
    "                if humWord2 == dCleanTokenDeps[humWord1][0]:\n",
    "                    dCleanDict[(humWord1, humWord2)] = (dCleanTokenDeps[humWord1][1], len(dCleanTokenDeps[humWord1][2]), dCleanTokenDeps[humWord1][-1])\n",
    "    \n",
    "    # allOcrLabels is a counter of all dependencies for which we have an aligned token\n",
    "    # in both sides of the dependency.\n",
    "    # correctLabel is a counter of all the dependencies between aligned OCR and human-corrected\n",
    "    # tokens that have the same entity type.\n",
    "    allOcrLabels = 0\n",
    "    correctLabel = 0\n",
    "    for i in dNoisyDict:\n",
    "        allOcrLabels += 1\n",
    "        ocrTok = i[0]\n",
    "        ocrHead = i[1]\n",
    "        humTok = dAlignedOcrHum[i[0]]\n",
    "        humHead = dAlignedOcrHum[i[1]]\n",
    "        if (humTok, humHead) in dCleanDict:\n",
    "            if dCleanDict[humTok, humHead][0] == dNoisyDict[i][0]:\n",
    "                correctLabel += 1\n",
    "        elif (humHead, humTok) in dCleanDict:\n",
    "            if dCleanDict[humHead, humTok][0] == dNoisyDict[i][0]:\n",
    "                correctLabel += 1\n",
    "    if allOcrLabels == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = correctLabel/float(allOcrLabels)\n",
    "    \n",
    "    # allHumLabels is a counter of all dependencies for which we have an aligned token\n",
    "    # in both sides of the dependency.\n",
    "    # correctLabel is a counter of all the dependencies between aligned OCR and human-corrected\n",
    "    # tokens that have the same entity type.\n",
    "    allHumLabels = 0\n",
    "    correctLabel = 0    \n",
    "    for i in dCleanDict:\n",
    "        dep_condition = False\n",
    "        # \"all\" for any distance between heads\n",
    "        # \"neighbour\" for only neighbouring relations between heads\n",
    "        # \"close\" for distance of three tokens between heads\n",
    "        # \"far\" for distance of above five tokens between heads\n",
    "        # \"nsubj\" for nsubj relationship\n",
    "        # \"compound\" for compound relationship\n",
    "        if dependency_length == \"all\":\n",
    "            dep_condition = True\n",
    "        elif dependency_length == \"neighbour\":\n",
    "            if dCleanDict[i][-1] <= 1:\n",
    "                dep_condition = True\n",
    "        elif dependency_length == \"close\":\n",
    "            if dCleanDict[i][-1] >= 3:\n",
    "                dep_condition = True\n",
    "        elif dependency_length == \"far\":\n",
    "            if dCleanDict[i][-1] >= 5:\n",
    "                dep_condition = True\n",
    "        elif dependency_length == \"nsubj\":\n",
    "            if dCleanDict[i][0] == \"nsubj\":\n",
    "                dep_condition = True\n",
    "        elif dependency_length == \"compound\":\n",
    "            if dCleanDict[i][0] == \"compound\":\n",
    "                dep_condition = True\n",
    "        \n",
    "        if dep_condition == True:\n",
    "            allHumLabels += 1\n",
    "            ocrTok = dAlignedHumOcr[i[0]]\n",
    "            ocrHead = dAlignedHumOcr[i[1]]\n",
    "            humTok = i[0]\n",
    "            humHead =i[1]\n",
    "            if (ocrTok, ocrHead) in dNoisyDict:\n",
    "                if dNoisyDict[ocrTok, ocrHead][0] == dCleanDict[i][0]:\n",
    "                    correctLabel += 1\n",
    "            elif (ocrHead, ocrTok) in dNoisyDict:\n",
    "                if dNoisyDict[ocrHead, ocrTok][0] == dCleanDict[i][0]:\n",
    "                    correctLabel += 1\n",
    "    if allOcrLabels == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = correctLabel/float(allOcrLabels)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        fscore = None\n",
    "    else:\n",
    "        fscore = (2.0 * precision * recall) / (precision + recall)\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluateDependencyParsing(...) function:\n",
    "for i, row in sampledf.iloc[:1].iterrows():\n",
    "    target = \"correct\" # 'correct', 'incorrect', 'all'\n",
    "    dependency_length = \"neighbour\" # 'all', 'neighbour', 'close', 'far', 'nsubj', 'compound'\n",
    "    ocrText = row['ocrText'].strip(\" \")\n",
    "    humanText = row['corrected'].strip(\" \")\n",
    "    spacyOcr = row['spacyOcr']\n",
    "    spacyHum = row['spacyHum']\n",
    "    alignment = ast.literal_eval(row['alignment'])\n",
    "    evaluateDependencyParsing(ocrText, humanText, spacyOcr, spacyHum, alignment, dependency_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"correct\" if the target token is identical between OCR and Hum text,\n",
    "# \"incorrect\" if not, \n",
    "# \"all\" if we want all cases\n",
    "target = \"all\"\n",
    "\n",
    "# \"all\" for any distance between heads\n",
    "# \"neighbour\" for only neighbouring relations between heads\n",
    "# \"close\" for distance of three tokens between heads\n",
    "# \"far\" for distance of above five tokens between heads\n",
    "# \"nsubj\" for nsubj relationship\n",
    "# \"compound\" for compound relationship\n",
    "dependency_length = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampledf['str_similarity'] = sampledf['str_similarity'].astype(float)\n",
    "sampledf[\"sent_fscore\"] = \"\"\n",
    "sampledf[\"sent_acc\"] = \"\"\n",
    "sampledf[\"neriobtype_acc\"] = \"\"\n",
    "sampledf[\"nertype_acc\"] = \"\"\n",
    "sampledf[\"ner_person_acc\"] = \"\"\n",
    "sampledf[\"ner_per_tp\"] = \"\"\n",
    "sampledf[\"ner_per_fp\"] = \"\"\n",
    "sampledf[\"ner_per_fn\"] = \"\"\n",
    "sampledf[\"ner_gpe_tp\"] = \"\"\n",
    "sampledf[\"ner_gpe_fp\"] = \"\"\n",
    "sampledf[\"ner_gpe_fn\"] = \"\"\n",
    "sampledf[\"ner_date_tp\"] = \"\"\n",
    "sampledf[\"ner_date_fp\"] = \"\"\n",
    "sampledf[\"ner_date_fn\"] = \"\"\n",
    "sampledf[\"neriobtype_acc\"] = \"\"\n",
    "sampledf[\"dependency_parsing\"] = \"\"\n",
    "for index, row in sampledf.iterrows():\n",
    "    if row['alignment'] != \"\":\n",
    "        ocrText = row['ocrText'].strip(\" \")\n",
    "        humanText = row['corrected'].strip(\" \")\n",
    "        spacyOcr = row['spacyOcr']\n",
    "        spacyHum = row['spacyHum']\n",
    "        alignment = ast.literal_eval(row['alignment'])\n",
    "        sentFscore, sentAcc = sentenceSplitting(spacyOcr, spacyHum, alignment)\n",
    "        accEnttype, accEntiobtype, person_tp, person_fp, person_fn, gpe_tp, gpe_fp, gpe_fn, date_tp, date_fp, date_fn = evaluateNer(ocrText, humanText, spacyOcr, spacyHum, alignment, target)\n",
    "        dep_parsing = evaluateDependencyParsing(ocrText, humanText, spacyOcr, spacyHum, alignment, dependency_length)\n",
    "        sampledf.loc[index, \"sent_fscore\"] = sentFscore\n",
    "        sampledf.loc[index, \"sent_acc\"] = sentAcc\n",
    "        sampledf.loc[index, 'nertype_acc'] = accEnttype\n",
    "        sampledf.loc[index, 'neriobtype_acc'] = accEntiobtype\n",
    "        sampledf.loc[index, \"ner_per_tp\"] = person_tp\n",
    "        sampledf.loc[index, \"ner_per_fp\"] = person_fp\n",
    "        sampledf.loc[index, \"ner_per_fn\"] = person_fn\n",
    "        sampledf.loc[index, \"ner_gpe_tp\"] = gpe_tp\n",
    "        sampledf.loc[index, \"ner_gpe_fp\"] = gpe_fp\n",
    "        sampledf.loc[index, \"ner_gpe_fn\"] = gpe_fn\n",
    "        sampledf.loc[index, \"ner_date_tp\"] = date_tp\n",
    "        sampledf.loc[index, \"ner_date_fp\"] = date_fp\n",
    "        sampledf.loc[index, \"ner_date_fn\"] = date_fn\n",
    "        sampledf.loc[index, 'dependency_parsing'] = dep_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sentence segmentation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sent_filtered_df = sampledf[~sampledf['sent_acc'].isnull()]\n",
    "sent_filtered_df = sent_filtered_df[sampledf['sent_acc'] != \"\"]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(sent_filtered_df['str_similarity'], sent_filtered_df['sent_acc'], 'ko', alpha=0.2)\n",
    "\n",
    "plt.text(0.95, 0.8, 'Quality band 1', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.85, 0.8, 'Quality band 2', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.75, 0.8, 'Quality band 3', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.65, 0.8, 'Quality band 4', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "\n",
    "plt.grid()\n",
    "plt.xticks(size=24)\n",
    "plt.yticks(size=24)\n",
    "plt.xlabel(\"String similarity (Levenshtein)\", size=28)\n",
    "plt.ylabel(\"Sentence segmentation accuracy\", size=28)\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.axvline(0.9, 0, 1, c='r', ls='--')\n",
    "plt.axvline(0.8, 0, 1, c='r', ls='--')\n",
    "plt.axvline(0.7, 0, 1, c='r', ls='--')\n",
    "plt.savefig(\"figures/linguistic_analysis/sent_split.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot named entity recognition accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_filtered_df = sampledf[~sampledf['neriobtype_acc'].isnull()]\n",
    "ner_filtered_df = ner_filtered_df[sampledf['neriobtype_acc'] != \"\"]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(ner_filtered_df['str_similarity'], ner_filtered_df['neriobtype_acc'], 'ko', alpha=0.2)\n",
    "\n",
    "plt.text(0.95, 0.05, 'Quality band 1', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.85, 0.05, 'Quality band 2', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.75, 0.05, 'Quality band 3', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.65, 0.05, 'Quality band 4', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "\n",
    "plt.grid()\n",
    "plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], size=24)\n",
    "plt.yticks(size=24)\n",
    "plt.xlabel(\"String similarity (Levenshtein)\", size=28)\n",
    "plt.ylabel(\"Named entity recognition accuracy\", size=28)\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.axvline(0.9, 0, 1, c='r', ls='--')\n",
    "plt.axvline(0.8, 0, 1, c='r', ls='--')\n",
    "plt.axvline(0.7, 0, 1, c='r', ls='--')\n",
    "plt.savefig(\"figures/linguistic_analysis/ner_acc.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot named dependency parsing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_filtered_df = sampledf[~sampledf['dependency_parsing'].isnull()]\n",
    "dep_filtered_df = dep_filtered_df[sampledf['dependency_parsing'] != \"\"]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(dep_filtered_df['str_similarity'], dep_filtered_df['dependency_parsing'], 'ko', alpha=0.2)\n",
    "\n",
    "plt.text(0.95, 0.8, 'Quality band 1', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.85, 0.8, 'Quality band 2', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.75, 0.8, 'Quality band 3', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "plt.text(0.65, 0.8, 'Quality band 4', fontsize=16,\n",
    "        rotation=90, rotation_mode='anchor', color='r')\n",
    "\n",
    "plt.grid()\n",
    "plt.xticks([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], size=24)\n",
    "plt.yticks(size=24)\n",
    "plt.xlabel(\"String similarity (Levenshtein)\", size=28)\n",
    "plt.ylabel(\"Dependency parsing accuracy\", size=28)\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.axvline(0.9, 0, 1, c='r', ls='--')\n",
    "plt.axvline(0.8, 0, 1, c='r', ls='--')\n",
    "plt.axvline(0.7, 0, 1, c='r', ls='--')\n",
    "plt.savefig(\"figures/linguistic_analysis/dep_parsing.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledf_band1 = sampledf[sampledf['quality_band'] == 1]\n",
    "sampledf_band2 = sampledf[sampledf['quality_band'] == 2]\n",
    "sampledf_band3 = sampledf[sampledf['quality_band'] == 3]\n",
    "sampledf_band4 = sampledf[sampledf['quality_band'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampledf_band1['dependency_parsing'].mean())\n",
    "print(sampledf_band2['dependency_parsing'].mean())\n",
    "print(sampledf_band3['dependency_parsing'].mean())\n",
    "print(sampledf_band4['dependency_parsing'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbands = [sampledf_band1, sampledf_band2, sampledf_band3, sampledf_band4]\n",
    "for enttype in ['per', 'gpe', 'date']:\n",
    "    print(enttype)\n",
    "    for band in reversed(range(len(dfbands))):\n",
    "        per_tp = dfbands[band]['ner_' + enttype + '_tp'].sum(axis=0)\n",
    "        per_fp = dfbands[band]['ner_' + enttype + '_fp'].sum(axis=0)\n",
    "        per_fn = dfbands[band]['ner_' + enttype + '_fn'].sum(axis=0)\n",
    "\n",
    "        precision = per_tp / (per_tp + per_fp)\n",
    "        recall = per_tp / (per_tp + per_fn)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        print(\"\\tBand\", str(band + 1) + \":\", fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "labels = ['per', 'gpe', 'date']\n",
    "\n",
    "bands_fscores = []\n",
    "for band in range(len(dfbands)):\n",
    "    entityfscores = []\n",
    "    for enttype in labels:\n",
    "        per_tp = dfbands[band]['ner_' + enttype + '_tp'].sum(axis=0)\n",
    "        per_fp = dfbands[band]['ner_' + enttype + '_fp'].sum(axis=0)\n",
    "        per_fn = dfbands[band]['ner_' + enttype + '_fn'].sum(axis=0)\n",
    "\n",
    "        precision = per_tp / (per_tp + per_fp)\n",
    "        recall = per_tp / (per_tp + per_fn)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        entityfscores.append(fscore)\n",
    "    bands_fscores.append(entityfscores)\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - 0.30, bands_fscores[0], width, label='Band1')\n",
    "rects2 = ax.bar(x - 0.10, bands_fscores[1], width, label='Band2')\n",
    "rects3 = ax.bar(x + 0.10, bands_fscores[2], width, label='Band3')\n",
    "rects4 = ax.bar(x + 0.30, bands_fscores[3], width, label='Band4')\n",
    "\n",
    "ax.set_ylabel('F-score', size=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['PERSON', 'GPE', 'DATE'], size=14)\n",
    "ax.legend()\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.grid()\n",
    "plt.savefig(\"figures/linguistic_analysis/ner_per_entity.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp] *",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
